<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Gatsby Starter Blog RSS Feed]]></title><description><![CDATA[A starter blog demonstrating what Gatsby can do.]]></description><link>https://rfaircloth.com</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 25 Mar 2022 13:50:19 GMT</lastBuildDate><item><title><![CDATA[Dueling time zones no one wins]]></title><description><![CDATA[The problem with a pistol dual is often both parties lose. I often have time zone-centered conversations around logging that consume an…]]></description><link>https://rfaircloth.com/2022/02/15/dueling-time-zones-no-one-wins/</link><guid isPermaLink="false">https://rfaircloth.com/2022/02/15/dueling-time-zones-no-one-wins/</guid><pubDate>Tue, 15 Feb 2022 14:03:20 GMT</pubDate><content:encoded>&lt;p&gt;The problem with a pistol dual is often both parties lose. I often have time zone-centered conversations around logging that consume an extraordinary amount of the admin and end-users time trying to prove or disprove the correctness of an event timestamp. The problem we always have in this conversation is relative perspective. We could almost describe this as a three-body problem but it’s not quite that hard.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The time the device thinks is correct unfortunatly for network gear this is often slightly wrong or very very wrong.&lt;/li&gt;
&lt;li&gt;The time the admin thinks the device should have (local time where device is)&lt;/li&gt;
&lt;li&gt;The time the admin is in when the admin is in another time zone.&lt;/li&gt;
&lt;li&gt;Day Light Saving…..&lt;/li&gt;
&lt;li&gt;Leap time&lt;/li&gt;
&lt;li&gt;Short time zones can mean more than one value CST (US and China both have one)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Save time and money by making the following choices&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All systems and devices that are not end-user compute (desktops laptops phones vdi) must use UTC as the system clock and the clock time for logging&lt;/li&gt;
&lt;li&gt;Users should set their display time zone in the Search app to their preference or local time zone&lt;/li&gt;
&lt;li&gt;If an exception justifies a exception to this policy other than infra admin preference the device must include the tz offset in logs using +/-HH:MM syntax&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Mitigating the impact of sources that have lost their place in time]]></title><description><![CDATA[There are certain things in machine data that can not be forgiven. Correct identification of time in UTC (time offsets are fine) Correct…]]></description><link>https://rfaircloth.com/2022/02/14/mitigating-the-impact-of-sources-that-have-lost-their-place-in-time/</link><guid isPermaLink="false">https://rfaircloth.com/2022/02/14/mitigating-the-impact-of-sources-that-have-lost-their-place-in-time/</guid><pubDate>Mon, 14 Feb 2022 16:56:05 GMT</pubDate><content:encoded>&lt;p&gt;There are certain things in machine data that can not be forgiven.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correct identification of time in UTC (time offsets are fine)&lt;/li&gt;
&lt;li&gt;Correct identification of metadata at write time (what kind of data is this)&lt;/li&gt;
&lt;li&gt;Correct breaking of “events” where does an event end and the next begin&lt;/li&gt;
&lt;li&gt;Correct storage of data for access control and retention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Getting these values wrong can prevent events from being found at all. Events with dates in the past won’t be found by alerting searches ever. Events from the future will cause performance harm in the future when data is accessed from slow storage as well as wasted space due to over retention.&lt;/p&gt;
&lt;p&gt;Some Splunky things you should do now to improve performance and usability&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
#indxes.conf
[default]
# This is already set for you on Splunk Cloud, if you have been living under a rock add
# this to indexes.conf and monitor for any new data in index=main as this is
# a likely indicator of a mistake
lastChanceIndex = main
# Keep bucket spans to no more than 12 hours. This helps performance with slow storage and typical search profiles for indexes with low total daily volume. High volume indexes will continue to roll based on size
maxHotSpanSecs = 43200
# Data from the future is not normal routing such data to quarantine prevents excessive retention.
quarantineFutureSecs = 86401
# We set this to default of one day to ensure data that is incorrectly time stamped is correctly retained per retention settings. On some indexes such as those from third party threat detection systems may send &quot;old&quot; events by design indexes should be configured specifically for those indexes don&apos;t mix &quot;old and batch&quot; with real time streaming sources
quarantinePastSecs = 86401
# Once this is set you can not roll back to versions prior to 7.2 for most deployments this is safe wait until you have upgraded to a 7.2 + version for 30 days if you currently are on unsupported versions
journalCompression = zstd

# more compact (better performance) use of tsidx for DMA
tsidxWritingLevel = 4

# props.conf
[default]
# this change will potentially &quot;break differenly&quot; some improperly onboarded log sources.
# When no configuration is provided Splunk will try to guess how to break events this
# requires holding data in memory as the Splunk indexer deployment scales to more indexers
# this can be increasingly painful its better to turn it off which will result in
# single line events and configure sourcetypes properly as needed
SHOULD_LINEMERGE = false
# related to &amp;lt;meta charset=&quot;utf-8&quot;&gt;&amp;lt;/meta&gt;quarantinePastSecs in indexes.conf this setting mitigates the impact of bad dates but must be set per source/sourcetype for the exception cases where this is valid
MAX_DAYS_AGO = 1
MAX_DAYS_HENCE = 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
The benefit of the application of these settings is a mitigation of the impact of badly setup data. While data will continue to have reduced value the impact will be short-lived and will dissipate rapidly as individual sources are corrected.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[When bigger isn't better]]></title><description><![CDATA[This won’t take long, I still read slashdot there you have my confession. This article discusses the “fallout” politically from the Tsar…]]></description><link>https://rfaircloth.com/2021/12/10/when-bigger-isnt-better/</link><guid isPermaLink="false">https://rfaircloth.com/2021/12/10/when-bigger-isnt-better/</guid><pubDate>Fri, 10 Dec 2021 13:49:34 GMT</pubDate><content:encoded>&lt;p&gt;This won’t take long, I still read slashdot there you have my confession. This article discusses the “&lt;a href=&quot;https://tech.slashdot.org/story/21/12/09/2214255/revisiting-the-tsar-bomba-nuclear-test&quot;&gt;fallout&lt;/a&gt;” politically from the Tsar Bomba tests and I found very interesting a single comment (I’ve read most of it before.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“There is always this temptation for big bombs. I found a memo by somebody at Sandia, talking about meeting with the military. He said that the military didn’t really know what they wanted these big bombs for, but they figured that if the Soviets thought they were a good idea, then the US should have one, too”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Why did this interest me? I notice the desire for “bigger” isn’t just an issue for bombs but often an issue for systems design. the Tsar Bomb was certainly the biggest, it certainly “worked” but could it ever be used? What we know about the bomb indicates it was so big it couldn’t fit in the plane they strapped it to the bottom and hoped not to crash on take off. The aerodynamic of the plane changed so much it would be defenseless in actual combat. It’s just too big to be used.&lt;/p&gt;
&lt;p&gt;How does this relate to system design, when we design a system so large we can just make one, we can’t replicate it, we can’t test it we find we have a system so large it “can work” but just “can’t work”.&lt;/p&gt;
&lt;p&gt;This doesn’t mean all definitions of “big” are poor choices but it’s a consideration something to be mindful of.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is my system so big it can’t fit in the network I have (plane)?&lt;/li&gt;
&lt;li&gt;Is the system so big its blast radius can take out my entire company/department if it fails?&lt;/li&gt;
&lt;li&gt;Is the system so big I can’t test it because I can only make one?&lt;/li&gt;
&lt;li&gt;Is the system so big I can’t test it outside of production because I can’t recreate the remainder of the environment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What can I do about a system too big?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can I distribute across zones or instances?&lt;/li&gt;
&lt;li&gt;Can I delegate functionality to external systems?&lt;/li&gt;
&lt;li&gt;Can I make the environment larger to avoid “big fish in little pond” and “noisy neighbor” concerns?&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Another reason you want agents]]></title><description><![CDATA[Microsoft has released a cool new tool for Linux ported from Windows. I was asked today why I don’t think “syslog” is an acceptable way to…]]></description><link>https://rfaircloth.com/2021/10/14/another-reason-you-want-agents/</link><guid isPermaLink="false">https://rfaircloth.com/2021/10/14/another-reason-you-want-agents/</guid><pubDate>Thu, 14 Oct 2021 17:28:26 GMT</pubDate><content:encoded>&lt;p&gt;Microsoft has released a cool new tool for Linux ported from Windows. I was asked today why I don’t think “syslog” is an acceptable way to bring large events into the SIEM (Splunk of course). It took about 60s to wrap up the conversation the original asker of the question was able to validate my concerns pretty quickly sadly many times expectations and requirements for open or no agents get set before the “problem we are trying to solve” or “use case is defined.&lt;/p&gt;
&lt;p&gt;When I first said no the immediate response was “yes it works fine”. This is the “it works on my machine problem.&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;[![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2021/10/image.png?resize=952%2C258&amp;ssl=1)](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2021/10/image.png?ssl=1)&lt;/figure&gt;In the pcap above we see a full syslog event over the wire (tcp) and it looks a-ok but there is always more keep watching packets and you start to see data truncation.
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;[![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2021/10/image-1.png?resize=938%2C337&amp;ssl=1)](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2021/10/image-1.png?ssl=1)&lt;/figure&gt;Hey man bad XML what gives? The system loggers on RedHat and ubuntu are forked builds of their upstream syslog-ng and rsyslog source products. The vendors keep a close eye on the features and functions they need to write to /var/log/\* but don’t actually test or validate most other functionality if you look closely at the source well not even that close you will need a Franken build of patches from the upstream that isn’t the upstream. To ship large events from a Linux host over TCP you need the upstream proper builds for your OS which means changing out the package of a core feature of the OS ask yourself who is going to support that.
&lt;p&gt;After you do this swap out (and figure out how to test validate and support) now you need to configure it. Using the 1980s bsd syslog is a bad look. Why? Well, there is an IETF standard (RFC5424) it addresses issues like breakage with \n and other special chars BSD didn’t have to think about but it’s never the default for the same reasons IBM still sells mainframes the industry is scared to break existing implementations. If you want /need to avoid an agent now you have to not only load up third-party builds not supported by your OS vendor but also have to make diverging config changes and figure out how to support that. &lt;a href=&quot;https://duckduckgo.com/?q=rsyslog+send+IETF+framed+syslog&amp;#x26;atb=v209-1&amp;#x26;ia=web&quot;&gt;Searching&lt;/a&gt; for that with all the correct words will land you &lt;a href=&quot;https://www.rsyslog.com/sending-messages-to-a-remote-syslog-server/&quot;&gt;here&lt;/a&gt;. So what to do? Using host agents is the best choice for Splunk that’s the Universal Forwarder if you really want to make this work you need to find the right combination of &lt;a href=&quot;https://www.rsyslog.com/doc/v8-stable/configuration/modules/omfwd.html&quot;&gt;settings&lt;/a&gt; and be prepared to be up all night when something goes wrong because no one else can find the docs on the internet.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[You don't have enough fingers]]></title><description><![CDATA[I have sensitive data in my logs and I need to filter that out Security teams world wide Filtering out sensitive data sounds like a good…]]></description><link>https://rfaircloth.com/2021/02/21/you-dont-have-enough-fingers/</link><guid isPermaLink="false">https://rfaircloth.com/2021/02/21/you-dont-have-enough-fingers/</guid><pubDate>Sun, 21 Feb 2021 14:39:36 GMT</pubDate><content:encoded>&lt;figure class=&quot;wp-block-gallery columns-1 is-cropped&quot;&gt;- &lt;figure&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2021/02/2fnkx4.jpg?resize=666%2C500&amp;ssl=1)&lt;/figure&gt;
&lt;/figure&gt;As you may have guessed I have been spending a substantial amount of time working with infrastructure log sources. I’ve recently had time to start addressing practice and theory of application level log sources and the substantial risks developers are taking without the awareness of their organization into those risks. The conversation starts like this.
&lt;blockquote&gt;
&lt;p&gt;I have sensitive data in my logs and I need to filter that out&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;Security teams world wide&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Filtering out sensitive data sounds like a good idea right. No its not right, its wrong and this is why.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application developers are maintaining unnecessary sensitive data in high risk in memory code. Why is this high risk? logging APIs are highly configurable while you may know the “WARN” level of logging doesn’t contain sensitive data the “TRACE” level may, the logging component could be configured easily in production to send sensitive data to another malicious target&lt;/li&gt;
&lt;li&gt;Application data written to the front end application tier is more easily exfiltrated&lt;/li&gt;
&lt;li&gt;Application data written to disk is NOT encrypted any user or tool i.e. Ansible could be used to access this data.&lt;/li&gt;
&lt;li&gt;This is a well known vulnerability that must be addressed.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;But defense in depth I want to filter out just in case&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;Security teams world wide&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This just in case approach is the deployment of untested code to production which is both an operational and security risk. Why is it untested glad you ask, because you don’t know your input you can’t test the behavior and unintended consequences to performance and integrity are possible.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is ineffective (think DLP) this approach is simply guessing&lt;/li&gt;
&lt;li&gt;It is expensive, string parsing on a CPU is SLOW, moving strings between processes is even slower.&lt;/li&gt;
&lt;li&gt;It introduces new risks: Man in the middle interception and manipulation of audit data in the stream.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What can we do instead?&lt;/p&gt;
&lt;p&gt;Glad you asked, logging, tracing, metrics are requirements issued to the developers as part of the software development life cycle. Requirements must be tested (automated and manual) as part of the delivery to production. Yes, that is hard everything we do is hard.&lt;/p&gt;
&lt;p&gt;Joe Crobak writes &lt;a href=&quot;https://medium.com/@joecrobak/seven-best-practices-for-keeping-sensitive-data-out-of-logs-3d7bbd12904&quot;&gt;Seven Best Practices for Keeping Sensitive Data Out of Logs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I want to focus on #6 automated QA. Joe mentioned the concept but didn’t elaborate on what that may mean. I will provide an example, using &lt;a href=&quot;https://pypi.org/project/pytest-bdd/&quot;&gt;BDD&lt;/a&gt; testing as an example.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
    Scenario Outline: &amp;lt;action&gt; fruits
        Given there are &amp;lt;start&gt; &amp;lt;fruits&gt;
        When &amp;lt;user&gt; eat &amp;lt;action&gt; &amp;lt;fruits&gt;
        Then &amp;lt;user&gt; should have &amp;lt;left&gt; &amp;lt;fruits&gt;
        Then &amp;lt;user&gt; and &amp;lt;action&gt; or &amp;lt;fruits&gt; combination should not be logged&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
An individuals fruit preference can the action of the individual can be presumed to be private matters that should not be available in logs.

When developing our test parser we will provide an additional test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre class=&quot;wp-block-code&quot;&gt;```
@then(&quot;&lt;user&gt; and &lt;action&gt; or &lt;fruits&gt; combination should not be logged&quot;)
def should_have_left_cucumbers(user, eat, fruits):
# The user itself should be logged
    assert splunk.search(f&quot;| search index=&quot;something&quot; term(\&quot;{user}\&quot;) &gt; 0
# the users action and preference should not be
    assert splunk.search(f&quot;| search index=&quot;something&quot; term(\&quot;{user}\&quot;) AND term(\&quot;{action}\&quot;) == 0
assert splunk.search(f&quot;| search index=&quot;something&quot; term(\&quot;{user}\&quot;) AND term(\&quot;{fruits}\&quot;) == 0
```
```

While a simplified example as you can see using BDD we can link the logging behavior to business requirements for logging and audit making clear to the developers what they can and can not log.

&lt;amp-fit-text height=&quot;80&quot; layout=&quot;fixed-height&quot; max-font-size=&quot;72&quot; min-font-size=&quot;6&quot;&gt;Do your business analyst capture requirements for logging and audit in clear testable ways, if not ask why.

&lt;/amp-fit-text&gt;
</content:encoded></item><item><title><![CDATA[Oh Sh**T we didn't think you would check our work.]]></title><description><![CDATA[Do you have a workflow to check your work or are you trusting the system because you think it works? One of the most frequent conversations…]]></description><link>https://rfaircloth.com/2020/12/21/how-sht-we-didnt-think-you-would-check-our-work/</link><guid isPermaLink="false">https://rfaircloth.com/2020/12/21/how-sht-we-didnt-think-you-would-check-our-work/</guid><pubDate>Tue, 22 Dec 2020 00:52:32 GMT</pubDate><content:encoded>&lt;p&gt;Do you have a workflow to check your work or are you trusting the system because you think it works? One of the most frequent conversations I have goes something like this. Ryan: The best way to accomplish this task is …… some common alternatives you might think work is A B and C but they often fail in silent ways and this is how you know by checking D E F. Frequently I am challenged on my experience with a reply to the effect of “We’ve been doing B for years and never had a problem. I say great I’m always eager to learn how do you validate that works. If you are a betting person what do you think the odds of an answer are here pretty low. Computers and humans are both very reliable one does exactly what it is told the other does exactly what it knows it has to.&lt;/p&gt;
&lt;p&gt;Enter Stamford in order to be fair in vaccine distribution they create a data drive algorithm to “do fair” or really delegate the determination of fair to someone else that didn’t know how to check their work. Please when writing software the lives, careers, and economies depend on test your code don’t be these guys.&lt;/p&gt;
&lt;figure class=&quot;wp-block-embed&quot;&gt;&lt;div class=&quot;wp-block-embed__wrapper&quot;&gt;https://www.theverge.com/2020/12/20/22191749/stanford-medicine-covid-19-vaccine-distribution-list-algorithm-medical-residents &lt;/div&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Commitment to diversity in tech]]></title><description><![CDATA[I’m very pleased with the progress tech has made this year, and I say progress, not arrival because change is hard for humans. As a segment…]]></description><link>https://rfaircloth.com/2020/12/21/commitment-to-diversity-in-tech/</link><guid isPermaLink="false">https://rfaircloth.com/2020/12/21/commitment-to-diversity-in-tech/</guid><pubDate>Mon, 21 Dec 2020 13:50:34 GMT</pubDate><content:encoded>&lt;p&gt;I’m very pleased with the progress tech has made this year, and I say progress, not arrival because change is hard for humans. As a segment of society, I think tech is willfully changing. Every now and then I have something to say on this topic. If it is not personal to me I don’t say much honestly because so much is already said and virtue signaling is a bad look. I commented recently on laziness in ML leading to re-enforced bias. &lt;a href=&quot;https://it.slashdot.org/story/20/12/21/038250/successful-it-workers-applaud-non-traditional-paths-to-tech&quot;&gt;Gatekeeping &lt;/a&gt;in tech has been an issue that has personally impacted me. My path to tech was a unique one, I wasn’t a Barista I was a bored C student in an underperforming rural Alabama highschool that learned how to “tech” the business way. I liked solving problems and people would pay me well to solve computer problems so that’s what I learned. I started really small. Networking Mac computers on apple talk using phonenet connectors to solve a problem the poor rural kids I went to school within Skipperville Alabama needed more opportunity to learn to read than their parents could provide them (Thank you Pizzahut). Also thank you one stoplight little town for giving me a chance to get started. My path to IT Started because no one in “tech” was willing to solve the problem. I’ve built a worldwide reputation not on my formal education but on my customer focus. I don’t like the gatekeeping in tech but I also highly value education and well-educated knowledgeable teams I work with. I often find especially west coast tech community members are focused on tech for tech’s sake and that’s great for R&amp;#x26;D but it brings solutions to real problems which are where “non-traditional” people like me come in. While I might not be the guy that invents a new way of storing machine data I am the guy you call to build the largest application of that software in the world we can’t do this without diversity.&lt;/p&gt;
&lt;p&gt;Lets talk about that phrase non-traditional. Think about the 10 names in tech you can quickly and got find their resume online. I promise you most of them don’t have formal tech education Like me we came to tech not because tech attracted us but the problem attracted us. Yes more people are in tech today because of STEM education but I would argue I am “classically trained” :). Lets work together to solve the worlds problems, I say for 2021 lets turn off the zoom camera. Change how we evaluate resumes to be focused on passion outcomes and less on certifications and degrees.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[MaxMind Databases and Splunk Enterprise]]></title><description><![CDATA[I’ve finally been able to take a couple of days and update and refresh my MaxMind Add-on for Splunk Enterprise and Enterprise Cloud. The…]]></description><link>https://rfaircloth.com/2020/11/13/maxmind-databases-and-splunk-enterprise/</link><guid isPermaLink="false">https://rfaircloth.com/2020/11/13/maxmind-databases-and-splunk-enterprise/</guid><pubDate>Sat, 14 Nov 2020 01:49:06 GMT</pubDate><content:encoded>&lt;p&gt;I’ve finally been able to take a couple of days and update and refresh my MaxMind Add-on for Splunk Enterprise and Enterprise Cloud. The latest version of the add-on updates the GeoIP2 library allowing for additional fields from the licensed anonymous IP database. It also built and tested using the new Addonfactory CI/CD infrastructure at Splunk. (See my conf talk). This is a major version as it introduces a requirement for python3 and thus Splunk Enterprise 8.0&gt; because GeoIP2 is now python3 only. Older versions should still work for now if you can not upgrade. Head over to Splunkbase to get it now &lt;a href=&quot;https://splunkbase.splunk.com/app/3022/%20&quot;&gt;https://splunkbase.splunk.com/app/3022/ &lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Your cloud vendor wants to send syslog cloud to cloud]]></title><description><![CDATA[IETF Syslog meaning RFC5424 over TLS (RFC5425) seems like a good idea until you think of the consequences and just what those consequences…]]></description><link>https://rfaircloth.com/2020/10/28/your-cloud-vendor-wants-to-send-syslog/</link><guid isPermaLink="false">https://rfaircloth.com/2020/10/28/your-cloud-vendor-wants-to-send-syslog/</guid><pubDate>Wed, 28 Oct 2020 21:06:32 GMT</pubDate><content:encoded>&lt;figure class=&quot;wp-block-gallery columns-1 is-cropped&quot;&gt;- &lt;figure&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2020/10/Batman-shocked-face-meme.jpg?resize=400%2C428&amp;ssl=1)&lt;/figure&gt;
&lt;figcaption class=&quot;blocks-gallery-caption&quot;&gt;Shocked customer learning their “security” provider wants to send syslog over the internet&lt;/figcaption&gt;&lt;/figure&gt;I get asked about this from time to time whats wrong with sending syslog over the internet its a standard right?
&lt;p&gt;IETF Syslog meaning RFC5424 over TLS (RFC5425) seems like a good idea until you think of the consequences and just what those consequences might be?&lt;/p&gt;
&lt;p&gt;How do you plan to authenticate that.&lt;/p&gt;
&lt;p&gt;Certificates well maybe this opens your SIEM up to a nasty low cost denial of service problem. Client cert auth is trivial to use as DOS with any invalid cert and expensive validation options. If this was happening how would you know neither syslog nor rsyslog will log this in an obvious way.&lt;/p&gt;
&lt;p&gt;Secret SDATA? now we allow any client to auth and send data we must accept and parse the data to find out if its allowed sure that can’t be abused&lt;/p&gt;
&lt;p&gt;IP Restrictions I have some beach front property for you.&lt;/p&gt;
&lt;p&gt;All of the above&lt;/p&gt;
&lt;p&gt;How will you scale that? please see prior posts on load balancing syslog&lt;/p&gt;
&lt;p&gt;Next time you hear the suggestion of RFC 5424 syslog just laugh at the joke and ask what options are really being proposed.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[When I say syslog what I really mean is]]></title><description><![CDATA[Syslog is a ambiguous term so I thought I would clarify what I am talking about syslog is a daemon where Linux/UNIX sent logs back in the…]]></description><link>https://rfaircloth.com/2020/09/23/when-i-say-syslog-what-i-really-mean-is/</link><guid isPermaLink="false">https://rfaircloth.com/2020/09/23/when-i-say-syslog-what-i-really-mean-is/</guid><pubDate>Wed, 23 Sep 2020 19:24:57 GMT</pubDate><content:encoded>&lt;p&gt;Syslog is a ambiguous term so I thought I would clarify what I am talking about&lt;/p&gt;
&lt;p&gt;syslog is a daemon where Linux/UNIX sent logs back in the day. This in most cases results in an entry in a file in /var/log that may or may not have any particular structure this is normally not what I am talking about&lt;/p&gt;
&lt;p&gt;Syslog was not a standard in the beginning. RFC 3164 is not a standards document its a memorialization of some common practices. Do you want a 1988 Honda Civic if you vendor’s Syslog looks like this you should look at it like a used car.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;111&gt; July 01 12:13:11 My old car&apos;s logs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Syslog is not just text over tcp/udp. A syslog message must have the PRI such as &amp;#x3C;111&gt; it must have a structure something like this:&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;34&gt;1 2003-10-11T22:14:15.003Z mymachine myapplication 1234 ID47 [example@0 class=&quot;high&quot;] BOMmyapplication is started&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Syslog is now a set of standards&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RFC 5424 is the transport neutral message format &lt;a href=&quot;https://tools.ietf.org/html/rfc5424&quot;&gt;https://tools.ietf.org/html/rfc5424&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RFC 5425 describes how to use TLS as the transport (best practice) if network security matters worst practice when performance matters &lt;a href=&quot;https://tools.ietf.org/html/rfc5425&quot;&gt;https://tools.ietf.org/html/rfc5425&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RFC 5426 describes how to use UDP as the transport best practice for performance &lt;a href=&quot;https://tools.ietf.org/html/rfc5426&quot;&gt;https://tools.ietf.org/html/rfc5426&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RFC 6587 describes how to use TCP as the transport worst practice for performance best practice for large messages over unreliable networks &lt;a href=&quot;https://tools.ietf.org/html/rfc5587&quot;&gt;https://tools.ietf.org/html/rfc5587&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A message should not be considered “standard Syslog” if it is not in the RFC5424 protocol using RFC 5425 5426 or 6587 as the transport. Standards compliance matters lets start making vendors feel bad they have had 12 years to get it right.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Devices that think you know their name]]></title><description><![CDATA[If the device has a host name in the event use that Else if our management/cmdb solution knows the right name use that instead Else maybe…]]></description><link>https://rfaircloth.com/2020/05/28/devices-that-think-you-know-their-name/</link><guid isPermaLink="false">https://rfaircloth.com/2020/05/28/devices-that-think-you-know-their-name/</guid><pubDate>Thu, 28 May 2020 19:46:48 GMT</pubDate><content:encoded>&lt;figure class=&quot;wp-block-gallery columns-1 is-cropped&quot;&gt;- &lt;figure&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2020/05/507.jpg?resize=621%2C691&amp;ssl=1)&lt;/figure&gt;
&lt;/figure&gt;What exactly is that talkers name is one of the most frustrating problems in syslog eventing and the most frustrating in analytics. For far too long the choices have been to use the devices name OR use reverse DNS but never both. Today SC4S 1.20.0 solves this problem by doing what you would do!
&lt;ol&gt;
&lt;li&gt;If the device has a host name in the event use that&lt;/li&gt;
&lt;li&gt;Else if our management/cmdb solution knows the right name use that instead&lt;/li&gt;
&lt;li&gt;Else maybe someone updated DNS try that instead.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simple logical easy to understand and available now in &lt;a href=&quot;https://splunk-connect-for-syslog.readthedocs.io/&quot;&gt;Splunk Connect for Syslog.&lt;/a&gt; No more of this&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2020/05/image.png?resize=654%2C51&amp;ssl=1)&lt;figcaption&gt;Event with IP as a host &lt;/figcaption&gt;&lt;/figure&gt;Plenty more like this
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2020/05/image-1.png?resize=662%2C54&amp;ssl=1)&lt;figcaption&gt;IP translated to host using CMDB sourced lookup&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Performant AND Reliable Syslog UDP is best]]></title><description><![CDATA[The faces I’ve seen made to this statement say a lot. I hope you read past the statement for my reasons and when other requirements may…]]></description><link>https://rfaircloth.com/2020/05/21/performant-and-reliable-syslog-udp-is-best/</link><guid isPermaLink="false">https://rfaircloth.com/2020/05/21/performant-and-reliable-syslog-udp-is-best/</guid><pubDate>Thu, 21 May 2020 20:51:21 GMT</pubDate><content:encoded>&lt;p&gt;The faces I’ve seen made to this statement say a lot. I hope you read past the statement for my reasons and when other requirements may prompt another choice.&lt;/p&gt;
&lt;div class=&quot;wp-container-623bb344cf022 wp-block-group&quot;&gt;&lt;div class=&quot;wp-block-group__inner-container&quot;&gt;&lt;figure class=&quot;wp-block-image size-large&quot;&gt;![](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2020/05/42cx56.jpg?resize=577%2C432&amp;ssl=1)&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;Wait you say TCP uses ACKS so data won’t be lost, yes that’s true but there are buts
&lt;ul&gt;
&lt;li&gt;But when the TCP session is closed events published while the system is creating a new session will be lost. (Closed Window Case)&lt;/li&gt;
&lt;li&gt;But when the remote side is busy and can not ack fast enough events are lost due to local buffer full&lt;/li&gt;
&lt;li&gt;But when a single ack is lost by the network and the client closes the connection. (local and remote buffer lost)&lt;/li&gt;
&lt;li&gt;But when the remote server restarts for any reason (local buffer lost)&lt;/li&gt;
&lt;li&gt;But when the remote server restarts without closing the connection (local buffer plus timeout time lost)&lt;/li&gt;
&lt;li&gt;But when the client side restarts without closing the connection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s a lot of buts and its why TCP is not my first choice when my requirement is for mostly available syslog (no such thing as HA) with minimized data loss.&lt;/p&gt;
&lt;p&gt;Wait you say when should I use TCP syslog. To be honest there is only one case. When the syslog event is larger than the maximum size of the UDP packet on your network typically limited to Web Proxy, DLP and IDs type sources. That is messages that are very large but not very fast compared to firewalls for example. So we jump to TCP when the network can’t handle the length of our events&lt;/p&gt;
&lt;p&gt;There is a third option TLS a subset of devices can forward logs using TLS over TCP this provides some advantages with proper implementation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TLS can continue a session over a broken TCP reducing buffer loss conditions&lt;/li&gt;
&lt;li&gt;TLS will fill packets for more efficient use of wire&lt;/li&gt;
&lt;li&gt;TLS will compress in most cases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I am here I want to say a word about Load Balancers as a means of high availability. This is snake oil.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TCP over an NLB double the opportunity for network error to cause data loss and almost always increases the size of the buffer lost I have seen over 25% loss on multiple occasions&lt;/li&gt;
&lt;li&gt;TCP over NLB can lead to imbalanced resource use due to long-lived sessions. The NLB is not designed to balance for connection KbS its design to balance connections in TCP all connections are not equal leading to out of disk space conditions&lt;/li&gt;
&lt;li&gt;UDP can not be probed UDP over NLB can lead to sending logs to long-dead servers.&lt;/li&gt;
&lt;li&gt;Load Balancers break message reassembly common examples of 1 of 3 type messages like Cisco ACS, Cisco ISE, Symantec Mail Gateway can not be properly processed when sprayed across multiple servers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wait you ask how do I mitigate down time for Syslog?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use VM Ware or hyper-v with a cluster of hosts which will reduce your outage to only host reboots which in this day and time is rare&lt;/li&gt;
&lt;li&gt;Use a Clustered IP solution (i.e. Keepalived) so you can drain the server to a partner before restart.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A few other idea’s you may have to bring “HA” to syslog that will be counter productive&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNS –
&lt;ul&gt;
&lt;li&gt;Most known Syslog sources will only use 1 typically the first or one random IP from a list of A records for a very long period of time ignoring the TTL. Using DNS to change the target is likely to not work in a short enough period of time in some cases hours&lt;/li&gt;
&lt;li&gt;DNS Global Load Balancer similar to the above clients often holds cached results for far longer than TTL. In addition, the actual device configuration does not use the correct DNS servers for GLB to properly detect distance and will route incorrectly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AnyCast
&lt;ul&gt;
&lt;li&gt;UDP anycast can work in exceptional condition the scale of a single clustered pair of Syslog servers can not provide capacity. (Greater than 10 TB per day) However, because of the polling issues described with NLBs above my experience with AnyCast has been high data loss and project failure. Over a dozen projects with well-known logos over the last 10 years names you would know. While Anycast can simplify administration it does not mitigate loss and if the routers in use are not up to the task can increase loss. Most anycast use cases have some method of recovery such as DNS. syslog does not. While AnyCast on paper seems to be an easy answer the engineering required to succed is not trival ask youself is it worth it, and can we monitor it effectivly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sending the message multiple times to multiple servers to so it can be “de-duplicated” by “someone’s software” Deduplication requires global unique keys this doesn’t exist so this isn’t possible. More than once is worse than sometimes never because if we are counting errors or attacks we see more than is real resulting in false positives and causing lack of operational trust in the data making your project effectively useless. A missed event will more likely than not occur again and be captured in short order.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[A syslog time zone is a terrible thing to get wrong]]></title><description><![CDATA[Splunk release 1.2.0 of Splunk Connect for syslog today. This release focused on timezone management. We all wish time was standardized on…]]></description><link>https://rfaircloth.com/2019/11/17/a-syslog-time-zone-is-a-terrible-thing-to-get-wrong/</link><guid isPermaLink="false">https://rfaircloth.com/2019/11/17/a-syslog-time-zone-is-a-terrible-thing-to-get-wrong/</guid><pubDate>Mon, 18 Nov 2019 01:17:14 GMT</pubDate><content:encoded>&lt;p&gt;Splunk release 1.2.0 of Splunk Connect for syslog today. This release focused on timezone management. We all wish time was standardized on UTC many of us have managed to get that written into approved standards but did not live to see the implementation of it. SC4S 1.2.0 enables the syslog-ng feature “guess-timezone” allowing the dynamic resolution of time zone of those poorly behaving devices relative to UTC. As a fall back or to deal with devices that batch/or stream with high latency device TZ can be managed at the host/ip/subnet level. Ready to upgrade? If you are running the container version just restart SC4S this feature is auto-magic.&lt;/p&gt;
&lt;p&gt;Want to know more about SC4S Checkout these blog posts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part 1: &lt;a href=&quot;https://www.splunk.com/en_us/blog/tips-and-tricks/splunk-connect-for-syslog-turnkey-and-scalable-syslog-gdi.html&quot;&gt;https://www.splunk.com/en_us/blog/tips-and-tricks/splunk-connect-for-syslog-turnkey-and-scalable-syslog-gdi.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 2: &lt;a href=&quot;https://www.splunk.com/en_us/blog/tips-and-tricks/splunk-connect-for-syslog-turnkey-and-scalable-syslog-gdi-part-2.html&quot;&gt;https://www.splunk.com/en_us/blog/tips-and-tricks/splunk-connect-for-syslog-turnkey-and-scalable-syslog-gdi-part-2.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Syslog server you say]]></title><description><![CDATA[I’ve had quite a bit to say about syslog as a component of a streaming data architecture primarily feeding Splunk Enterprise (or Enterprise…]]></description><link>https://rfaircloth.com/2019/10/16/syslog-server-you-say/</link><guid isPermaLink="false">https://rfaircloth.com/2019/10/16/syslog-server-you-say/</guid><pubDate>Wed, 16 Oct 2019 23:23:00 GMT</pubDate><content:encoded>&lt;p&gt;I’ve had quite a bit to say about syslog as a component of a streaming data architecture primarily feeding Splunk Enterprise (or Enterprise Cloud). In seven days I will be presenting the culmination of small developments that have taken shape into the brand new Splunk Connect for Syslog (SC4S).&lt;/p&gt;
&lt;p&gt;You don’t have to wait swing over via Splunk Base &lt;a href=&quot;https://splunkbase.splunk.com/app/4740/#/details&quot;&gt;https://splunkbase.splunk.com/app/4740/#/details&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SC4S is designed to:&lt;/p&gt;
&lt;p&gt;Do the heavy lifting of deploying a functioning current build of the awesome syslog-ng OSE (3.24.1 as of this posting).&lt;/p&gt;
&lt;p&gt;Support many popular syslog vendor products OOB with zero configuration or as little configuration as a host glob or IP address&lt;/p&gt;
&lt;p&gt;Scale your Splunk vertically by very evenly distributing events across indexers by the second&lt;/p&gt;
&lt;p&gt;Scale your syslog-ng servers by reducing constrains on CPU and disk&lt;/p&gt;
&lt;p&gt;Reduce your exposure to data loss by minimizing the amount of data at rest on the syslog-ng instance&lt;/p&gt;
&lt;p&gt;Promote great practices and collaboration. SC4S is a liberally licensed open source solution. We will be able to collaborate directly with the end users on filters and usage to promote great big data deployments.&lt;/p&gt;
&lt;p&gt;Personal thanks to many but especially Mark Bonsack and Balazs Scheidler (syslog-ng creator)&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Bias in ML]]></title><description><![CDATA[One day perhaps we can teach machines to avoid bias but maybe just maybe we need to understand how to teach humans the same first. https…]]></description><link>https://rfaircloth.com/2019/08/16/bias-in-ml/</link><guid isPermaLink="false">https://rfaircloth.com/2019/08/16/bias-in-ml/</guid><pubDate>Fri, 16 Aug 2019 22:13:10 GMT</pubDate><content:encoded>&lt;p&gt;One day perhaps we can teach machines to avoid bias but maybe just maybe we need to understand how to teach humans the same first.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://tech.slashdot.org/story/19/08/16/1916202/the-algorithms-that-detect-hate-speech-online-are-biased-against-black-people&quot;&gt;https://tech.slashdot.org/story/19/08/16/1916202/the-algorithms-that-detect-hate-speech-online-are-biased-against-black-people&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It shouldn’t be a news flash that bias people “train” bias into computers just like we train bias into our children. We will one day realize we have no other choice but hard continuous work to eliminate bias.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Phishing from someone else's container ship.]]></title><description><![CDATA[This is a theoretical attack abusing a compromised kubectl certificate pair and exposed K8s api to deploy a phishing site transparently on…]]></description><link>https://rfaircloth.com/2019/08/11/phishing-from-someone-elses-container-ship/</link><guid isPermaLink="false">https://rfaircloth.com/2019/08/11/phishing-from-someone-elses-container-ship/</guid><pubDate>Sun, 11 Aug 2019 23:14:48 GMT</pubDate><content:encoded>&lt;p&gt;This is a theoretical attack abusing a compromised kubectl certificate pair and exposed K8s api to deploy a phishing site transparently on your targets infrastructure. This is a difficult attack to pull off and required existing compromised administrative access to the k8s cluster. A privileged insider, or compromised cert based authentication credential can be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Target &lt;a href=&quot;http://www.spl.guru&quot;&gt;www.spl.guru&lt;/a&gt; which is one of my test domains.&lt;/li&gt;
&lt;li&gt;Desired outcome detect an attempt to intercept admin login for a wordpress site we will utilize a fake email alert informing the administrator a critical update must be applied.&lt;/li&gt;
&lt;li&gt;We will deploy the site hidden behind the targets existing ingress controller, this allows us utilize the customers own domain and certificates eliminating detection by domain name (typo squatting etc) and certificate transparency reporting monitoring.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Phase one: Recon&lt;/h2&gt;
&lt;p&gt;Using kubectl identify name spaces and find the ingress controller used for the site you intended to compromise. For the purposes of my poc my target used a very obvious “wordpress” namespace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt; kubectl -n wordpress get ing&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;NAME HOSTS ADDRESS PORTS AGE&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;site &lt;a href=&quot;http://www.spl.guru&quot;&gt;www.spl.guru&lt;/a&gt; 133a0685-wordpress-site-62d9-1332557661.us-east-1.elb.amazonaws.com 8020h&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Phase two: deploy &lt;a href=&quot;https://getgophish.com&quot;&gt;gophish&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I’m not going to go into details on deploying gophish and setting up or sending the phishing emails. Thats beyond the scope of the blog post, I’m here to help the blue team so lets get on to detection.&lt;/p&gt;
&lt;p&gt;The following manifest hides the gophish instance on a path under the main site url. Of note in this case /wplogin.cgi” is the real site while /wplogin is where we are credential harvesting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;apiVersion: extensions/v1beta1&lt;br&gt;
kind: Ingress&lt;br&gt;
metadata:&lt;br&gt;
name: “site”&lt;br&gt;
namespace: wordpress&lt;br&gt;
annotations:&lt;br&gt;
kubernetes.io/ingress.class: alb&lt;br&gt;
alb.ingress.kubernetes.io/scheme: internet-facing&lt;br&gt;
alb.ingress.kubernetes.io/tags: Environment=dev,Team=test&lt;br&gt;
alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:174701313045:certificate/15d484c8-ca0c-4194-a4ef-f38a43b7b977&lt;br&gt;
alb.ingress.kubernetes.io/listen-ports: ‘[{“HTTP”: 80}, {“HTTPS”:443}]’&lt;br&gt;
alb.ingress.kubernetes.io/actions.ssl-redirect: ‘{“Type”: “redirect”, “RedirectConfig”: { “Protocol”: “HTTPS”, “Port”: “443”, “StatusCode”: “HTTP_301”}}’&lt;br&gt;
# external-dns.alpha.kubernetes.io/hostname: search.gdi.spl.guru.,master.gdi.spl.guru.&lt;br&gt;
spec:&lt;br&gt;
rules:&lt;br&gt;
– host: &lt;a href=&quot;http://www.spl.guru&quot;&gt;www.spl.guru&lt;/a&gt;&lt;br&gt;
http:&lt;br&gt;
paths:&lt;br&gt;
– path: /wplogin&lt;br&gt;
backend:&lt;br&gt;
serviceName: gophish&lt;br&gt;
servicePort: 80&lt;br&gt;
– path: /&lt;br&gt;
backend:&lt;br&gt;
serviceName: wordpress&lt;br&gt;
servicePort: 80&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Phase three: Detecting what we did&lt;/h2&gt;
&lt;p&gt;Using the K8S events and meta data onboard using Splunk Connect for K8S we have some solid places we can monitor for abuse. Side note don’t get hung up on “gophish” this is a hammer type tool your opponent may be much more subtle&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modification to an ingress in my case AWS ALB, ingress records for most will not change often when they are changed an associated approved deployment should also exist.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“kubernetes.io/ingress-name:” sourcetype=”kube:container:alb-ingress-controller” modifying&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;New Image, New Image Repository, New Registry, maintain a list of approved images and registries alert when an image is used not on the pre-defined list, this . may be noisy in dev clusters for no prod clusters reporting may be better than alerting.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;index=em_meta “spec.containers{}.image”=”*”&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[SafelyExposing Splunk S2S to the internet]]></title><description><![CDATA[Splunk has a great token based auth solution for its S2S protocol it was added several versions back. Inputs have both just worked and…]]></description><link>https://rfaircloth.com/2019/08/08/safely-exposing-splunk-s2s-to-the-internet/</link><guid isPermaLink="false">https://rfaircloth.com/2019/08/08/safely-exposing-splunk-s2s-to-the-internet/</guid><pubDate>Thu, 08 Aug 2019 17:56:18 GMT</pubDate><content:encoded>&lt;p&gt;Splunk has a great token based auth solution for its S2S protocol it was added several versions back. Inputs have both just worked and remained unchanged for so long many administrators have not noticed the feature. This allows you to safely expose indexers or heavy forwarders so that UFs on the internet can forward data back in without VPN. This is super critical for Splunk endpoints that don’t require a connection to the corporate network via VPN constantly&lt;/p&gt;
&lt;p&gt;When a Splunk input is exposed to the internet there is a risk of resource exhaustion dos. A simple type of attack where junk data or “well formed but misleading” data is feed to Splunk until all CPU/memory/disk is consumed.&lt;/p&gt;
&lt;p&gt;Once this feature is enabled all UF/HF clients must supply a token to be allowed to connect if your adding this feature to a running Splunk deployment be ready to push the outputs.conf update and inputs.conf updates in close succession to prevent a data flow break.&lt;/p&gt;
&lt;p&gt;Update your inputs.conf as follows, note you can use multiple tokens just like HEC so you can mitigate the number of tokens that need to be replaced if a stolen token is used in a DOS attack&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;mw-collapsed&quot;&gt;# Access control settings.
[splunktcptoken://&amp;lt;token name&gt;]
* Use this stanza to specify forwarders from which to accept data.
* You must configure a token on the receiver, then configure the same
  token on forwarders.
* The receiver discards data from forwarders that do not have the
  token configured.
* This setting is enabled for all receiving ports.
* This setting is optional.

token = &amp;lt;string&gt;
* token should match regex [A-Za-Z0-9\-]+ and with a min length of 12&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Update outputs.conf use the token value of choice from inputs.conf&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;mw-collapsed&quot;&gt;[tcpout:&amp;lt;target_group&gt;]
token = &amp;lt;string&gt;
* The access token for receiving data.
* If you configured an access token for receiving data from a forwarder,
  Splunk software populates that token here.
* If you configured a receiver with an access token and that token is not
  specified here, the receiver rejects all data sent to it.
* This setting is optional.
* No default.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Keeping that up with MaxMind for Splunk and Splunk Cloud]]></title><description><![CDATA[This is a rather long time coming, today version 4.0.0 of SecKit for Geolocation with MaxMind is available for Splunk via Splunk base and…]]></description><link>https://rfaircloth.com/2019/06/04/keeping-that-up-with-maxmind-for-splunk-and-splunk-cloud/</link><guid isPermaLink="false">https://rfaircloth.com/2019/06/04/keeping-that-up-with-maxmind-for-splunk-and-splunk-cloud/</guid><pubDate>Tue, 04 Jun 2019 11:46:13 GMT</pubDate><content:encoded>&lt;p&gt;This is a rather long time coming, today version 4.0.0 of SecKit for Geolocation with MaxMind is available for Splunk via Splunk base and Splunk cloud. This version includes a built in solution for keeping the database files up to date for both free and paid subscribers. Free subscribers are of course limited to the basic information available as equivalent to the built in iplocation command from Splunk. Commercial subscribers can use all of available fields from their licensed files. Jump over to Splunk base to check it out &lt;a href=&quot;https://splunkbase.splunk.com/app/3022/&quot;&gt;https://splunkbase.splunk.com/app/3022/&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[To HEC with syslog All grown up]]></title><description><![CDATA[A few years ago flying across the Atlantic, unable to sleep, I had an idea to integrate common syslog aggregation servers using Splunk’s new…]]></description><link>https://rfaircloth.com/2019/04/22/to-hec-with-syslog-all-grown-up/</link><guid isPermaLink="false">https://rfaircloth.com/2019/04/22/to-hec-with-syslog-all-grown-up/</guid><pubDate>Mon, 22 Apr 2019 14:18:30 GMT</pubDate><content:encoded>&lt;p&gt;A few years ago flying across the Atlantic, unable to sleep, I had an idea to integrate common syslog aggregation servers using Splunk’s new HTTP event Collector rather than file and the tired and true Universal Forwarder. This little idea implemented in python started solving real problems of throughput and latency while reducing the complexity of configuring syslog aggregation servers. I’m very pleased to say the &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src&quot;&gt;python script&lt;/a&gt; I created is now obsolete. Leading syslog server products syslog-ng and rsyslog upstreams have implemented maintained modules. Even more exciting Mark Bonsack has invested considerable time to further develop modular configuration for both to make getting data though syslog and into Splunk even easier!&lt;/p&gt;
&lt;p&gt;Native syslog –&gt; http event collector modules&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://support.oneidentity.com/technical-documents/syslog-ng-premium-edition/7.0.12/administration-guide/sending-and-storing-log-messages-%E2%80%94-destinations-and-destination-drivers/splunk-hec-sending-messages-to-splunk-http-event-collector/&quot;&gt;Syslog-NG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.rsyslog.com/doc/v8-stable/configuration/modules/omhttp.html?highlight=omhttp&quot;&gt;RSYSLOG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Modular Configuration Repositories&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk-syslog-ng&quot;&gt;Syslog-NG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk-rsyslog.&quot;&gt;RSYSLOG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Quick Public Service Notice&lt;/h1&gt;
&lt;p&gt;While all linux distributions include a syslog server this should NOT be used as the production syslog aggregation solution. Linux distros are often many point releases or worse selectively back port patches based only on their own customer reported issues. Before attempting to build a syslog aggregation solution for production it is critical you source current upstream binaries or build your own.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Is your LDAP Slow? It might make your Splunk Slow]]></title><description><![CDATA[I’ve had this crop up enough times, I think its worth a short post. Most Splunk deployments use local and/or LDAP authentication. LDAP…]]></description><link>https://rfaircloth.com/2019/04/12/is-your-ldap-slow-it-might-make-your-splunk-slow/</link><guid isPermaLink="false">https://rfaircloth.com/2019/04/12/is-your-ldap-slow-it-might-make-your-splunk-slow/</guid><pubDate>Fri, 12 Apr 2019 15:04:00 GMT</pubDate><content:encoded>&lt;p&gt;I’ve had this crop up enough times, I think its worth a short post. Most Splunk deployments use local and/or LDAP authentication. LDAP configuration is something of a black art and often the minimal configuration that works is the first and last time this is considered. It is worth your time as an administrator to optimize your LDAP configuration or better yet move to the more secure and reliable SAML standard.&lt;/p&gt;
&lt;p&gt;Things to consider&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stop using LDAP and use SAML&lt;/li&gt;
&lt;li&gt;LDAP authentication should never be enabled on indexers. If you have enabled LDAP authentication remove this. Indexers rarely require authentication, when required only applicable to admins and then under very strict conditions.&lt;/li&gt;
&lt;li&gt;Ensure the Group BIND and Group Filters are both in use and limit the groups to only those required for Splunk access management&lt;/li&gt;
&lt;li&gt;Ensure the User BIND and User Filters are appropriate to limit the potential users to only those users who may login to Splunk.&lt;/li&gt;
&lt;li&gt;Validate the number of users returned by the LDAP query used is under 1000 or increase the number of precached users via limits.conf to an appropriate number.&lt;/li&gt;
&lt;li&gt;Ensure DNSMASQ or an alternative DNS cache client is installed on Linux Search Heads and Indexers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stages to LDAP Auth&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interactive User Login (ad-hoc search) or scheduled report execution&lt;/li&gt;
&lt;li&gt;Check User Cache if not cached or cache expired&lt;/li&gt;
&lt;li&gt;DNS lookup to resolve the LDAP host to IP (This is the reason DNS Cache on linux is important)&lt;/li&gt;
&lt;li&gt;TCP Connection to LDAP Server&lt;/li&gt;
&lt;li&gt;Post query for specific user to LDAP&lt;/li&gt;
&lt;li&gt;Wait for response&lt;/li&gt;
&lt;li&gt;Process Referrals if applicable by repeating the above sequence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The time taken for each DNS query and LDAP query is added to the time taken too login to the Splunk UI, execute an ad-hoc search OR scheduled report. Its important to ensure the DNS and LDAP infrastructure is highly available and able to service potentially thousands of requests per second. Proper use of caching ensures resources on the Splunk Server including TCP client sessions are not exhausted causing users to wait in line for their turn at authentication or worst case time outs and authorization failures.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Wrong sourcetype blues]]></title><description><![CDATA[A Splunk customer wrote a utility to help translate old sourcetype to new source/sourcetype with visual review and a nice workflow for…]]></description><link>https://rfaircloth.com/2019/04/11/wrong-sourcetype-blues/</link><guid isPermaLink="false">https://rfaircloth.com/2019/04/11/wrong-sourcetype-blues/</guid><pubDate>Thu, 11 Apr 2019 14:16:48 GMT</pubDate><content:encoded>&lt;p&gt;A Splunk customer wrote a utility to help translate old sourcetype to new source/sourcetype with visual review and a nice workflow for helping the admin apply changes. Worth checking out and adding to your utility belt.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/dstaulcu/SplunkKOHelper&quot;&gt;https://github.com/dstaulcu/SplunkKOHelper&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[SecKit for Splunk TA Windows is out]]></title><description><![CDATA[I’ve updated the SecKit templates and guidance for Windows TA 6.0 no longer do you need to also deploy the TA for Microsoft DNS and TA for…]]></description><link>https://rfaircloth.com/2019/03/22/seckit-for-splunk-ta-windows-is-out/</link><guid isPermaLink="false">https://rfaircloth.com/2019/03/22/seckit-for-splunk-ta-windows-is-out/</guid><pubDate>Fri, 22 Mar 2019 12:25:59 GMT</pubDate><content:encoded>&lt;p&gt;I’ve updated the SecKit templates and guidance for Windows TA 6.0 no longer do you need to also deploy the TA for Microsoft DNS and TA for Microsoft AD!.&lt;/p&gt;
&lt;p&gt;Check it out &lt;a href=&quot;https://seckit.readthedocs.io/projects/splunk-ta-windows-seckit/en/latest/&quot;&gt;https://seckit.readthedocs.io/projects/splunk-ta-windows-seckit/en/latest/&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Redirecting _internal for a large forwarder deployment]]></title><description><![CDATA[Sometimes it is not noticed because there is no license charge associated with Splunk’s Universal forwarder internal logs and in some cases…]]></description><link>https://rfaircloth.com/2019/03/20/redirecting-_internal-for-a-large-uf-deployment/</link><guid isPermaLink="false">https://rfaircloth.com/2019/03/20/redirecting-_internal-for-a-large-uf-deployment/</guid><pubDate>Thu, 21 Mar 2019 01:43:43 GMT</pubDate><content:encoded>&lt;p&gt;Sometimes it is not noticed because there is no license charge associated with Splunk’s Universal forwarder internal logs and in some cases heavy forwarders. In very large deployments this can be a significant portion of storage used per day. Do you really need to keep those events around as long as the events associated with the Splunk Enterprise instances probably not.&lt;/p&gt;
&lt;h2&gt;License Warning – Updated&lt;/h2&gt;
&lt;p&gt;It has been pointed out this change WILL impact license on recent versions of Splunk in older versions and customers with EAA agreements in place this is OK. If you are on a recent (not sure which version) this change will impact license.&lt;/p&gt;
&lt;h2&gt;Warning!&lt;/h2&gt;
&lt;p&gt;The following changes will disable the Splunk Monitoring consoles built in forwarder monitoring feature. You can customize the searches but be aware this is not upgrade safe.&lt;/p&gt;
&lt;h2&gt;Second Warning!&lt;/h2&gt;
&lt;p&gt;If you have any custom forwarder monitoring searches/dashboards/alerts they may be impacted.&lt;/p&gt;
&lt;h2&gt;Define an index&lt;/h2&gt;
&lt;p&gt;The index we need to define is _internal_forwarder the following sample configuration will allow us to keep about 3 days of data from our forwarders adjust according to need.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
[_internal_forwarder]
maxWarmDBCount = 200
frozenTimePeriodInSecs = 259200
quarantinePastSecs = 459200
homePath = $SPLUNK_DB/$_index_name/db
coldPath = $SPLUNK_DB/$_index_name/colddb
thawedPath = $SPLUNK_DB/$_index_name/thaweddb
maxHotSpanSecs = 43200
maxHotBuckets = 10&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
Change the index for internal logs
----------------------------------

We need to create a new “TA” named “Splunk\_TA\_splunkforwarder we will CAREFULLY use the DS to push this to forwarders only. DO NOT push this to any Splunk Enterprise instance (CM/LM/MC/SH/IDX/deployer/ds) but you may push this to a “heavy” or intermediate forwarder. The app only needs two files in default app.conf and inputs.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre class=&quot;wp-block-code&quot;&gt;```
#app.conf
[install]
state_change_requires_restart = true
is_configured = 0
state = enabled
build = 2

[launcher]
author = Ryan Faircloth
version = 1.0.0

[ui]
is_visible = 0
label = Splunk_UF Inputs

[package]
id = Splunk_TA_splunkforwarder
```
```

```
&lt;pre class=&quot;wp-block-code&quot;&gt;```
#inputs.conf
[monitor://$SPLUNK_HOME/var/log/splunk]
index = _internal_forwarder
```
```

Check our Work
--------------

First lets check positive make sure UFs have moved to the new index, we should get results.

```
&lt;pre class=&quot;wp-block-code&quot;&gt;```
index=_internal_forwarder source=*splunkforwarder*
```
```

Second lets check the negative make sure only UF logs got moved we should get no results

```
&lt;pre class=&quot;wp-block-code&quot;&gt;```
index= _internal_forwarder source=*splunk* NOT source=*splunkforwarder*
```
```

Updates
-------

- Index definition example used “\_internal” rather than “\_internal\_uf”
- renamed app to “Splunk\_TA\_splunkforwarder
- renamed index to \_internal\_forwarder
</content:encoded></item><item><title><![CDATA[Windows TA 6.0 is out!]]></title><description><![CDATA[Splunk released a major update to the Splunk TA for Windows last month you may not have noticed but I think you should take a closer look. A…]]></description><link>https://rfaircloth.com/2019/03/08/windows-ta-6-0-is-out/</link><guid isPermaLink="false">https://rfaircloth.com/2019/03/08/windows-ta-6-0-is-out/</guid><pubDate>Fri, 08 Mar 2019 15:08:16 GMT</pubDate><content:encoded>&lt;p&gt;Splunk released a major update to the &lt;a href=&quot;https://splunkbase.splunk.com/app/742/&quot;&gt;Splunk TA for Windows&lt;/a&gt; last month you may not have noticed but I think you should take a closer look. A few key things&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplified deployment for new customers Splunk merged the TA for Microsoft DNS and TA for Microsoft AD&lt;/li&gt;
&lt;li&gt;The improved support for “XML” format Windows events from 5.0.1 is now the default in 6.0.0 there is upgrade action to accept this switch. XML events allow for extraction of additional value-able data such as the restart reason from event ID 1074&lt;/li&gt;
&lt;li&gt;Improved CIM compliance for Security events from modern logging channels like Remote Desktop Session&lt;/li&gt;
&lt;li&gt;Improved extensibility its now much easier to add support for third part logging via Windows Event Log&lt;/li&gt;
&lt;li&gt;Improved support for Windows Event forwarding – Note I still strongly discourage this solution for performance, reliability and audit reasons.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are a SecKit for Windows user it is safe to upgrade just follow Splunk’s upgrade instructions. Need some guidance on good practices for Windows data on-boarding to Splunk be sure to checkout &lt;a href=&quot;https://seckit.readthedocs.io/projects/seckit-ta/en/latest/index.html&quot;&gt;SecKit&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;But Change!&lt;/h2&gt;
&lt;p&gt;While this is not a replacement for the upgrade notes you are probably wondering how will this impact my users.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sourcetype changes: Prepare for the upgrade review use of sourcetype=wineventlog:* and replace with an appropriate eventtype OR source= With this TA version we use the source to differentiate between the specific event logs. sourcetype which represents the format of the log becomes a constant regardless of log type. This reduces the memory used in index and search time.&lt;/li&gt;
&lt;li&gt;License impact: XML is bigger, yes but classic has white space and thats not free either and all that static text is gone. In my travels I have not seen much impact if any to license it seems to be a wash&lt;/li&gt;
&lt;li&gt;XML logs are ugly: You are not wrong there. What can I say its Windows&lt;/li&gt;
&lt;li&gt;XML parsing is slower: Yes and no overall the impact of switch from Classic to XML is not much slower. The TA uses regex parsing not “XML”, while you see XML on screen Splunk treats it like normal text. The changes implemented in the prior release (5.0.1) made improvements compared to 4.8.4 if your prior experience relates to this version its worth a second look.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Five things you can do now to get ready for Splunk Smart Store]]></title><description><![CDATA[Splunk’s SmartStore technology is a game changing advancement in data retention for Splunk Enterprise. Allowing Splunk to move least used…]]></description><link>https://rfaircloth.com/2019/03/07/five-things-you-can-do-now-to-get-ready-for-splunk-smart-store/</link><guid isPermaLink="false">https://rfaircloth.com/2019/03/07/five-things-you-can-do-now-to-get-ready-for-splunk-smart-store/</guid><pubDate>Thu, 07 Mar 2019 15:46:56 GMT</pubDate><content:encoded>&lt;p&gt;Splunk’s &lt;a href=&quot;https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore&quot;&gt;SmartStore&lt;/a&gt; technology is a game changing advancement in data retention for Splunk Enterprise. Allowing Splunk to move least used data to an AWS for low cost “colder storage”.&lt;/p&gt;
&lt;h2&gt;Reduce the maximum size of a bucket&lt;/h2&gt;
&lt;p&gt;We will review indexes.conf on the indexer and identify any references to the setting maxDataSize. Common historical practice has been to increase the size of this setting from the default of auto to an arbitrary large value or auto_high_volume. SmartStore is optimized and enforces the use of “auto” or 750mb as the max bucket size. This task should be completed at least 7 days prior cutover to SmartStore.&lt;/p&gt;
&lt;h2&gt;Reduce the maximum span of a bucket&lt;/h2&gt;
&lt;p&gt;We will review indexes.conf and identify all indexes which continuously stream data. Common historical practice to leave this as default value which are very wide this increases the likely a user will retrieve buckets from S3 that do not actually meet their needs. We will determine a value of maxHotSpanSecs that will SmartStore to uncache buckets not used while also keeping buckets available likely to be used. Often 1 day (86400s) is appropriate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the time window a typical search will use for this index relative to now i.e 15 min, 1 day, 3 days, 1 week&lt;/li&gt;
&lt;li&gt;What span of time would allow a set of buckets to contain the events for the user search without excessive “extra” events. For example if the span is 90 days and the users primarily only work with 1 days worth of events therefore 89 days of events will use cache space in a wasteful way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Review Getting Data In problems impacting bucket use&lt;/h2&gt;
&lt;p&gt;Certain oversights in onboard data into Splunk impact both use-ability of data and performance review and resolve any issues identified by the Splunk Monitoring Console page &lt;a href=&quot;https://docs.splunk.com/Documentation/Splunk/7.2.4/DMC/Dataquality&quot;&gt;Data Quality&lt;/a&gt; the most important indicators of concern are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;div class=&quot;li_content&quot;&gt;time stamp extraction&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;li_content&quot;&gt;time zone detection&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;li_content&quot;&gt;indexing latency (`_indextime - _time`)&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One common source of “latency” is events from offline endpoints such as windows laptops. Any endpoint that can spool locally for an undetermined period of time then forward old events should be routed to a index not used for normal streaming events. For example “oswinsec” is the normal index I use for Windows Security Events however for endpoint monitoring I use “oswinsecep”.&lt;/p&gt;
&lt;h2&gt;Review bucket roll behavior&lt;/h2&gt;
&lt;p&gt;After the above activities are done, wait an hour before beginning this work. We should identify pre-mature bucket roll behavior that is buckets rolled from hot to warm regularly for not great reasons. The following search&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;index=_internal source=&quot;/opt/splunk/var/log/splunk/splunkd.log&quot;
component=HotDBManager evicting_count=&quot;*&quot;
| stats max(maxHotBuckets) values(count) as reason count by idx
| sort -count&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This search identifies buckets which are “high volume” and rolling due to lack of an available bucket to index a new event in correct relative order. For each index where the maxHotBuckets is less than 10 increase the value of maxHotBuckets in indexes.conf to no more than 10. For these indexes 10 is a safe value.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building a Splunk CIM compatible source addon]]></title><description><![CDATA[This walk through will build a Splunk CIM compatible source addon extending the CEF source type from my CEF framework TA. This is part three…]]></description><link>https://rfaircloth.com/2019/01/08/building-a-splunk-cim-compatible-source-addon/</link><guid isPermaLink="false">https://rfaircloth.com/2019/01/08/building-a-splunk-cim-compatible-source-addon/</guid><pubDate>Tue, 08 Jan 2019 18:03:20 GMT</pubDate><content:encoded>&lt;p&gt;This walk through will build a Splunk CIM compatible source addon extending the CEF source type from my CEF framework TA. This is part three in a three part Series&lt;/p&gt;
&lt;p&gt;Before you start, I will have to gloss over many topics you should have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the prior two articles in this series.&lt;/li&gt;
&lt;li&gt;You should also be comfortable with the ways Splunk can be used to parse and enrich data notable, TRANSFORMS, REPORT, EXTRACT, EVAL, and LOOKUP. A great cheat sheet is available from &lt;a href=&quot;https://www.aplura.com/wp-content/uploads/2016/09/data_onboarding_cheat_sheet_v2.pdf&quot;&gt;Alpura&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Be familiar with the&lt;a href=&quot;https://docs.splunk.com/Documentation/CIM/4.12.0/User/Web&quot;&gt; web data model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Be familiar with the &lt;a href=&quot;https://docs.incapsula.com/Content/read-more/log-file-structure.htm#Logfields&quot;&gt;data dictionary&lt;/a&gt; for our sample data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the prior two articles we create a project style development environment for our add-on and a minimally viable set of field parse but have not yet considered any specific model. Reviewing our samples and vendor documentation we learn the data is most similar to a web access log which is known as the “web” model in the Splunk Common information model. In our case we have only two events available and vendor documentation that describes the events. When replicating this process in a new data source all unique events should be considered.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All events are a web access event&lt;/li&gt;
&lt;li&gt;A subset of these events are “attack” events which do not have a web model to compare to.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considering this we will use the “web” model as our basis and add a number of connivence elements for our users. The following table illustrates how we will map our data.&lt;/p&gt;
&lt;p&gt;The implementation of the mapping is explained in the following table our implementation of the mapping can be viewed in &lt;a href=&quot;https://bitbucket.org/SPLServices/ta-cef-imperva-incapsula/src/master/src/TA-cef-imperva-incapsula-for-splunk/&quot;&gt;bitbucket&lt;/a&gt;. Review default/props.conf default/transforms.conf and the lookups present in lookups/&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CEF Field&lt;/th&gt;
&lt;th&gt;Splunk Field&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;sip&lt;/td&gt;
&lt;td&gt;dest_ip&lt;/td&gt;
&lt;td&gt;Not a CIM field in the web model used by convention.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not present in sample not validated&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spt&lt;/td&gt;
&lt;td&gt;dest_port&lt;/td&gt;
&lt;td&gt;Not a CIM field in the web model used by convention.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not present in sample not validated&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;qstr&lt;/td&gt;
&lt;td&gt;uri_query&lt;/td&gt;
&lt;td&gt;The formatting of this field contains escaped equal (=) signs and is omitting the leading question mark (?) used a complex eval to adjust&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cs9&lt;/td&gt;
&lt;td&gt;Rule_Name AND signature&lt;/td&gt;
&lt;td&gt;Not a CIM field however Rule_Name is similar to an attack signature. Use an eval to split by comma and remove empties&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Attack Severity&lt;/td&gt;
&lt;td&gt;CEF_severity&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;severity&lt;/td&gt;
&lt;td&gt;This field requires a lookup to set severity as one of low,medium,high,critical created “imperva_incapsula_severity.csv&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;requestmethod&lt;/td&gt;
&lt;td&gt;http_method&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ref&lt;/td&gt;
&lt;td&gt;http_referrer&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;requestClientApplication&lt;/td&gt;
&lt;td&gt;http_user_agent&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dest&lt;/td&gt;
&lt;td&gt;site&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;src_user_id&lt;/td&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;action&lt;/td&gt;
&lt;td&gt;act&lt;/td&gt;
&lt;td&gt;This field requires a lookup to translate act to action which can be allowed or blocked&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vendor_product&lt;/td&gt;
&lt;td&gt;Constant string “Imperva Incapsula”&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app&lt;/td&gt;
&lt;td&gt;vendor_app&lt;/td&gt;
&lt;td&gt;saved for user search not used in the CIM model&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app&lt;/td&gt;
&lt;td&gt;Constant String “incapsula”&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;act&lt;/td&gt;
&lt;td&gt;cached&lt;/td&gt;
&lt;td&gt;Some values of act can indicate cached which is set to true using the actions lookup above&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Creating Eventtypes&lt;/h2&gt;
&lt;p&gt;Fields alone are not enough to include an event in a datamodel. In-fact incorrect configuration of eventtypes and tags and include data which is invalid for a model compromising the usefulness of a model.&lt;/p&gt;
&lt;p&gt;We will create two eventtypes for this data, our implementation can be viewed in eventype.conf using the bitbucket link above:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;eventtype&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;imperva_incapsula_web&lt;/td&gt;
&lt;td&gt;All events matching our source and sourcetype&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;imperva_incapsula_web_attack&lt;/td&gt;
&lt;td&gt;All eventtypes imperva_incapsula_web with a signature field.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Creating Tags&lt;/h2&gt;
&lt;p&gt;The final step to include events in a data model is to tag the events. Additional tags can be created in this case “attack” make sense for the subset of events that indicate a detection by the Incapsula WAF service. Tags which are not used by the data model are not included by default and are only available to the users in search activities.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;tag&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web&lt;/td&gt;
&lt;td&gt;eventtype=imperva_incapsula_web&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;attack&lt;/td&gt;
&lt;td&gt;eventtype=imperva_incapsula_web_attack&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Testing our work&lt;/h2&gt;
&lt;p&gt;Using “make package_test” ensure no unexpected errors or warnings are produced.&lt;/p&gt;
&lt;p&gt;Using our development environment and EventGen via “make docker_dev” we can interactively validate our mapping is CIM compliant.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building a CEF source add on for Splunk Enterprise]]></title><description><![CDATA[In my prior post I walked you through setting up a development environment for Splunk Enterprise to allow for an IDE/RAD development…]]></description><link>https://rfaircloth.com/2019/01/07/building-a-cef-source-add-on-for-splunk-enterprise/</link><guid isPermaLink="false">https://rfaircloth.com/2019/01/07/building-a-cef-source-add-on-for-splunk-enterprise/</guid><pubDate>Tue, 08 Jan 2019 02:06:03 GMT</pubDate><content:encoded>&lt;p&gt;In my prior post I walked you through setting up a development environment for Splunk Enterprise to allow for an IDE/RAD development experience. In this article we are going to walk through creating an add on for Imperva’s Incapsula service the app name will be “&lt;span class=&quot;s1&quot;&gt;ta-cef-imperva-incapsula”. This is a very basic add-on I’ll write another post focusing on data on-boarding and the details that are important. This walkthrough focuses on the fully integrated use of the tools in your development activities.&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a new project locally&lt;/li&gt;
&lt;li&gt;Develop the add-on including an event gen&lt;/li&gt;
&lt;li&gt;Build, package and manually test&lt;/li&gt;
&lt;li&gt;Create a bitbucket project&lt;/li&gt;
&lt;li&gt;Build and package with pipelines for CI/CD&lt;/li&gt;
&lt;li&gt;Publish your docs on “read the docs”&lt;/li&gt;
&lt;li&gt;Publish your app on Splunkbase&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this walk through we will not have time to cover CIM mapping this event source stay tuned for a follow up.&lt;/p&gt;
&lt;h2&gt;Creating a new project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;create a new directory “mkdir ta-cef-imperva-incapsula”&lt;/li&gt;
&lt;li&gt;cd to the directory “cd ta-cef-imperva-incapsula”&lt;/li&gt;
&lt;li&gt;Initialize git with the flow module “git flow init -d”&lt;/li&gt;
&lt;li&gt;Add the build tools submodule this does the heavy lifting of make for us “git submodule add -b master &lt;a href=&quot;https://bitbucket.org/SPLServices/buildtools%E2%80%9D&quot;&gt;https://bitbucket.org/SPLServices/buildtools”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Make a folder for dependencies “mkdir deps”
&lt;ul&gt;
&lt;li&gt;Add eventgen “git submodule add -b master &lt;a href=&quot;https://bitbucket.org/SPLServices/sa-eventgen.git&quot;&gt;https://bitbucket.org/SPLServices/sa-eventgen.git&lt;/a&gt; deps/SA-Eventgen”&lt;/li&gt;
&lt;li&gt;Add our parent TA “git submodule add -b master &lt;a href=&quot;https://bitbucket.org/SPLServices/ta-cef-for-splunk.git&quot;&gt;https://bitbucket.org/SPLServices/ta-cef-for-splunk.git&lt;/a&gt; deps/TA-cef-for-splunk”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Copy the make file “cp buildtools/bootstrap/Makefile .”&lt;/li&gt;
&lt;li&gt;Copy the gitignore file “cp buildtools/bootstrap/.gitignore .gitignore”&lt;/li&gt;
&lt;li&gt;Copy the sample docs “cp -R buildtools/bootstrap/docs .”&lt;/li&gt;
&lt;li&gt;Copy the make config file “cp buildtools/bootstrap/common.mk .”&lt;/li&gt;
&lt;li&gt;Update the common.mk file “MAIN_APP” must be updated at this point the other configuration can be updated later but must be reviewed and editor before release. This value should be the same as the folder name the app will be published in and confirm to the guidance in app.conf.spec package.id for this walk through we will use MAIN_APP=TA-cef-imperva-incapsula-for-splunk&lt;/li&gt;
&lt;li&gt;create the folder src/$MAIN_APP as updated above ie. “mkdir -p src/TA-cef-imperva-incapsula-for-splunk”&lt;/li&gt;
&lt;li&gt;copy the add on template to our working directory “cp -R buildtools/bootstrap/addon/* src/TA-cef-imperva-incapsula-for-splunk/”&lt;/li&gt;
&lt;li&gt;copy the Splunk License “cp buildtools/bootstrap/license-eula.txt .”&lt;/li&gt;
&lt;li&gt;copy the pipelines configuration “cp buildtools/bootstrap/bitbucket-pipelines.yml .”&lt;/li&gt;
&lt;li&gt;Check our work “make package_test” we expect one failure reported “Major.Minor.Revision” this is normal as development builds provide a version number pattern that is SEMVER compliant and is not allowed in Splunk base but is allowed in app.conf.spec&lt;/li&gt;
&lt;li&gt;browse to “out/package/splunkbase and verify the app is packaged&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Developing our Add-on Word of Warning&lt;/h2&gt;
&lt;p&gt;This add-on takes advantage of existing sourcetype definitions in TA-cef-for-splunk in the parent sourcetype the “big 8” props are addressed. If you are following this to build a totally new add-on the best practices for your specific sourcetype should be considered.&lt;/p&gt;
&lt;h2&gt;Creating Samples and eventgen.conf&lt;/h2&gt;
&lt;p&gt;We are using the samples provided by Imperva &lt;a href=&quot;https://docs.incapsula.com/Content/read-more/example-logs.htm#logEx1&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rename the file in src/TA-cef-imperva-incapsula-for-splunk/samples from future.sample to imperva_incapsula.sample&lt;/li&gt;
&lt;li&gt;Replace the sample file contents with the two CEF formatted events below&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
CEF:0|Incapsula|SIEMintegration|1|1|Illegal Resource Access|3| fileid=3412341160002518171 sourceServiceName=site123.abcd.info siteid=1509732 suid=50005477 requestClientApplication=Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0 deviceFacility=mia cs2=true cs2Label=Javascript Support cs3=true cs3Label=CO Support src=12.12.12.12 caIP=13.13.13.13 ccode=IL tag=www.elvis.com cn1=200 in=54 xff=44.44.44.44 cs1=NOT_SUPPORTED cs1Label=Cap Support cs4=c2e72124-0e8a-4dd8-b13b-3da246af3ab2 cs4Label=VID cs5=de3c633ac428e0678f3aac20cf7f239431e54cbb8a17e8302f53653923305e1835a9cd871db32aa4fc7b8a9463366cc4 cs5Label=clappsigdproc=Browser cs6=Firefox cs6Label=clapp ccode=IL cicode=Rehovot cs7=31.8969 cs7Label=latitude cs8=34.8186 cs8Label=longitude Customer=CEFcustomer123 ver=TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 start=1453290121336 request=site123.abcd.info/ requestmethod=GET qstr=p\=%2fetc%2fpasswd app=HTTP act=REQ_CHALLENGE_CAPTCHA deviceExternalID=33411452762204224 cpt=443 filetype=30037,1001, filepermission=2,1, cs9=Block Malicious User,High Risk Resources, cs9Label=Rule name
CEF:0|Incapsula|SIEMintegration|1|1|Normal|0| sourceServiceName=site123.abcd.info siteid=1509732 suid=50005477 requestClientApplication=Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0 deviceFacility=mia src=12.12.12.12 caIP=13.13.13.13 ccode=IL tag=www.elvis.com cicode=Rehovot cs7=31.8969 cs7Label=latitude cs8=34.8186 cs8Label=longitude Customer=CEFcustomer123 ver=TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 start=1453290121336 request=site123.abcd.info/main.css ref=www.incapsula.com/lama requestmethod=GET cn1=200 app=HTTP deviceExternalID=33411452762204224 in=54 xff=44.44.44.44 cpt=443&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
- Create a new file src/TA-cef-imperva-incapsula-for-splunk/default/eventgen.conf this file will replace our samples generated above more advanced config is possible but out of the scope of this tutorial.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre class=&quot;wp-block-code&quot;&gt;```
[imperva_incapsula.sample]
source=/var/syslog/remote/incapsula.log
sourcetype=cef:file
#mode = replay
timeMultiple = 2
backfill = -15m

token.0.token = \d{13}
token.0.replacementType = timestamp
token.0.replacement = %s
```
```

- Create a new file src/TA-cef-imperva-incapsula-for-splunk/default/props.conf use the following content initially we will do more work on this later, right now all we need to do is setup index time transforms

```
&lt;pre class=&quot;wp-block-code&quot;&gt;```
[cef:syslog]
TRANSFORMS-zzTACEFimpervaincapsula = ta_cef_imperva_incapsula_for_splunk_v0_source

[cef:file]
TRANSFORMS-zzTACEFimpervaincapsula = ta_cef_imperva_incapsula_for_splunk_v0_source
```
```

- Create a new file src/TA-cef-imperva-incapsula-for-splunk/default/transforms.conf use the following content initially we will do more work on this later, right now all we need to do is setup index time transforms

```
&lt;pre class=&quot;wp-block-code&quot;&gt;```
[ta_cef_imperva_incapsula_for_splunk_v0_source]
DEST_KEY=MetaData:Source
REGEX = CEF:\d+\|Incapsula\|SIEMintegration\|[^\|]*\|[^\|]*\|[^\|]*\|[^\|]*\|
FORMAT= source::Imperva:Incapsula
```
```

- Check our work so far. use “make docker\_dev” to start a splunk instance and enable event gen. Using search verify records match the search “source=”Imperva:Incapsula” sourcetype=cef”

Creating the first release
--------------------------

We will use git flow to tag and create the first release

- add working files to git “git add .”
- add a comment and checkin git commit -m “Initial work”
- Start the release process “git flow release start 0.1.0”
- edit the version in src/TA-cef-imperva-incapsula-for-splunk/default/app.conf to “0.1.0”
- git add src/TA-cef-imperva-incapsula-for-splunk/default/app.conf
- git commit -m “bump version”
- git flow release finish ‘0.1.0’ #note each comments screen must have some form of comments. “Create release” will do for now
- 

Add our package to bitbucket
----------------------------

I use bitbucket but another vcs such as github will do CI/CD processes will be different and require your own creativity for integration.

- Using your organization’s account create a new repository named “ta-cef-imperva-incapsula” I enable issue tracker and use a public repository
- Follow instructions to “Get your local Git repository on Bitbucket”
- Push your other tags “git push –all –follow-tags”
- Navigate to bitbucket settings
- Select Branching Model
  - Select “develop” for development branch
  - Select “master” for main branch
  - Check each of the boxes and click save (keep defaults)
- Navigate to pipeline/settings
  - Use the toggle to enable
  - Click Configure which should show “Hooray”
- 

Edit our docs
-------------

The documentation uses restructureText in a similar what to the python documentation project. Review and update docs/index.rst view our copy on bitbucket for an up to date [example](https://bitbucket.org/SPLServices/ta-cef-imperva-incapsula/src/master/docs/index.rst).

Publish our docs
----------------

Login to readthedocs.io and following provided instructions connect your bitbucket account to readthedocs.io and publish the docs.

Continue development
--------------------

Continue development to completion a future article may elaborate on how to optimize this source for CIM and enterprise security.

Publish our 1.0.0 version to our VCS
------------------------------------

Once development and testing is complete we are ready to publish 1.0.0.

- Ensure no working files are dirty “git status”
- git flow release start 1.0.0
- edit the version in src/TA-cef-imperva-incapsula-for-splunk/default/app.conf to “0.1.0”
- git add src/TA-cef-imperva-incapsula-for-splunk/default/app.conf
- git commit -m “bump version”
- git flow release finish ‘0.1.0’ #note each comments screen must have some form of comments. “Create release” will do for now
- publish the release to bitbucket “git push –all –follow-tags”
- navigate to bitbucket and select “pipelines” in navigation
- Remember: “develop” builds will fail due to a fatal error reported by appinspect. This is presently normal
- Wait for the master build to complete (success)
- Navigate to downloads and find the “1.0.0” release package and download.

Publish our release to Splunkbase
---------------------------------

With each “release” we can download a “app inspected” package ready for Splunkbase. Follow the on page instructions to publish your app at splunkbase.splunk.com
</content:encoded></item><item><title><![CDATA[Dev Life: Splunk Add-ons like a developer]]></title><description><![CDATA[As a life long (seems that way) software developer come to Splunk I would like to have some of the properties of a Integrated Development…]]></description><link>https://rfaircloth.com/2019/01/07/dev-life-splunk-add-ons-like-a-developer/</link><guid isPermaLink="false">https://rfaircloth.com/2019/01/07/dev-life-splunk-add-ons-like-a-developer/</guid><pubDate>Mon, 07 Jan 2019 21:10:08 GMT</pubDate><content:encoded>&lt;p&gt;As a life long (seems that way) software developer come to Splunk I would like to have some of the properties of a Integrated Development Environment (IDE). This blog post walks you through setting up and experiencing my approach to development for Splunk I wrote a second post in this series creating an actual add-on for Splunk using this toolchain. &lt;a href=&quot;https://www.rfaircloth.com/2019/01/07/building-a-cef-source-add-on-for-splunk-enterprise/&quot;&gt;https://www.rfaircloth.com/2019/01/07/building-a-cef-source-add-on-for-splunk-enterprise/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I can edit “code” i.e splunk conf in my editor and reload the code without restarting&lt;/li&gt;
&lt;li&gt;Every time I build/debug I have a clean environment&lt;/li&gt;
&lt;li&gt;I can run unit tests manually or automatically in a consistent way.&lt;/li&gt;
&lt;li&gt;I can participate in VCS (i.e. git) if desired&lt;/li&gt;
&lt;li&gt;I can consistently reproduce build and packaging including integration into a CI/CD process&lt;/li&gt;
&lt;li&gt;I can leverage dependencies from other developed products.&lt;/li&gt;
&lt;li&gt;Have ready access to common tools like add-on builder and eventgen&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setting up the environment Mac OSX&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;a href=&quot;https://brew.sh/&quot;&gt;Brew&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install LibMagic “brew install libmagic”&lt;/li&gt;
&lt;li&gt;Install python “brew install python”&lt;/li&gt;
&lt;li&gt;Install pandoc “brew install pandoc”&lt;/li&gt;
&lt;li&gt;Install moreutills “brew &lt;strong&gt;install moreutils”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Install jq “brew install jq”&lt;/li&gt;
&lt;li&gt;Install lxml support “xcode-select –install”&lt;/li&gt;
&lt;li&gt;Install git “brew install git”&lt;/li&gt;
&lt;li&gt;Install git flow “brew install git-flow”&lt;/li&gt;
&lt;li&gt;Install gitversion “brew &lt;strong&gt;install&lt;/strong&gt; gitversion”&lt;/li&gt;
&lt;li&gt;Install virtual env for python “sudo pip install virtualenv”&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&quot;https://docs.docker.com/docker-for-mac/install/&quot;&gt;docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create the virtual env “virtualenv ~/venv/splservices”&lt;/li&gt;
&lt;li&gt;Activate the new env “source ~/venv/splservices/bin/activate”&lt;/li&gt;
&lt;li&gt;Install pip “sudo python easy_install pip”&lt;/li&gt;
&lt;li&gt;Install our specific requirements “pip install -r &lt;a href=&quot;https://bitbucket.org/SPLServices/addonbuildimage/raw/master/requirements.txt%E2%80%9D&quot;&gt;https://bitbucket.org/SPLServices/addonbuildimage/raw/master/requirements.txt”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I personally prefer the &lt;a href=&quot;https://atom.io/&quot;&gt;atom editor &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setup the local project&lt;/h2&gt;
&lt;p&gt;For demonstration purposes we are going to work with one of my recent add-ons for Splunk. A full tutorial on git is beyond the scope of this article we will simply clone the repo and start a feature branch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clone the repo “git clone &lt;a href=&quot;https://bitbucket.org/SPLServices/ta-cef-for-splunk.git%E2%80%9D&quot;&gt;https://bitbucket.org/SPLServices/ta-cef-for-splunk.git”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cd into the repo “cd ta-cef-for-splunk”&lt;/li&gt;
&lt;li&gt;Initials git submodules “git submodule init”&lt;/li&gt;
&lt;li&gt;Setup git flow “git flow init -d”&lt;/li&gt;
&lt;li&gt;Start a new feature “git flow feature start myfeature”&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Package and Test&lt;/h2&gt;
&lt;p&gt;Before we change anything we should verify we can recreate a successful build.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build a package “make package”&lt;/li&gt;
&lt;li&gt;Verify the package builds the last line will report something like this, path and version will vary.&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
slim package: [NOTE] Source package exported to &quot;/Users/user/Downloads/ta-cef-for-splunk/out/packages/splunkbase/TA-cef-for-splunk-0.2.0-myfeature.1+17.tar.gz&quot;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
- Test the package using Splunk’s appinspect “make package\_test”
- Verify the test report shows one failure. While developing this one failure is expected which is the version number does not conform to release rules for Splunk Base. Note: per semver.org the feature branch version clearly indicates this is a development build this is helpful in preventing accidental “escapes” to production
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre class=&quot;wp-block-code&quot;&gt;```
splunk-appinspect inspect out/packages/splunkbase/TA-cef-for-splunk-0.2.0-myfeature.1+17.tar.gz --data-format junitxml --output-file test-reports/TA-cef-for-splunk.xml --excluded-tags manual
Validating: TA-cef-for-splunk Version: 0.2.0-myfeature.1+17
.......F.....SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS

A default value of 25 for max-messages will be used.
Splunk app packaging standards These checks validate that a Splunk app has been 
correctly packaged, and can be provided safely for package validation. 
    Check that the extracted Splunk App contains a default/app.conf file 
    that contains an [id] or [launcher] stanza with a version property that is 
    formatted as Major.Minor.Revision. 
        FAILURE: `Major.Minor.Revision` version numbering is required. 
            File: default/app.conf Line Number: 20 


TA-cef-for-splunk Report Summary:

       skipped: 176
       success:  9
  manual_check:  0
       failure:  1
       warning:  0
         error:  0
not_applicable:  3
-------------------
         Total: 189

Please note that more issues could be found out later during the optional manual review process.
```
```

Interactive Development
-----------------------

Now for the good stuff how can we interactively lets fire up a Splunk Docker container with the latest version of Splunk and our local copy of the addon. “make docker\_dev” wait for the text “Ansible playbook complete” to appear on terminal indicating Splunk is ready to work. Visit “http://127.0.0.1:8000” and login to a fresh copy of Splunk with the addon ready to go. The password will be “Changed!11” lets prove life by making a simple change to our addon.

- Open atom or the editor of your choice
- Navigate to &amp;lt;project&amp;gt;/src/TA-cef-for-splunk/default/props.conf
- Add the “EVAL-alive=”yes”” to the \[cef\] stanza
- Return to the running copy of Splunk and visit http://127.0.0.1:8000/debug/refresh/ (click refresh)
- Turn on the event gen “Settings –&amp;gt;Data Inputs –&amp;gt; SA Event-Gen then click enable
- Wait about and minute and click disable
- Go back to search and check for the alive field “index=\* sourcetype=cef | head | table sourcetype,alive”

Further reading
---------------

- Read more about [Event Gen ](https://github.com/splunk/eventgen)
- Read more about [developing](http://dev.splunk.com) for Splunk
- Want to see a fully working CI/CD for Splunk Add ons? Visit the bitbucket repository [https://bitbucket.org/splservices/](https://bitbucket.org/SPLServices/ta-cef-for-splunk/overview)
</content:encoded></item><item><title><![CDATA[On boarding Windows Data for Splunk]]></title><description><![CDATA[Last year I created content to help customers quickly get up and running with Windows Data making optimal use of their license. Splunk TA…]]></description><link>https://rfaircloth.com/2018/10/07/on-boarding-windows-data-for-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2018/10/07/on-boarding-windows-data-for-splunk/</guid><pubDate>Sun, 07 Oct 2018 09:52:13 GMT</pubDate><content:encoded>&lt;p&gt;Last year I created content to help customers quickly get up and running with Windows Data making optimal use of their license. Splunk TA Windows 5.0.1 is out now and I wanted to take the opportunity to update the content and make it more accessible by adding web based documentation. Its now posted on &lt;a href=&quot;https://seckit-ta.readthedocs.io&quot;&gt;read the docs&lt;/a&gt;!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Upgrading  Splunk Add Ons]]></title><description><![CDATA[This topic comes up every now and then working with customers and partners deploying and upgrading add ons for Splunk does not have to be…]]></description><link>https://rfaircloth.com/2018/10/07/upgrading-splunk-add-ons/</link><guid isPermaLink="false">https://rfaircloth.com/2018/10/07/upgrading-splunk-add-ons/</guid><pubDate>Sun, 07 Oct 2018 08:48:31 GMT</pubDate><content:encoded>&lt;p&gt;This topic comes up every now and then working with customers and partners deploying and upgrading add ons for Splunk does not have to be hard there are a few rules to live by. I’m going to use Splunk_TA_Windows 5.0.1 in this walk through. This upgrade has some &lt;a href=&quot;http://docs.splunk.com/Documentation/WindowsAddOn/5.0.1/User/Upgrade&quot;&gt;specific guidance&lt;/a&gt; in addition to the usual steps. As can often be the case with software correcting issues can require additional work for compatibility.&lt;/p&gt;
&lt;h1&gt;Upgrading things to do first&lt;/h1&gt;
&lt;h2&gt;Be proactive read the docs&lt;/h2&gt;
&lt;p&gt;In the release notes Splunk advises that sourcetypes will change. Two new source types “EventLog” and “XMLEventLog” will replace all previous event log specific source types. The source will indicate the specific log used which is consistent with most other source/sourcetypes use in Splunk. Sourcetype is a structure and source is an instance of the structure for a specific host. As instructed review custom searches and eventypes and update to utilize source rather than sourcetype.&lt;/p&gt;
&lt;p&gt;Review the additional changes in the docs determine if any apply to your environment.&lt;/p&gt;
&lt;h2&gt;Review local changes&lt;/h2&gt;
&lt;p&gt;Identify any search time local changes made to the sourcetypes managed by the add-on in question. In most cases these will located in the $SPLUNK_HOME/etc/apps/Splunk_TA_windows/local folder however in some cases you may find them in $SPLUNK_HOME/etc/apps/Splunk_TA_windows/search. Review and compare to the latest version of the add on, confirm they can or should remain at upgrade time.&lt;/p&gt;
&lt;p&gt;Identify any index time local changes made to the sourcetypes managed by the add on in most cases these can be found in the cluster master $SPLUNK_HOME/etc/master-apps/Splunk_TA_windows/local however in some organizations customizations are made in another custom app. If you have inherited this environment be sure to consider how others may have made customizations before you.&lt;/p&gt;
&lt;p&gt;While this post is specific to Splunk_TA_windows most steps do apply to any add-on deployment.&lt;/p&gt;
&lt;h1&gt;Installing the upgrade&lt;/h1&gt;
&lt;p&gt;The first step when possible is to test in a non production environment, in many cases the only complete environment is Production care should be taken and changes should be made in off hours.&lt;/p&gt;
&lt;h2&gt;Search Heads (non clustered)&lt;/h2&gt;
&lt;p&gt;Repeat on each search head&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Backup the current app from by copying Splunk_TA_windows to a safe location&lt;/li&gt;
&lt;li&gt;Install the app using the CLI or app browser “install from file”&lt;/li&gt;
&lt;li&gt;Restart the search head&lt;/li&gt;
&lt;li&gt;Verify any custom dashboards or alert searches continue as expected&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Search Heads (clustered)&lt;/h2&gt;
&lt;p&gt;Repeat on each search head cluster&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Backup the current app from by copying Splunk_TA_windows to a safe location on a search head cluster member&lt;/li&gt;
&lt;li&gt;(ES Only) For a ES Search head cluster only remove the Splunk_TA_Windows from shcluster/apps on the deployer and apply the new bundle to the cluster using the preserve-lookups option as documented in the Enterprise Security documentation&lt;/li&gt;
&lt;li&gt;Verify Splunk_TA_Windows is removed from the peers&lt;/li&gt;
&lt;li&gt;Expand Splunk_TA_Windows into shcluster/apps&lt;/li&gt;
&lt;li&gt;Apply the new bundle using preserve-lookups if ES&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Indexers&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Expand Splunk_TA_Windows in a temporary location, remove the following files&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/bin&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/eventgen.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/inputs.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/wmi.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/indexes.conf&lt;/li&gt;
&lt;li&gt;Splunk has removed all index definitions from this add-on in accordance with best practices and app verification requirements. Review the indexes in use and ensure the indexes have been re-defined according to your environments requirements.&lt;/li&gt;
&lt;li&gt;Verify the organizations indexes.conf contains all required indexes&lt;/li&gt;
&lt;li&gt;Deploy the updated add-on via master-apps for clustered indexers (automatic rolling restart) or to apps on all non clustered indexers and restart.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Intermediate Heavy Forwarders&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Expand Splunk_TA_Windows in a temporary location, remove the following files&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/bin&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/eventgen.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/inputs.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/wmi.conf&lt;/li&gt;
&lt;li&gt;&amp;#x3C;app&gt;/default/indexes.conf&lt;/li&gt;
&lt;li&gt;Deploy to apps on all instances and restart&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Collecting Forwarders using the deployment server&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Review all deployment-apps/*/local/inputs.conf applied to windows systems as follows.&lt;/li&gt;
&lt;li&gt;Ensure index is specified on each utilized input&lt;/li&gt;
&lt;li&gt;Ensure disabled=false is specified on each utilized input&lt;/li&gt;
&lt;li&gt;If no inputs.conf is found “demo defaults” has been utilized up to this point. Copy Splunk_TA_windows/default/inputs to Splunk_TA_windows/local and review stanzas to determine which should remain enabled&lt;/li&gt;
&lt;li&gt;Backup deployment-apps/Splunk_TA_windows to a safe location and remove&lt;/li&gt;
&lt;li&gt;Expand Splunk_TA_windows to deployment-apps.&lt;/li&gt;
&lt;li&gt;Reload the deployment server&lt;/li&gt;
&lt;li&gt;Verify no “missing index” messages appear in the cluster if so identify the incorrectly configured input and redeploy&lt;/li&gt;
&lt;li&gt;Verify no new use of last change or main index if so identify the incorrectly configured input and redeploy.&lt;/li&gt;
&lt;li&gt;Repeat verification of searches and alerts as above.&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Identifying obvious sourcetype problems in Splunk]]></title><description><![CDATA[This is a short one, on boarding data into any system is great making it identifiable and usable by the end users thats even more important…]]></description><link>https://rfaircloth.com/2018/08/23/identifying-obvious-sourcetype-problems-in-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2018/08/23/identifying-obvious-sourcetype-problems-in-splunk/</guid><pubDate>Thu, 23 Aug 2018 14:39:39 GMT</pubDate><content:encoded>&lt;p&gt;This is a short one, on boarding data into any system is great making it identifiable and usable by the end users thats even more important. In Splunk source, sourcetype, and index are the most basic bits of metadata available to users and often they work with only these three because its just so easy. When our upstream sources don’t set these values correctly it can stress the environment because we are doing unnecessary work like “line merging” and our users can’d find data. Using Splunk logs we can see where this may be happening and start to fix it. This search will identify suspect sourcetypes. Review the onboarding of each identified to make it better.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;wp-block-code&quot;&gt;```
index=_internal source=&quot;*metrics.log&quot; sourcetype=splunkd group=per_sourcetype_thruput
| eval sourcetype_error=if(match(series,&quot;^[\$\%\#]&quot;),&quot;__Invalid_char&quot;,sourcetype_error)
| eval sourcetype_error=if(isnull(series) OR st=&quot;&quot; ,&quot;__Invalid_null&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;^\/&quot;),&quot;__Invalid_usedpath&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;^\d+\.\d+\.\d+\.\d+&quot;),&quot;__Invalid_used_IP&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\s&quot;),&quot;__Invalid_space&quot;,sourcetype_error)
| eval sourcetype_error=if(like(series,&quot;%small&quot;),&quot;__Invalid_too_small&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\d+&quot;),&quot;__Invalid_numeric&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\-\d&quot;),&quot;__Invalid_learnednum&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\-error&quot;),&quot;__Invalid_learnederror&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\*&quot;),&quot;__Invalid_asterisk&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;\.\w{1,4}$&quot;),&quot;__Invalid_filename&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;[\.\-]log$&quot;),&quot;__Invalid_autousinglogfilename&quot;,sourcetype_error)
| eval sourcetype_error=if(match(series,&quot;^![\w\_\-\:]+$&quot;),&quot;__Invalid_nonsourcetype_errorndardform&quot;,sourcetype_error)
| search sourcetype_error=*
| stats sum(kb) as kb avg(kbps) as kbps_avg avg(eps) as eps_avg sum(ev) as ev values(sourcetype_error) by series
| eval mb=round(kb/1024,2)
| fields - kb
| sort limit=0 -mb&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
Code as snippet &amp;lt;https://bitbucket.org/snippets/rfaircloth-splunk/Benb45&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Protecting ATMs from the two arm bandits]]></title><description><![CDATA[Just before Y2K and in the years after banking systems moved from proprietary operating systems and applications, custom interfaces and…]]></description><link>https://rfaircloth.com/2018/08/14/protecting-atms-from-the-two-arm-bandits/</link><guid isPermaLink="false">https://rfaircloth.com/2018/08/14/protecting-atms-from-the-two-arm-bandits/</guid><pubDate>Tue, 14 Aug 2018 13:13:46 GMT</pubDate><content:encoded>&lt;figure class=&quot;wp-block-image alignleft&quot;&gt;![Jackpot ATM style](https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2018/08/images.jpeg?resize=259%2C194)&lt;/figure&gt;According to [Krebs](https://krebsonsecurity.com/2018/08/fbi-warns-of-unlimited-atm-cashout-blitz/) two arm bandits are about to hit the jack pot on American ATMS, also known as ABM machines out side of the US. Like most security issues its an arms race, did you know ATM machines have holes in the bottom so crooks can’t fill them with water and blow up the door without damaging the cash? Well they started out solid someone noticed that flaw and exploited it we learned and got better.
&lt;p&gt;Just before Y2K and in the years after banking systems moved from proprietary operating systems and applications, custom interfaces and hardware to Windows based “open” systems with vendor agnostic drivers and tools allowing for innovation and cost reduction. This change swapped out custom controller cards for “USB” devices, Bisync serial for TCP over ethernet, wifi, 4G, PPP. The builders of these new networks didn’t have much experience in network security and left open many many doors. The physical design protects you card number and pin but left the cash open. To keep service costs low the PC components are in a section of the machine called the “hood” and can be serviced without opening the safe and exposing the cash. This is a great design from the perspective of PC service. It also ensure the safety of the repair tech as they can not access the bulk cash there is no reason to rob them at gun point. Great but we still have a problem. The USB and network interfaced are now protected by a 4-6 pin basic lock, all the keys in a region are the same because keeping track of keys are hard. Protecting from a breach from a physical attacker is something the design precludes so we could die on this hill but we can’t take it, what can we do?&lt;/p&gt;
&lt;p&gt;You have Splunk! you also have a remote CCTV system (nvr) or physical alarm what if we pull this data together build a threat model and respond faster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor “motion” events from the NVR system
&lt;ul&gt;
&lt;li&gt;Identify cameras indicating motion front and back of the ATM
&lt;ul&gt;
&lt;li&gt;ATM ID&lt;/li&gt;
&lt;li&gt;Front/Back&lt;/li&gt;
&lt;li&gt;Duration of Motion&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Motion in back of more than n seconds and motion in front of more than x seconds without y duration alert&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Monitor the network switch/wifi
&lt;ul&gt;
&lt;li&gt;map switch/ap events where the port/connection disconnects to the ATM ID&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Monitor the _internal source from the installed UF silence of more than n seconds&lt;/li&gt;
&lt;li&gt;Use the UF to monitor for XFS events via ETW or windows events
&lt;ul&gt;
&lt;li&gt;Hood open&lt;/li&gt;
&lt;li&gt;Dispenser disconnect&lt;/li&gt;
&lt;li&gt;New Device&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install Splunk Stream to monitor TLS/HTTPS aggregate by certificate ID every 5 min. Map src to atm ID alert if the presented cert changes for the Authorization Server&lt;/li&gt;
&lt;li&gt;Using XYGate monitor your Switch (base24/efunds) or SyncSort (Z/OS based custom) monitor for dispenser totals mismatch for the ATM ID&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summarize each of the alerts above using | collect normalizing based on ATM ID. Use Splunk built in alert function to notify ATM OPS and physical security on any occurrence of 3 or more in 15 min, tune for false positives.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Lets Encrypt and get an A for A Great Splunk TLS config]]></title><description><![CDATA[Setting up SSL/TLS on Splunk doesn’t have to be super hard or costly. While running Splunk in cloud providers has many benefits there are…]]></description><link>https://rfaircloth.com/2018/01/17/lets-encrypt-and-get-an-a-for-a-great-splunk-tls-config/</link><guid isPermaLink="false">https://rfaircloth.com/2018/01/17/lets-encrypt-and-get-an-a-for-a-great-splunk-tls-config/</guid><pubDate>Wed, 17 Jan 2018 06:52:23 GMT</pubDate><content:encoded>&lt;p&gt;Setting up SSL/TLS on Splunk doesn’t have to be super hard or costly. While running Splunk in cloud providers has many benefits there are some hassles like provisioning certificates we can better manage using let’s encrypt. This method of installing browser trusted certificates can help to keep your administrative costs down in large Splunk deployments such as MssP services.&lt;/p&gt;
&lt;p&gt;Expanding on prior work &lt;a href=&quot;https://www.splunk.com/blog/2016/08/12/secure-splunk-web-in-five-minutes-using-lets-encrypt.html&quot;&gt;https://www.splunk.com/blog/2016/08/12/secure-splunk-web-in-five-minutes-using-lets-encrypt.html&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;NGINX&lt;/h2&gt;
&lt;p&gt;First we are going to install NGINX we will use this as a front end reverse proxy. Why, we can renew our certs with minimal own time in the future, OCSP stapling (improved page load times) and other things (future posts)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;#centos&lt;/p&gt;
&lt;p&gt;yum install nginx&lt;/p&gt;
&lt;p&gt;#ubuntu&lt;/p&gt;
&lt;p&gt;apt-get install nginx&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Second setup a new vhost for the splunk reverse proxy. Any request to http will be redirected to https except for requests related to certificate management.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;map $uri $redirect_https {&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;/.well-known/&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;0;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;default&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;1;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;server {&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;listen &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;80;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;server_name&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;hf-scan.splunk.example.com;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;root /usr/share/nginx/html;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;if ($redirect_https = 1) {&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;return 301 https://$server_name$request_uri;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;#&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;return &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;301 $scheme://hf-scan.splunk.example.com$request_uri;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;server {&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;listen 443 ssl http2;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;server_name hf-scan.splunk.example.com;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;root /usr/share/nginx/html;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;index index.html index.htm;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;location / {&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;proxy_pass_request_headers on;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;proxy_set_header x-real-IP $remote_addr;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;proxy_set_header host $host;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;proxy_pass &lt;a href=&quot;https://127.0.0.1:8000;&quot;&gt;https://127.0.0.1:8000;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;add_header Strict-Transport-Security “max-age=31536000; includeSubDomains” always;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_certificate &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;/etc/letsencrypt/live/hf-scan.splunk.example.com/fullchain.pem;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_certificate_key /etc/letsencrypt/live/hf-scan.splunk.example.com/privkey.pem;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_protocols &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;TLSv1.2;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_ciphers &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;HIGH:!aNULL:!MD5;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_dhparam /etc/nginx/ssl/dhparam.pem;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_session_cache shared:SSL:50m;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_session_timeout 1d;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_session_tickets off;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_prefer_server_ciphers on;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_stapling on;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;ssl_stapling_verify on;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;resolver 8.8.8.8 8.8.4.4 valid=300s;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;resolver_timeout 5s;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;add_header Strict-Transport-Security “max-age=31536000; includeSubDomains” always;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Setup a deploy hook script this will prepare the cert files as splunk needs them and will also be used on renewal. Save this script as &lt;span class=&quot;s1&quot;&gt;/etc/letsencrypt/renewal-hooks/deploy/splunk.sh&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#!/bin/bash
#deploy to /etc/letsencrypt/renewal-hooks/deploy/splunk.sh
#when requesting a cert add &quot;--deploy-hook /etc/letsencrypt/renewal-hooks/deploy/splunk.sh&quot; to the command
dir=/opt/splunk/etc/auth/ssl
if [[ ! -e $dir ]]; then
    mkdir -p $dir
elif [[ ! -d $dir ]]; then
    echo &quot;$dir already exists but is not a directory&quot; 1&gt;&amp;amp;2
fi
openssl rsa -aes256 -in $RENEWED_LINEAGE/privkey.pem -out $dir/protected.pem -passout pass:password
if [[ ! -f $dir/protected.pem ]]; then
    exit 1
fi
cat $dir/protected.pem $RENEWED_LINEAGE/fullchain.pem &gt; $dir/server.pem
cp $RENEWED_LINEAGE/fullchain.pem $dir/
cp $RENEWED_LINEAGE/privkey.pem $dir/
chown splunk:splunk $dir/*
systemctl restart splunk&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;Request the certificate note correct the webroot folder for your platform and the certificate with the fqdn of your server&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;certbot &lt;span class=&quot;s1&quot;&gt;certonly –webroot -w /var/www/html –hsts -d hf-scan.splunk.example.com &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;–noninteractive –agree-tos –email &lt;a href=&quot;mailto:your@example.com&quot;&gt;your@example.com&lt;/a&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;–deploy-hook /etc/letsencrypt/renewal-hooks/deploy/splunk.sh&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Setup Splunk&lt;/h2&gt;
&lt;p&gt;Update /opt/splunk/etc/system/local/web.conf&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;[settings]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;enableSplunkWebSSL = true&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;#sendStrictTransportSecurityHeader = true&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sslVersions = tls1.2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;cipherSuite = TLSv1.2:!NULL-SHA256:!AES128-SHA256:!ADH-AES128-SHA256:!ADH-AES256-SHA256:!ADH-AES128-GCM-SHA256:!ADH-AES256-GCM-SHA384&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;privKeyPath =&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;/opt/splunk/etc/auth/ssl/privkey.pem&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;caCertPath = /opt/splunk/etc/auth/ssl/fullchain.pem&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Update /opt/splunk/etc/system/local/server.conf&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;[general]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;serverName = hf-scan.splunk.example.com&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;[sslConfig]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sslVersions = tls1.2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sslVersionsForClient = tls1.2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;serverCert = $SPLUNK_HOME/etc/auth/ssl/server.pem&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sslRootCAPath = $SPLUNK_HOME/etc/auth/ssl/fullchain.pem&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;dhFile = /opt/splunk/etc/auth/ssl/dhparam.pem&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sendStrictTransportSecurityHeader = true&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;allowSslCompression = false&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;cipherSuite = TLSv1.2:!NULL-SHA256:!AES128-SHA256:!ADH-AES128-SHA256:!ADH-AES256-SHA256:!ADH-AES128-GCM-SHA256:!ADH-AES256-GCM-SHA384&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;useClientSSLCompression = false&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;useSplunkdClientSSLCompression = false&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Option 1 SSL labs, limited to port 443 (don’t forget about 8089)&lt;/li&gt;
&lt;li&gt;Option 2 testssl.sh CLI based doesn’t share data no letter grade (management likes letters)&lt;/li&gt;
&lt;li&gt;Option 3 High Tech Bridge &lt;a href=&quot;https://www.htbridge.com/ssl&quot;&gt;https://www.htbridge.com/ssl&lt;/a&gt; allows testing multiple ports similar coverage to ssllabs less well known&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Renew certs&lt;/h2&gt;
&lt;p&gt;Setup a cron job to run the following command at least once per week in your scheduled change window. If a certificate renewal is required splunk will be restarted&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;certbot renew –webroot -w /usr/share/nginx/html&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[Can we even patch this Spectre/Meltdown oh and AV also]]></title><description><![CDATA[Isn’t it great when things are in meltdown and you can’t patch yet because your waiting on another patch? Microsoft has stated you can’t…]]></description><link>https://rfaircloth.com/2018/01/09/can-we-even-patch-this-spectre-meltdown-oh-and-av-also/</link><guid isPermaLink="false">https://rfaircloth.com/2018/01/09/can-we-even-patch-this-spectre-meltdown-oh-and-av-also/</guid><pubDate>Tue, 09 Jan 2018 16:04:38 GMT</pubDate><content:encoded>&lt;p&gt;Isn’t it great when things are in meltdown and you can’t patch yet because your waiting on another patch?&lt;/p&gt;
&lt;p&gt;Microsoft has stated you can’t patch until AV goes first&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.zdnet.com/article/windows-meltdown-spectre-fix-how-to-check-if-your-av-is-blocking-microsoft-patch/&quot;&gt;http://www.zdnet.com/article/windows-meltdown-spectre-fix-how-to-check-if-your-av-is-blocking-microsoft-patch/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://support.microsoft.com/en-us/help/4072699/january-3-2018-windows-security-updates-and-antivirus-software&quot;&gt;https://support.microsoft.com/en-us/help/4072699/january-3-2018-windows-security-updates-and-antivirus-software&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bottom line if your AV vendor hasn’t update to set this registry to give the update permissions to install or you don’t use AV and instead use an application whitelist approach for security the patch won’t apply. You can use splunk to track down hosts that will refuse to apply the patch by adding this monitor to splunk and well Splunking the results&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Key=&quot;HKEY_LOCAL_MACHINE&quot; Subkey=&quot;SOFTWARE\Microsoft\Windows\CurrentVersion\QualityCompat&quot; Value=&quot;cadca5fe-87d3-4b96-b7fb-a231484277cc&quot; Type=&quot;REG_DWORD”
&amp;lt;span class=&quot;&quot;&gt;Data=&quot;0x00000000&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Add the following to the inputs.conf applied to all windows system and ensure the server class is set to restart the UF and happy Splunking&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[WinRegMon://HKLMSoftwareMSWindowsQualityCompat]
index = epintel
baseline = 1
disabled = 0
hive = \\REGISTRY\\MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\QualityCompat\\.*
proc = .*
type = delete|create|set|rename&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Tuning Splunk when max concurrent searches are reached]]></title><description><![CDATA[Your searches are queued but you have cores, memory and IO to spare? Tuning your limits can allow Splunk to utilize “more” of your hardware…]]></description><link>https://rfaircloth.com/2017/12/12/tuning-splunk-when-max-concurrent-searches-are-reached/</link><guid isPermaLink="false">https://rfaircloth.com/2017/12/12/tuning-splunk-when-max-concurrent-searches-are-reached/</guid><pubDate>Tue, 12 Dec 2017 16:11:15 GMT</pubDate><content:encoded>&lt;p&gt;Your searches are queued but you have cores, memory and IO to spare? Tuning your limits can allow Splunk to utilize “more” of your hardware when scaled up instances are in use.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warnings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This approach is &lt;strong&gt;NOT&lt;/strong&gt; useful when searches run &lt;strong&gt;LONG.&lt;/strong&gt; If regular searches such as datamodel acceleration, summary and reporting searches are not completing inside of the expected/required time constraints this information could make the symptoms &lt;em&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;strong&gt;worse&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This approach is useful when searches consistently execute faster than the required times for datamodel acceleration, summary and reporting and additional searches are queued while the utilization of cpu, memory, storage IOPS, storage bandwidth are well below the &lt;strong&gt;&lt;em&gt;validated&lt;/em&gt;&lt;/strong&gt; capacity of the infrastructure.&lt;/p&gt;
&lt;h2&gt;Details&lt;/h2&gt;
&lt;p&gt;First in all certain versions of Splunk apply the following setting to disable a feature that can slow search initialization.&lt;/p&gt;
&lt;p&gt;$SPLUNK_HOME/etc/local/limits.conf&lt;/p&gt;
&lt;p&gt;$SPLUNK_HOME/etc/master-apps/_cluster/local/limits.conf&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[search]
#Splunk version &gt;=6.5.0 &amp;lt;6.5.6
#Splunk version &gt;=6.6.0 &amp;lt;6.6.3
#Not required &gt;7.0.0
#SPL-136845 Review future release notes to determine if this can be reverted to auto
max_searches_per_process = 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On the search head only where DMA is utilized (ES) update the following&lt;/p&gt;
&lt;p&gt;$SPLUNK_HOME/etc/local/limits.conf&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#this is useful when you have ad-hoc to spare but are skipping searches (ES I&apos;m looking at you) or other
# home grown or similar things
[scheduler]
max_searches_perc = 75
auto_summary_perc = 100&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Evaluate the load percentage on the search heads and indexers including memory, cpu utilized and memory utilized. We can increase the value of base_max_searches in increments of 10 to allow more concurrent searches per SH until one of the following occurs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU or memory utilization is 60% on IDX or SH&lt;/li&gt;
&lt;li&gt;IOPS or storage throughput hits ceiling and no longer increases decrease the system is fully utilized to prevent failure due to unexpected load decrement the base_max_searches value by 10 and confirm IOPS is no longer constant.&lt;/li&gt;
&lt;li&gt;Skipping /queuing no longer occurs (increase by 1-3 additional units from this point to provide some “head room”&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#limits.conf set SH only
[search]
#base value is 6 increase by 10 until utilization on IDX or SH is at 60% CPU/memory starting with 20
#base_max_searches = TBD&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Outage due to DDOS]]></title><description><![CDATA[The sites been down for a few days, BlueHost has been suffering from a DDOS on at least one of the sites they host. My site shared…]]></description><link>https://rfaircloth.com/2017/07/09/outage-due-to-ddos/</link><guid isPermaLink="false">https://rfaircloth.com/2017/07/09/outage-due-to-ddos/</guid><pubDate>Sun, 09 Jul 2017 16:47:29 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;The sites been down for a few days, BlueHost has been suffering from a DDOS on at least one of the sites they host. My site shared infrastructure. for $3.95 a month I don’t expect too much but having some ability to move sites to new hosts would be nice. Anyways, I’m up on Azure now until I decide if I want to be my own webmaster or revert to paying someone else to pretend to worry about things like that. On the plus side of things, the outage forced me to update the site infrastructure. Now using certificates from Let’s Encrypt. If you have CLI access to your apache hosted site, super easy and free to enable good encryption.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;sudo certbot –apache -d &lt;a href=&quot;http://www.rfaircloth.com&quot;&gt;www.rfaircloth.com&lt;/a&gt; -d rfaircloth.com -d rfaircloth.westus.cloudapp.azure.com –must-staple –redirect &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;–hsts &lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt;–uir –rsa 4096&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[What's in a URL now you can Splunk that]]></title><description><![CDATA[Hunting we find URLs in logs both email and proxy that are interesting all the time. What will that URL return, if it redirects where is it…]]></description><link>https://rfaircloth.com/2017/06/28/whats-url-now-can-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2017/06/28/whats-url-now-can-splunk/</guid><pubDate>Wed, 28 Jun 2017 11:34:56 GMT</pubDate><content:encoded>&lt;p&gt;Hunting we find URLs in logs both email and proxy that are interesting all the time. What will that URL return, if it redirects where is it going and what kind of content questions you might be asking. If you are not asking them now is the time to start. I’ve released a new add on to Splunk Base, a little adaptive response action that can be used with just Splunk Enterprise OR Splunk Enterprise Security to collect and index information about those URLs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://splunkbase.splunk.com/app/3630/&quot;&gt;https://splunkbase.splunk.com/app/3630/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/06/2017-06-23_14-40-29.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/06/2017-06-23_14-40-29-300x124.png?resize=300%2C124&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[How to enable the Alexa Domain list in ES 4.7]]></title><description><![CDATA[This post is short and sweet, in ES 4.7 the Alexa download is not enabled by default enabling and using this list which can be very valuable…]]></description><link>https://rfaircloth.com/2017/06/14/enable-alexa-domain-list-es-4-7/</link><guid isPermaLink="false">https://rfaircloth.com/2017/06/14/enable-alexa-domain-list-es-4-7/</guid><pubDate>Wed, 14 Jun 2017 12:52:42 GMT</pubDate><content:encoded>&lt;p&gt;This post is short and sweet, in ES 4.7 the Alexa download is not enabled by default enabling and using this list which can be very valuable in domain/fqdn based analysis is a simple two step process&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to Enterprise Security –&gt; Configure –&gt; Threat Intelligence Downloads&lt;/li&gt;
&lt;li&gt;Find Alexa&lt;/li&gt;
&lt;li&gt;Click enable&lt;/li&gt;
&lt;li&gt;Navigate to Splunk Settings –&gt; Search Reports and Alerts&lt;/li&gt;
&lt;li&gt;Select “All” from the app drop down&lt;/li&gt;
&lt;li&gt;Search for “&lt;a href=&quot;https://srvsplunkidx.ad.southsideag.com:8000/en-US/manager/SplunkEnterpriseSecuritySuite/saved/searches?app=&amp;#x26;count=10&amp;#x26;offset=0&amp;#x26;itemType=&amp;#x26;owner=&amp;#x26;search=alexa#&quot;&gt;Threat – Alexa Top Sites – Lookup Gen&lt;/a&gt;“&lt;/li&gt;
&lt;li&gt;Click Edit under actions and then enable&lt;/li&gt;
&lt;li&gt;Optional Click Edit under actions again and cron schedule, Set the task to daily execution 03:00 with an auto window. This reduces the chances the list will not be updated if skipped due to search head maintenance.&lt;/li&gt;
&lt;li&gt;Optional the OOB gen search creates a large dispatch directory entry which is not desirable on search head clusters or where disk space is premium such as public clouds. Update the search as follow (appending the stats count) to prevent creation of a result set on the search head | inputthreatlist alexa_top_one_million_sites fieldnames=”rank,domain” | outputlookup alexa_lookup_by_stra &lt;strong&gt;&lt;span style=&quot;color: #000000;&quot;&gt;&lt;em&gt;| stats count&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Click “Run” to build the list so you can have it right now&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Data Streams Fill the data river with events from Splunk]]></title><description><![CDATA[I’ve had this in the bucket for a while waiting for the right time to share. There is a growing demand to develop “real time” analytic…]]></description><link>https://rfaircloth.com/2017/05/15/data-streams-fill-data-river-events-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2017/05/15/data-streams-fill-data-river-events-splunk/</guid><pubDate>Tue, 16 May 2017 01:43:36 GMT</pubDate><content:encoded>&lt;p&gt;I’ve had this in the bucket for a while waiting for the right time to share. There is a growing demand to develop “real time” analytic capability using machine data. Some great things are being created in labs their problem coming out of the lab is generally the inability to get events from the source systems, immediately following by difficulty normalizing events. If you’ve been working with these systems for very long and also worked with Splunk you may share my opinion that the Universal Forwarder, and the Schema at read power of Splunk is simply unmatched. How can we leverage the power of Splunk without reinventing the wheel, the axel, and the engine.&lt;/p&gt;
&lt;p&gt;Credits&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Liu-yuan Lai, Engineer, Splunk &lt;a href=&quot;https://conf.splunk.com/session/2015/conf2015%5C_LYuan%5C_Splunk%5C_BigData%5C_DistributedProcessingwithSpark.pdf&quot;&gt;https://conf.splunk.com/session/2015/conf2015\_LYuan\_Splunk\_BigData\_DistributedProcessingwithSpark.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Splunk App for CEF &lt;a href=&quot;https://splunkbase.splunk.com/app/1847/&quot;&gt;https://splunkbase.splunk.com/app/1847/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Back in 2015 I attended a short conf presentation that introduced me to the concepts and the value of Spark like engines. Last year our new CEF app introduced the idea message distribution can be executed on the indexer allowing very large scale processing with Splunk.&lt;/p&gt;
&lt;p&gt;Introducing Integration Kit (IntKit)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Message Preparation Tools &lt;a href=&quot;https://bitbucket.org/SPLServices/intkit_sa_msgtools&quot;&gt;https://bitbucket.org/SPLServices/intkit_sa_msgtools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kafka Producer &lt;a href=&quot;https://bitbucket.org/SPLServices/intkit_sa_kafkaproducer&quot;&gt;https://bitbucket.org/SPLServices/intkit_sa_kafkaproducer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The solution adds three interesting abilities to Splunk using “summarizing searches” to distribute events via a durable message bus.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Send raw events using durable message queue&lt;/li&gt;
&lt;li&gt;Send reformated events using an arbitrary schema&lt;/li&gt;
&lt;li&gt;Send “Data Model” schema eliminating the need to build parsing logic for each type of source on the receiving side.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But what about other solutions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Syslog Output using the heavy forwarder
&lt;ul&gt;
&lt;li&gt;Syslog is not a reliable delivery protocol unable to resend lost events can cause backup on the UF&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CEF 2.0
&lt;ul&gt;
&lt;li&gt;Great tool limited to single line events or reformating also allows for data loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tools consist of a message formatter currently preparing a _json field, other formats such as xml or csv could be implemented and a producer that will place the message into the kafka queue (other queues can also be implemented)&lt;/p&gt;
&lt;p&gt;example&lt;/p&gt;
&lt;p&gt;[code lang=text]&lt;br&gt;
| datamodel Network_Traffic All_Traffic search&lt;br&gt;
| fields + _raw,All_Traffic.*&lt;br&gt;
| generatejsonmsg suppress_empty=true suppress_unknown=true suppress_stringnull=true output_field=_json&lt;br&gt;
include_metadata=true include_fields=true include_raw=false sort_fields=true sort_mv=true&lt;br&gt;
| ProduceKafkamsgCommand bootstrap_servers=“localhost:9092” topic=“topicname” msgfield=“_json”&lt;br&gt;
| stats count&lt;br&gt;
[/code]&lt;/p&gt;
&lt;p&gt;What does this do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using the datamodel command gather all Network_Traffic events&lt;/li&gt;
&lt;li&gt;Keep only _raw and the data model fields&lt;/li&gt;
&lt;li&gt;generate a _json field containing the fields in json format omit empty strings, “null”, sort the values of mv fields&lt;/li&gt;
&lt;li&gt;Send the message to kafka using a bootstrap server (localhost) topic “topicname”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This project is slightly above science project. That is poorly documented and mostly functional. I expect it will fit in well with the ecosystem its helping. Please submit enhancements to make it better including documentation if you use it.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Using systemd to squash THP and start splunk enterprise]]></title><description><![CDATA[Updated Jan, 16, 2018 user security issue Updated Jan 19,2018 using forking type for splunk Updated Oct 2019 for format issues after…]]></description><link>https://rfaircloth.com/2017/04/28/using-systemd-squash-thp-start-splunk-enterprise/</link><guid isPermaLink="false">https://rfaircloth.com/2017/04/28/using-systemd-squash-thp-start-splunk-enterprise/</guid><pubDate>Fri, 28 Apr 2017 21:00:41 GMT</pubDate><content:encoded>&lt;p&gt;Updated Jan, 16, 2018 user security issue&lt;/p&gt;
&lt;p&gt;Updated Jan 19,2018 using forking type for splunk&lt;/p&gt;
&lt;p&gt;Updated Oct 2019 for format issues after wordpress upgrade&lt;/p&gt;
&lt;h2&gt;Fixing INIT Scripts&lt;/h2&gt;
&lt;p&gt;If you are currently or prefer using init script startup to remain as close to “out of box” configuration as possible be aware of a serious security risk present in the traditional startup method. REF: &lt;a href=&quot;https://www.splunk.com/view/SP-CAAAP3M&quot;&gt;https://www.splunk.com/view/SP-CAAAP3M &lt;/a&gt;To mitigate the issue and address THP/Ulimits consider moving to a field modified version of the script. &lt;a href=&quot;https://bitbucket.org/snippets/rfaircloth-splunk/Gek8My&quot;&gt;https://bitbucket.org/snippets/rfaircloth-splunk/Gek8My&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Going forward using SYSTEMD&lt;/h3&gt;
&lt;p&gt;The concept presented in this post, as well as the original inspiration, have some risks. Using alternatives to the vendor provided init scripts have support risks including loss of the configuration by future upgrades. Each operating system vendor has their own specific guidance on how to do this, each automation vendor has example automation scripts as well. Picking an approach that is appropriate for your environment is up to you.&lt;/p&gt;
&lt;p&gt;THP the bain of performance for so many things in big data is often left on by default and is slightly difficult to disable. As a popular Splunk answers post and Splunk consultants include Marquis have found the best way to ensure ulimit and THP settings are properly configured is to modify the init scripts. This is a really crafty and reliable way to ensure THP is disabled for Splunk, it works on all Linux operating systems regardless of how services are started.&lt;/p&gt;
&lt;p&gt;I’m doing some work with newer operating systems and wanted to explore how systemd really works and changes what is possible in managing a server. Lets face it systemd has not gotten the best of receptions in the community, after all it moved our cheese, toys and the ball all at once. It seems to be here to stay what if we could use its powers for good in relation to Splunk. Let’s put an end to THP and start Splunk the systemd native way.&lt;/p&gt;
&lt;p&gt;Note: the following config file is present for readability and google. Downloadable text file is available &lt;a href=&quot;https://bitbucket.org/snippets/rfaircloth-splunk/ze7rqL&quot;&gt;https://bitbucket.org/snippets/rfaircloth-splunk/ze7rqL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Create the file &lt;code class=&quot;language-text&quot;&gt;/etc/systemd/system/disable-transparent-huge-pages.service&lt;/code&gt;&lt;br&gt;
“&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;[Unit]&amp;lt;br&gt;&amp;lt;/br&gt;Description=Disable Transparent Huge Pages&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[Service]&lt;br&gt;
Type=oneshot&lt;br&gt;
ExecStart=/bin/sh -c “echo never &gt;/sys/kernel/mm/transparent_hugepage/enabled”&lt;br&gt;
ExecStart=/bin/sh -c “echo never &gt;/sys/kernel/mm/transparent_hugepage/defrag”&lt;br&gt;
RemainAfterExit=true&lt;br&gt;
[Install]&lt;br&gt;
WantedBy=multi-user.target&lt;/p&gt;
&lt;p&gt;Verify THP and defrag is presently enabled to avoid a false sense of success&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# cat /sys/kernel/mm/transparent_hugepage/enabled&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[always] madvise never&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# cat /sys/kernel/mm/transparent_hugepage/defrag&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[always] madvise never&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Enable and start the unit to disable THP&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# systemctl enable disable-transparent-huge-pages.service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# systemctl start disable-transparent-huge-pages.service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# cat /sys/kernel/mm/transparent_hugepage/enabled&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;always madvise [never]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# cat /sys/kernel/mm/transparent_hugepage/defrag&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;always madvise [never]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reboot and repeat the verification to ensure the process is enforced&lt;/p&gt;
&lt;p&gt;Note: the following config file is present for readability and google. Downloadable text file is available &lt;a href=&quot;https://bitbucket.org/snippets/rfaircloth-splunk/xe7rqj&quot;&gt;https://bitbucket.org/snippets/rfaircloth-splunk/xe7rqj&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;create the unit file &lt;code class=&quot;language-text&quot;&gt;/etc/systemd/system/splunk.service&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#2018-01-19 Switched to forking indexers with no web port exit differentl than search heads
[Unit]
After=network.target
Wants=network.target
Description=Splunk Enterprise


[Service]
Type=forking
RemainAfterExit=False
User=splunk
Group=splunk

ExecStart=/opt/splunk/bin/splunk start --answer-yes --no-prompt --accept-license
ExecStop=/opt/splunk/bin/splunk stop
PIDFile=/opt/splunk/var/run/splunk/splunkd.pid

Restart=on-failure
TimeoutSec=300

#ulimit -Sn 65535
#ulimit -Hn 65535
LimitNOFILE=65535
#ulimit -Su 20480
#ulimit -Hu 20480
LimitNPROC=20480
#ulimit -Hf unlimited
#ulimit -Sf unlimited
LimitFSIZE=infinity
LimitCORE=infinity
[Install]
WantedBy=multi-user.target&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# systemctl enable splunk.service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# systemctl start splunk.service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Verify the ulimits have been applied via splunk logs&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;#cat /opt/splunk/var/log/splunk/splunkd.log | grep ulimit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Reboot and repeate all verifications&lt;/p&gt;
&lt;p&gt;Bonus material, kill Splunk (lab env only) and watch systemd bring it back&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# killall splunk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;# ps aux | grep splunk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You just noticed splunkd was brought back to up when it died without using systemctl stop. This means using splunk start|stop is not valid when systemd started Splunk.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Splunk OS Data on boarding best practices updated]]></title><description><![CDATA[I’ve updated my best practices a bit and moved the implementation guides from confluence out to the bitbuckets in markdown so they can be…]]></description><link>https://rfaircloth.com/2017/03/14/splunk-os-data-boarding-best-practices-updated/</link><guid isPermaLink="false">https://rfaircloth.com/2017/03/14/splunk-os-data-boarding-best-practices-updated/</guid><pubDate>Tue, 14 Mar 2017 13:32:41 GMT</pubDate><content:encoded>&lt;p&gt;I’ve updated my best practices a bit and moved the implementation guides from confluence out to the bitbuckets in markdown so they can be more easily referenced on any platform or secured environments where PDFs might be discouraged.&lt;/p&gt;
&lt;p&gt;Each repo will contain a README.md and one or more INSTALL.md files with the implementation guides. If you find an issue have a better practice or other enhancement, please open an issue in the repositories tracker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_windows&quot;&gt;Splunk_TA_windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_microsoft_ad&quot;&gt;Splunk_TA_microsoft_ad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_microsoft_dns&quot;&gt;Splunk_TA_microsoft_dns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_microsoft-iis&quot;&gt;Splunk_TA_microsoft-iis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/ta-microsoft-sysmon&quot;&gt;TA-microsoft-sysmon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_microsoft_exchange&quot;&gt;Splunk_TA_microsoft_exchange&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitbucket.org/SPLServices/splunk_ta_nix&quot;&gt;Splunk_TA_nix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Trust but verify your logs might be lieing to you aka, the case for network tap traffic monitoring]]></title><description><![CDATA[I really do “get” it, logging and monitoring can be very costly, we all agree not nearly as costly as a breach. Each organization is…]]></description><link>https://rfaircloth.com/2017/03/08/trust-verify-logs-might-lieing-aka-case-network-tap-traffic-monitoring/</link><guid isPermaLink="false">https://rfaircloth.com/2017/03/08/trust-verify-logs-might-lieing-aka-case-network-tap-traffic-monitoring/</guid><pubDate>Wed, 08 Mar 2017 14:55:18 GMT</pubDate><content:encoded>&lt;p&gt;I really do “get” it, logging and monitoring can be very costly, we all agree not nearly as costly as a breach. Each organization is struggling to ensure they log enough to see detection and value while being good stewards of their company budget. It has been a day reading vault 7 leaks and I see honestly not much that surprises me. I do see something worth a strong restatement, that is an encouragement to rethink what you log and how you log it. The CIA has a very cool (sorry hacker at heart) tool we have known about for some time but have not been able to talk about. Their tool “Drillbit” allows the creation of a covert tunnel using common cisco gear in such a way typical monitoring and logging using IDS and firewalls will not identify. American companies should note criminal gangs and foreign governments certainly have similar capabilities. Splunk has your back if you are willing to let us. Using &lt;a href=&quot;https://splunkbase.splunk.com/app/1809/&quot;&gt;Splunk Stream&lt;/a&gt; and proper sensor placement we can collect data from the inside and outside of your firewall that can be used to identify covert tunnels. Detection should be performed three approaches. The danger these leaks are presenting you is an increased awareness of the effectiveness of these techniques, encouraging the advancement of commodity cybercrime toolkits with ever more difficult to detect features. Don’t use cisco, sorry bad news is almost every major gear vendor has been exploited with similar approaches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Static Rules such as
&lt;ul&gt;
&lt;li&gt;This not that “Stream identified traffic not in firewall logs”&lt;/li&gt;
&lt;li&gt;New patterns in DNS, NTP, GRE flows&lt;/li&gt;
&lt;li&gt;Change/login to firewall or switch not associated with a change record&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Threat List Enrichment and detection
&lt;ul&gt;
&lt;li&gt;Source and Destination traffic on quality threat lists. Traffic for protocols other than http(s) and DNS should be treated with high or critical priority&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Machine Learning
&lt;ul&gt;
&lt;li&gt;Anomalous egress traffic by source from network devices&lt;/li&gt;
&lt;li&gt;Anomalous admin connections by source to network devices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Automating Splunk deployment for RedHat/Centos (poor man's edition)]]></title><description><![CDATA[I pulled this out of the archives , on request notice this was originally developed for Splunk 6.2.x and RHEL 7.0. Please review the details…]]></description><link>https://rfaircloth.com/2017/03/07/automating-splunk-deployment-redhatcentos-poor-mans-edition/</link><guid isPermaLink="false">https://rfaircloth.com/2017/03/07/automating-splunk-deployment-redhatcentos-poor-mans-edition/</guid><pubDate>Tue, 07 Mar 2017 23:24:44 GMT</pubDate><content:encoded>&lt;p&gt;I pulled this out of the archives , on request notice this was originally developed for Splunk 6.2.x and RHEL 7.0. Please review the details make sure it is suitable for you and TEST. If I can talk you out of doing things this way I would. Salt is a great way to manage app config its free and just awesome.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;bp-text bp-text-plain hljs&quot;&gt;```
Title: Splunk Universal Forwarder Version 6.2.3+ Red Hat Enterprise Linux 7

Author: Ryan Faircloth

Summary: Using repositories for version managment of the Splunk Universal Forwarder assists in ensuring managed Red Hat and compatible linux  systems are using the approved version of the software at all times.

[TOC]

## Setup the repository server ##
1. Install createrepo  and nginx ``` yum install createrepo apache2 ```
3. Create a user to work with the repository

 ```
 sudo adduser  repouser
 ```
3. Change user to our repouser user all commands for the repository should be executed using this ID

  ```
  sudo su - repouser
  ```

## Generate GPG Keys ##
1. Change user to our repouser user all commands for the repository should be executed using this ID

 ```
 sudo su - repouser
 ```
2. Create the default configuration for gpg by running the command

 ```
 gpg --list-keys
 ```
3. Edit ~/.gnupg/gpg.conf
 * uncomment the line ``` no-greeting ```
 * add the following content to the end of the file

 ```
 # Prioritize stronger algorithms for new keys.
 default-preference-list SHA512 SHA384 SHA256 SHA224 AES256 AES192 AES CAST5 BZIP2 ZLIB ZIP UNCOMPRESSED
 # Use a stronger digest than the default SHA1 for certifications.
 cert-digest-algo SHA512
 ```

4. Generate a new key with the command ``` gpg --gen-key ```
5. Select the folowing options ON CENTOS/RHEL this procedure must be executed on the console or SSH having logged in as the repouser
 1. Type of key &quot;(1) RSA and RSA (default)&quot;
 2. Key size &quot;4096&quot;
 3. Expires &quot;10y&quot;
 4. Confirm &quot;Y&quot;
 5. Real Name &quot;Splunk local repository&quot;
 6. Email address on repository contact this generally should be an alias or distribution list
 7. Leave the comment blank
 8. Confirm and &quot;O&quot; to Okay
 9. Leave passphrase blank and confirm, a key will be generated not the sub KEY ID in the following example * E507D48E *

 ```
 gpg: checking the trustdb
 gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model
 gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u
 gpg: next trustdb check due at 2025-05-24
 pub   4096R/410E1699 2015-05-27 [expires: 2025-05-24]
       Key fingerprint = 7CB8 81A9 E07F DA7B 83FF  2E1B 8B31 DA83 410E 1699
 uid                  Splunk local repository &amp;lt;repo@example.com&gt;
 sub   4096R/E507D48E 2015-05-27 [expires: 2025-05-24]
 ```
10. Export the signing keys public component save this content for use later

 ```
 gpg --export --armor KEY_ID &gt;~/repo.pub
 ```
11. Install the new key into the RPM database
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;sudo cp ~/repo.pub /etc/pki/rpm-gpg/RPM-GPG-KEY-splunkrepo
sudo rpm —import /etc/pki/rpm-gpg/RPM-GPG-KEY-splunkrepo&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;12. Configure RPM signing with the new key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;echo ”%_signature gpg” &gt; ~/.rpmmacros
echo ”%_gpg_name splunkrepo” &gt;&gt; ~/.rpmmacros
“&lt;/p&gt;
&lt;ol start=&quot;13&quot;&gt;
&lt;li&gt;Create a repository&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;mkdir /opt/splunkrepo
cp splunkforwarder*.rpm /opt/splunkrepo
createrepo /opt/splunkrepo&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;14&quot;&gt;
&lt;li&gt;Configure the local repository create the following configuration /etc/yum.repos.d/splunk.repo&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[splunkrepo]
name=splunk repository
baseurl=file:///opt/splunkrepo/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-splunkrepo
enabled=1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;15&quot;&gt;
&lt;li&gt;Test the local repository by installing splunkforwarder&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;sudo yum update
sudo yum install splunkforwader&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note: Create a configuration RPM refer to &lt;a href=&quot;https://fedoraproject.org/wiki/How_to_create_an_RPM_package&quot;&gt;https://fedoraproject.org/wiki/How_to_create_an_RPM_package&lt;/a&gt; and &lt;a href=&quot;https://www.redhat.com/promo/summit/2010/presentations/summit/opensource-for-it-leaders/thurs/pwaterma-2-rpm/RPM-ifying-System-Configurations.pdf&quot;&gt;https://www.redhat.com/promo/summit/2010/presentations/summit/opensource-for-it-leaders/thurs/pwaterma-2-rpm/RPM-ifying-System-Configurations.pdf&lt;/a&gt; for more information do not run as root sudo to repouser 17. Prepare the rpm tree &lt;code class=&quot;language-text&quot;&gt;rpmdev-setuptree&lt;/code&gt; 18. Create a spec file with the following content ~/splunkforwarder-baseconfig.spec&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#--------------------------------------------------------------------------
# This spec file is Copyright 2010, My Company, Inc.
#--------------------------------------------------------------------------
Summary: My Company general configuration RPM
Name: splunkforwarder-baseconfig
Version: 1
Release: 3
License: Copyright 2010, My Company, Inc.
Group: MyCompany/Configs
Packager: Packager Name &amp;lt;my-email@mycompany.com&gt;
requires: splunkforwarder
BuildArch: noarch
%description
This RPM provides general services and security configuration for My Company.

%triggerin -- splunkforwarder
/opt/splunkforwarder/bin/splunk enable boot-start --accept-license --answer-yes
service splunk stop
if [ -d &quot;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local&quot; ]
then
    echo &quot;Directory /opt/splunkforwarder/etc/apps/org_all_deploymentclient/local exists.&quot;
else
    mkdir -p /opt/splunkforwarder/etc/apps/org_all_deploymentclient/local
fi
echo #Base deployment configuration &gt;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local/deploymentclient.conf
echo [deployment-client] &gt;&gt;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local/deploymentclient.conf
#echoclientName  &gt;&gt;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local/deploymentclient.conf
echo [deployment-client] &gt;&gt;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local/deploymentclient.conf
echo targetUri = ds.example.com:8089 &gt;&gt;/opt/splunkforwarder/etc/apps/org_all_deploymentclient/local/deploymentclient.conf

service splunk start


%triggerun -- splunkforwarder
if [ $1 -eq 0 -a $2 -gt 0 ] ; then
 /opt/splunkforwarder/bin/splunk stop
 /opt/splunkforwarder/bin/splunk disable boot-start
 rm -Rf /opt/splunkforwarder/etc/apps/org_all_deploymentclient
fi
%files
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;18&quot;&gt;
&lt;li&gt;Build the RPM &lt;code class=&quot;language-text&quot;&gt;rpmbuild -sign -ba splunkforwarder-baseconfig.spec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy the RPM to the repository cp ~/rpmbuild/RPMS/noarch/splunkforwarder-baseconfig-1-3.noarch.rpm / /opt/splunkrepo&lt;/li&gt;
&lt;li&gt;Update repository DB &lt;code class=&quot;language-text&quot;&gt;createrepo /opt/splunkrepo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Test the rpms&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;yum update
yum install splunkforwarder-baseconfig&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;22&quot;&gt;
&lt;li&gt;Configure a web server (Apache) for use as a repository server&lt;/li&gt;
&lt;li&gt;Set permissions on the repository folder &lt;code class=&quot;language-text&quot;&gt;chmod -R 755 /opt/splunkrepo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create the web server configuration file with the following contents /etc/http/conf.d/splunkrepo.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Alias /splunkrepo/ &quot;/opt/splunkrepo/&quot;

&amp;lt;Directory &quot;/opt/splunkrepo&quot;&gt;
   Options Indexes FollowSymLinks MultiViews
   AllowOverride All
   Require local
   Order allow,deny
   Allow from all
&amp;lt;/Directory&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;25&quot;&gt;
&lt;li&gt;Reload (or restart) the web server &lt;code class=&quot;language-text&quot;&gt;service httpd reload&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Test “ lynx &lt;a href=&quot;http://localhost/splunkrepo/repodata/repomd.xml&quot;&gt;http://localhost/splunkrepo/repodata/repomd.xml&lt;/a&gt; ```&lt;/li&gt;
&lt;li&gt;Enable the new repository on the first test client&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;sudo yum-config-manager --add-repo http://localhost/splunkrepo
sudo yum update
sudo yum install splunkforwarder-baseconfig&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;bp-popup-modal&quot;&gt;&lt;/div&gt;&lt;div class=&quot;bp-controls-wrapper&quot;&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Splunk the server and the enpoint aka All the Things]]></title><description><![CDATA[I’m sharing something today that has been available thanks to many in white papers and presentations dealing with identification of…]]></description><link>https://rfaircloth.com/2017/03/01/splunk-server-enpoint-aka-things/</link><guid isPermaLink="false">https://rfaircloth.com/2017/03/01/splunk-server-enpoint-aka-things/</guid><pubDate>Wed, 01 Mar 2017 11:38:28 GMT</pubDate><content:encoded>&lt;p&gt;I’m sharing something today that has been available thanks to many in white papers and presentations dealing with identification of malicious code and activities in your windows event data. Shout out to everyone from our &lt;a href=&quot;https://www.iad.gov/iad/library/reports/spotting-the-adversary-with-windows-event-log-monitoring.cfm&quot;&gt;“friends” at the NSA,&lt;/a&gt; to Splunk .Conf presenters and &lt;a href=&quot;https://www.malwarearchaeology.com&quot;&gt;malwarearcheology.com&lt;/a&gt; just to name a few.&lt;/p&gt;
&lt;p&gt;The PDF attached is a portion of the next evolution of the Use Case Repository I maintain at Splunk. Along with the reference TAs and inputs, this will allow you to quickly and consistently collect very valuable data supporting security use cases at multiple levels of maturity. If it seems like too much don’t work Splunk Pro Services and partners are able to help you get this visibility just contact your account team.&lt;/p&gt;
&lt;p&gt;Standard disclaimer, this is a blog post, I built the content from public non-warrantied information, and this is still public non-warrantied information, your situation might not match the advice given.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.rfaircloth.com/wp-content/uploads/2017/03/PT005-Microsoft-Windows.pdf&quot;&gt;PT005-Microsoft-Windows&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Good Assets and Identities make for big bundles]]></title><description><![CDATA[Having great and informative data will make for some hefty lookups. I’ve heard from a few customers that run into this rather than plan for…]]></description><link>https://rfaircloth.com/2017/02/28/good-assets-identities-make-big-bundles/</link><guid isPermaLink="false">https://rfaircloth.com/2017/02/28/good-assets-identities-make-big-bundles/</guid><pubDate>Tue, 28 Feb 2017 17:26:00 GMT</pubDate><content:encoded>&lt;p&gt;Having great and informative data will make for some hefty lookups. I’ve heard from a few customers that run into this rather than plan for it so let us talk about the levers we need to pull.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don’t wait around upgrade to Splunk Enterprise 6.5.2+ Now is the time&lt;/li&gt;
&lt;li&gt;Don’t wait any longer upgrade to Splunk Enterprise Security 4.5.1 the dev team invested in improvements to assets and identities lookups that also improve by decreasing the size of the merged lookups.&lt;/li&gt;
&lt;li&gt;Update server.conf on the indexers and search head cluster peers.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;span class=&quot;pun&quot;&gt;[&amp;lt;/span&gt;&amp;lt;span class=&quot;pln&quot;&gt;httpServer&amp;lt;/span&gt;&amp;lt;span class=&quot;pun&quot;&gt;]&amp;lt;/span&gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;pln&quot; style=&quot;font-family: Consolas, Monaco, &apos;Lucida Console&apos;, monospace; font-size: 0.857143rem;&quot;&gt;max_content_length &lt;/span&gt;&lt;span class=&quot;pun&quot; style=&quot;font-family: Consolas, Monaco, &apos;Lucida Console&apos;, monospace; font-size: 0.857143rem;&quot;&gt;= 1610612736 # 1.5 GB&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Update distsearch.conf to better replication on the SH/SHC&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;mw-collapsed&quot;&gt;[replicationSettings]
# 1.5 GB with encoding room this will increase the memory utilization while decreasing CPU utilization
maxMemoryBundleSize = 1700
#1.5 GB to match server.conf on the other side
maxBundleSize = 1536&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[RSYSLOG read the docs]]></title><description><![CDATA[Ok, I said posts in threes so here it is. We all know RYSLOG config is much more painful than syslog-ng but for reasons beyond all of our…]]></description><link>https://rfaircloth.com/2017/02/16/rsyslog-read-docs/</link><guid isPermaLink="false">https://rfaircloth.com/2017/02/16/rsyslog-read-docs/</guid><pubDate>Fri, 17 Feb 2017 00:09:25 GMT</pubDate><content:encoded>&lt;p&gt;Ok, I said posts in threes so here it is. We all know RYSLOG config is much more painful than syslog-ng but for reasons beyond all of our control, it is readily available for more customers than syslog-ng is today. Thanks to Splunk users I want to share a couple links to better doc to make this not so awful&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RedHat &lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red%5C_Hat%5C_Enterprise%5C_Linux/7/html/System%5C_Administrators%5C_Guide/s1-basic%5C_configuration%5C_of%5C_rsyslog.html&quot;&gt;https://access.redhat.com/documentation/en-US/Red\_Hat\_Enterprise\_Linux/7/html/System\_Administrators\_Guide/s1-basic\_configuration\_of\_rsyslog.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Usenix &lt;a href=&quot;https://www.usenix.org/system/files/login/articles/06%5C_lang-online.pdf&quot;&gt;https://www.usenix.org/system/files/login/articles/06\_lang-online.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;thank you @mattymo and @lowell via Splunk Slack chat&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Unbelievably simple (ipfix|(net|j|s)flow) collection]]></title><description><![CDATA[Do blog posts come in threes, keep watching to find out? Yesterday I gave you the run down on a new way to collect syslog. Today I’m going…]]></description><link>https://rfaircloth.com/2017/02/11/unbelievably-simple-ipfixnetjsflow-collection/</link><guid isPermaLink="false">https://rfaircloth.com/2017/02/11/unbelievably-simple-ipfixnetjsflow-collection/</guid><pubDate>Sat, 11 Feb 2017 16:57:12 GMT</pubDate><content:encoded>&lt;p&gt;Do blog posts come in threes, keep watching to find out? Yesterday I gave you the run down on a new way to collect &lt;a href=&quot;http://www.rfaircloth.com/2017/02/10/building-perfect-syslog-collection-infrastructure/&quot;&gt;syslog&lt;/a&gt;. Today I’m going to spend some time on a simple low cost and performant way to collect flow data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At least two indexers with http event collector, more = better. For this use case it is not appropriate to utilize dedicated HEC servers.&lt;/li&gt;
&lt;li&gt;One http load balancer, I use HA proxy. You can certainly use the same one from our rsyslog configuration.&lt;/li&gt;
&lt;li&gt;Optional one UDP load balancer such as NGNIX. I am not documenting this setup at this time.&lt;/li&gt;
&lt;li&gt;One ubuntu 16.04 VM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basic Setup&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Follow docs, to setup HTTP event collector on your indexers, note if your indexers are clustered docs does not cover this, you must create the configuration manually be sure to generate a unique GUID manually. Clusters environments can use the sample configuration below: IMPORTANT ensure your data indexes AND _internal are allowed for the token&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;[http] &amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;disabled=0&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;port=8088&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;#&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;[http://streamfwd]&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;disabled=0&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;index=main&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;token=DAA61EE1-F8B2-4DB1-9159-6D7AA5220B21&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;indexes=_internal,main&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;3. Follow documentation for your load balancer of choice to create a http VIP with https back end servers. HEC listens on 8088 by default.
4. Install stream for the independent per [Docs](http://docs.splunk.com/Documentation/StreamApp/7.0.1/DeployStreamApp/InstallStreamForwarderonindependentmachine)
5. Kill stream if its running “killall -9 streamfwd”
6. Remove the init script
1. “**update**&amp;lt;span class=&quot;s2&quot;&gt;-rc.d -f streamfwd remove”&amp;lt;/span&gt;
2. rm /etc/init.d/streamfwd
7. Create a new service unit file for systemd /etc/systemd/system/streamfwd.service ```
&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;[Unit]&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;Description= Splunk Stream Dedicated Service&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;After=syslog.target network.target&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;[Service]&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;Type=simple&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;ExecStart=/opt/streamfwd/bin/streamfwd -D&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;
&lt;p&gt;Enable the new service “systemctl enable streamfwd”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create/update the streamfwd.conf replacing GUID VIP and INTERFACE&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;[streamfwd]&amp;lt;/span&gt;

&amp;lt;span class=&quot;s1&quot;&gt;httpEventCollectorToken = &amp;lt;GUID&gt;&amp;lt;/span&gt;

&amp;lt;span class=&quot;s1&quot;&gt;indexer.0.uri= &amp;lt;HEC VIP&gt;&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;netflowReceiver.0.ip = &amp;lt;INTERFACE TO BIND&gt;&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;netflowReceiver.0.port = 9995&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;netflowReceiver.0.decoder = netflow&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create/update the inputs.conf ensure the URL is correct for the location of your stream app&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;[streamfwd://streamfwd]&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;splunk_stream_app_location = &lt;a href=&quot;https://192.168.100.62:8000/en-us/custom/splunk_app_stream/&quot;&gt;https://192.168.100.62:8000/en-us/custom/splunk_app_stream/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;s1&quot;&gt;stream_forwarder_id=infra_netflow&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;12. Start the streamfwd “systemctl start streamfwd”
13. Login to the search head where Splunk App for Stream is Installed
14. Navigate to Splunk App for Stream –&amp;amp;gt; Configuration –&amp;amp;gt; Distributed Forwarder Managment
15. Click Create New Group
16. Enter Name as “INFRA\_NETFLOW”
17. Enter a Description
18. Click Next
19. Enter “INFRA\_NETFLOW” as the rule and click next
20. Click Finish without selecting options
21. Navigate to Splunk App for Stream –&amp;amp;gt; Configuration –&amp;amp;gt; Configure Streams
22. Click New Stream select netflow as the protocol (this is correct for netflow/sflow/jflow/ipfix
23. Enter Name as “INFRA\_NETFLOW”
24. Enter a Description and click next
25. No Aggregation and click next
26. Deselect any fields NOT interesting for your use case and click next
27. Optional develop filters to reduce noise from high traffic devices and click next
28. Select the index for this collection and click enable then click next
29. Select only the Infra\_netflow group and Create\_Stream
30. Configure your NETFLOW generator to send records to the new streamfwd

Validation! search the index configured in step 27&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Building a more perfect Syslog Collection Infrastructure]]></title><description><![CDATA[A little while back I created a bit of code to help get data from linux systems in real time where the Splunk Universal Forwarder could not…]]></description><link>https://rfaircloth.com/2017/02/10/building-perfect-syslog-collection-infrastructure/</link><guid isPermaLink="false">https://rfaircloth.com/2017/02/10/building-perfect-syslog-collection-infrastructure/</guid><pubDate>Fri, 10 Feb 2017 15:48:20 GMT</pubDate><content:encoded>&lt;p&gt;A little while back I created a bit of &lt;a href=&quot;http://www.rfaircloth.com/2016/05/16/building-high-performance-low-latency-rsyslog-splunk/&quot;&gt;code&lt;/a&gt; to help get data from linux systems in real time where the Splunk Universal Forwarder could not be installed. At the time we had a few limitations the biggest problem being time stamps were never parsed only “current” time on the indexer could be used. Want to try out version 2 lets get started! First let me explain what we are doing&lt;/p&gt;
&lt;p&gt;If you manage a Splunk environment with high rate sources such as a Palo Alto firewall or Web Proxy you will notice that events are not evenly distributed over the indexers because the the data is not evenly balanced across your aggregation tier. The reasons for this are boiled down to “time based load balancing” in Larger environments the universal forwarder may not be able to split by time to distribute a high load. So what is an admin to do? Lets look for a connection load balancing solution. We need to find a way to switch from “SYSLOG” to HTTP(s) so we can utilize a proper load balancer. How will we do this?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using containers we will dedicate one or more instance of RSYSLOG for each “type” of data,&lt;/li&gt;
&lt;li&gt;Use a custom plugin to package and forward batches of events over http(s)&lt;/li&gt;
&lt;li&gt;Use a load balancer configured for least connected round robin to balance the batches of events&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/02/syslog.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/02/syslog.png?resize=677%2C456&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At least two indexers with http event collector, more = better. The “benefits” of this solution require collection on the indexer dedicated collectors will not be a adequate substitute&lt;/li&gt;
&lt;li&gt;One load balancer, I use HA Proxy&lt;/li&gt;
&lt;li&gt;One syslog collection server with rsyslog 8.24+ host I use LXC instances hosted on proxmox. Optimal deployment will utilize 1 collector per source technology. For example 1 instance collecting for Cisco IOS and another for Palo Alto Firewalls. Using advanced configuration and filters you can combine several low volume source.&lt;/li&gt;
&lt;li&gt;A GUID if you need one generated there are many ways this one is quick and easy &lt;a href=&quot;https://www.guidgenerator.com/online-guid-generator.aspx&quot;&gt;https://www.guidgenerator.com/online-guid-generator.aspx&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basic Setup&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Follow docs, to setup HTTP event collector on your indexers, note if your indexers are clustered docs does not cover this, you must create the configuration manually be sure to generate a unique GUID manually. Clusters environments can use the sample configuration below:&lt;/li&gt;
&lt;li&gt;Follow documentation for your load balancer of choice to create a http VIP with https back end servers. HEC listens on 8088 by default&lt;/li&gt;
&lt;li&gt;Grab the code and configuration examples from &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src&quot;&gt;bitbucket&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deploy the script &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src/445676ad128d8ca5de3b573c55450ecc13b3dd88/omsplunkhec.py?at=master&quot; title=&quot;omsplunkhec.py&quot;&gt;omsplunkhec.py&lt;/a&gt; to /opt/rsyslog/ ensure the script is executable&lt;/li&gt;
&lt;li&gt;Review &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src/445676ad128d8ca5de3b573c55450ecc13b3dd88/rsyslogd.d.conf.example?at=master&quot; title=&quot;rsyslogd.d.conf.example&quot;&gt;rsyslogd.d.conf.example&lt;/a&gt; and your configuration in /etc/rsyslog.d/00-splunkhec.conf replace the GUID and IP with your correct values&lt;/li&gt;
&lt;li&gt;Restart rsyslog&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What to expect, My hope data balance Zen.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/02/chart.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2017/02/chart.png?resize=1100%2C243&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;HTTP Event Collector inputs.conf example deployed via master-apps&lt;/h2&gt;
&lt;blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;[http] &amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;disabled=0&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;port=8088&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;#&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;[http://SM_rsyslog_routerboard]&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;disabled=0&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;index=main&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;token=DAA61EE1-F8B2-4DB1-9159-6D7AA5220B21&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;indexes=main,summary&amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Example /etc/rsyslog.d/00-splunk.conf&lt;/h2&gt;
&lt;p&gt;This example will listen on 514 TCP and UDP sending events via http, be sure to replace the GUID and ip address&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;module(load=&quot;imudp&quot;)
input(type=&quot;imudp&quot; port=&quot;514&quot; ruleset=&quot;default_file&quot;)
module(load=&quot;imptcp&quot;)
input(type=&quot;imptcp&quot; port=&quot;514&quot; ruleset=&quot;default_file&quot;)
module(load=&quot;omprog&quot;)

ruleset(name=&quot;default_file&quot;){
    $RulesetCreateMainQueue
    action(type=&quot;omprog&quot;
       binary=&quot;/opt/rsyslog/omsplunkhec.py DAA61EE1-F8B2-4DB1-9159-6D7AA5220B21 192.168.100.70 --sourcetype=syslog --index=main&quot;
       template=&quot;RSYSLOG_TraditionalFileFormat&quot;)
    stop
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Example HAProxy Configuration 1.7 /etc/haproxy/haproxy.cfg&lt;/h2&gt;
&lt;blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;pre class=&quot;p1&quot;&gt;&amp;lt;span class=&quot;s1&quot;&gt;global&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;log /dev/log&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;local0&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;log /dev/log&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;local1 notice&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;chroot /var/lib/haproxy&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats socket /run/haproxy/admin.sock mode 660 level admin&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats timeout 30s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;user haproxy&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;group haproxy&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;daemon&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;# Default SSL material locations&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;ca-base /etc/ssl/certs&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;crt-base /etc/ssl/private&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;# Default ciphers to use on SSL-enabled listening sockets.&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;# For more information, see ciphers(1SSL).&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;ssl-default-bind-ciphers kEECDH+aRSA+AES:kRSA+AES:+AES256:RC4-SHA:!kEDH:!LOW:!EXP:!MD5:!aNULL:!eNULL&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;defaults&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;log &amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;global&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;mode&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;option&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;httplog&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;option&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;dontlognull&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;timeout connect 5000&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;timeout client&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;50000&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;timeout server&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;50000&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 400 /etc/haproxy/errors/400.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 403 /etc/haproxy/errors/403.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 408 /etc/haproxy/errors/408.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 500 /etc/haproxy/errors/500.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 502 /etc/haproxy/errors/502.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 503 /etc/haproxy/errors/503.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;errorfile 504 /etc/haproxy/errors/504.http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;listen&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;stats&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;   &amp;lt;/span&gt;&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;bind&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;            &amp;lt;/span&gt;*:1936&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;mode&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;            &amp;lt;/span&gt;http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;log &amp;lt;span class=&quot;Apple-converted-space&quot;&gt;            &amp;lt;/span&gt;global&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;maxconn 10&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;clitimeout&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;      &amp;lt;/span&gt;100s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;srvtimeout&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;      &amp;lt;/span&gt;100s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;contimeout&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;      &amp;lt;/span&gt;100s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;timeout queue &amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;100s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats enable&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats hide-version&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats refresh 30s&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats show-node&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats auth admin:password&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;        &amp;lt;/span&gt;stats uri&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;  &amp;lt;/span&gt;/haproxy?stats&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;frontend localnodes&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;bind *:8088&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;mode http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;default_backend nodes&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;backend nodes&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;mode http&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;balance leastconn&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;option forwardfor&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;http-request set-header X-Forwarded-Port %[dst_port]&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;http-request add-header X-Forwarded-Proto https if { ssl_fc }&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;option httpchk&amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;server idx2 192.168.100.52:8088 ssl verify none check &amp;lt;/span&gt;
&amp;lt;span class=&quot;s1&quot;&gt;&amp;lt;span class=&quot;Apple-converted-space&quot;&gt;    &amp;lt;/span&gt;server idx1 192.168.100.51:8088 ssl verify none check &amp;lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[Making Splunk Certified Apps]]></title><description><![CDATA[As a developer of “Apps” for the Splunk platform; I have been very eager to automate more tedious tasks including build and static code…]]></description><link>https://rfaircloth.com/2016/09/27/making-splunk-certified-apps/</link><guid isPermaLink="false">https://rfaircloth.com/2016/09/27/making-splunk-certified-apps/</guid><pubDate>Tue, 27 Sep 2016 15:37:31 GMT</pubDate><content:encoded>&lt;p&gt;As a developer of “Apps” for the Splunk platform; I have been very eager to automate more tedious tasks including build and static code analysis. Today our very awesome development community has access to a new tool &lt;a href=&quot;http://dev.splunk.com/goto/appinspectdocs&quot;&gt;App Inspect&lt;/a&gt;. The new python based extensible framework will allow your automated build process to validate key issues and prepare for formal certification for Public apps on Splunk Base, or assure quality for internally developed apps. The process example can easily be ported to the tool section of your choice allowing for effective version control and testing of applications built on the Splunk platform.&lt;/p&gt;
&lt;p&gt;To help you get started I’ve developed an example using our partner’s tools at Atlassian.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bitbucket repository containing the source&lt;/li&gt;
&lt;li&gt;CMAKE build script for packaging and versioning&lt;/li&gt;
&lt;li&gt;Bitbucket pipelines integration using docker to ensure a clean package and execute validation&lt;/li&gt;
&lt;li&gt;Publish to AWS S3 as a package repository before manual publishing to Splunk Base&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Getting started review &lt;a href=&quot;https://bitbucket.org/Splunk-SecPS/seckit%5C_sa%5C_geolocation&quot;&gt;https://bitbucket.org/Splunk-SecPS/seckit\_sa\_geolocation&lt;/a&gt; this is my first and most complete example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CMakeLists.txt controls the build process&lt;/li&gt;
&lt;li&gt;src/ contains the applications source&lt;/li&gt;
&lt;li&gt;src/default/app.conf.in is the template for app.conf our build will update this file with the correct version tag supplied by git&lt;/li&gt;
&lt;li&gt;bitbucket-pipelines.yml controls the pipelines automated integration process
&lt;ul&gt;
&lt;li&gt;Retrieve and deploy the latest docker image with build tools and app inspect&lt;/li&gt;
&lt;li&gt;Package the app&lt;/li&gt;
&lt;li&gt;Push to S3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Try it yourself!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Don't call mom for proxy help from DefCon]]></title><description><![CDATA[Hanging out in the dark corners of DefCon and watching what passes by you see some things. What can we find with Splunk…. Who’s here and…]]></description><link>https://rfaircloth.com/2016/08/05/dont-call-mom-proxy-help-defcon/</link><guid isPermaLink="false">https://rfaircloth.com/2016/08/05/dont-call-mom-proxy-help-defcon/</guid><pubDate>Fri, 05 Aug 2016 23:50:45 GMT</pubDate><content:encoded>&lt;p&gt;Hanging out in the dark corners of DefCon and watching what passes by you see some things. What can we find with Splunk…. Who’s here and leaking information&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/08/2016-08-05_16-44-53.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/08/2016-08-05_16-44-53.png?resize=960%2C385&quot; alt=&quot;2016-08-05_16-44-53&quot;&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Syncing up shclusterapps]]></title><description><![CDATA[This one is short and sweet, when building a Splunk search head cluster we often will create a search head unattached to indexers to “stage…]]></description><link>https://rfaircloth.com/2016/07/27/syncing-up-shclusterapps/</link><guid isPermaLink="false">https://rfaircloth.com/2016/07/27/syncing-up-shclusterapps/</guid><pubDate>Wed, 27 Jul 2016 10:01:33 GMT</pubDate><content:encoded>&lt;p&gt;This one is short and sweet, when building a Splunk search head cluster we often will create a search head unattached to indexers to “stage” .spl deployments, configure THEN update shcluster/apps and push the following rsync command does this for you and obeys the golden rule to avoid default core apps. The list is correct as of 6.4.1 update as needed for new versions and be sure to exclude anything like an “app” containing deployment client&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;rsync –verbose –progress –stats –recursive –times –perms \&lt;br&gt;
–exclude alert_logevent \&lt;br&gt;
–exclude launcher \&lt;br&gt;
–exclude SplunkForwarder \&lt;br&gt;
–exclude alert_webhook \&lt;br&gt;
–exclude learned \&lt;br&gt;
–exclude splunk_httpinput \&lt;br&gt;
–exclude appsbrowser \&lt;br&gt;
–exclude legacy \&lt;br&gt;
–exclude SplunkLightForwarder \&lt;br&gt;
–exclude framework \&lt;br&gt;
–exclude sample_app \&lt;br&gt;
–exclude splunk_management_console \&lt;br&gt;
–exclude gettingstarted \&lt;br&gt;
–exclude search \&lt;br&gt;
–exclude*_deploymentclient** \&lt;br&gt;
–exclude introspection_generator_addon \&lt;br&gt;
–exclude splunk_archiver \&lt;br&gt;
–exclude user-prefs \&lt;br&gt;
/opt/splunk/etc/apps/* /opt/splunk/etc/shcluster-test/apps&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[Building High Performance low latency rsyslog for Splunk]]></title><description><![CDATA[This is a brief followup on my earlier post in a very large scale environment write -> monitor –> read between a log appending source such…]]></description><link>https://rfaircloth.com/2016/05/16/building-high-performance-low-latency-rsyslog-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2016/05/16/building-high-performance-low-latency-rsyslog-splunk/</guid><pubDate>Mon, 16 May 2016 13:15:12 GMT</pubDate><content:encoded>&lt;p&gt;This is a brief followup on my earlier post in a very large scale environment write -&gt; monitor –&gt; read between a log appending source such as rsyslogd and Splunk can impact the latency of log data entry into the destination environment. Last week I stumbled onto a feature of Rsyslog developed a couple of major versions ago that has been very under appreciated. OmProgram allows a developer to receive events from rsyslog using any program without first waiting for disk write. I’ve developed a little bit of code allowing direct transfer of events to Splunk using the http collector download and try it out.&lt;/p&gt;
&lt;p&gt;What the output module allows for is direct scale-able transfer between rsyslog and splunk in native protocols. Ideal use cases include dynamically scaling cloud environments and embedded devices where agents are not acceptable.&lt;/p&gt;
&lt;p&gt;Credits&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rsyslog dev team for making this possible and Rainer for this &lt;a href=&quot;http://www.rsyslog.com/rsyslog-v8-improvements-and-how-to-write-plugins-in-any-language/&quot;&gt;presentation&lt;/a&gt; that inspired me&lt;/li&gt;
&lt;li&gt;Splunk dev team for the really awesome http event collector and George who developed the &lt;a href=&quot;http://blogs.splunk.com/2015/12/11/http-event-collect-a-python-class/&quot;&gt;python class interface &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Splunk Stream team who added direct event collector usage in stream 6.5 proving significant scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Setup&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup http event collector behind a load balancer&lt;/li&gt;
&lt;li&gt;Note your token&lt;/li&gt;
&lt;li&gt;Install requests using apt,yum or pip &lt;a href=&quot;http://docs.python-requests.org/en/master/user/install/&quot;&gt;http://docs.python-requests.org/en/master/user/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If using certificate verification setup what is required for requests&lt;/li&gt;
&lt;li&gt;“git” the code &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src&quot;&gt;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;place &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src/ae4c14509a181b7f543382be75d76a0393f9c937/omsplunkhec.py?at=master&quot; title=&quot;omsplunkhec.py&quot;&gt;omsplunkhec.py&lt;/a&gt; and &lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/rsyslog-omsplunk/src/ae4c14509a181b7f543382be75d76a0393f9c937/splunk_http_event_collector.py?at=master&quot; title=&quot;splunk_http_event_collector.py&quot;&gt;splunk_http_event_collector.py&lt;/a&gt; in a location executable by rsyslog&lt;/li&gt;
&lt;li&gt;Setup rsyslog rule set with an action similar to the following ```
&lt;span class=&quot;k&quot;&gt;module&lt;/span&gt;(&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;=&lt;span class=&quot;s&quot;&gt;“omprog”&lt;/span&gt;)
&lt;a name=&quot;rsyslogd.d.conf.example-2&quot;&gt;&lt;/a&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;(&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;=&lt;span class=&quot;s&quot;&gt;“omprog”&lt;/span&gt;
&lt;a name=&quot;rsyslogd.d.conf.example-3&quot;&gt;&lt;/a&gt; &lt;span class=&quot;k&quot;&gt;binary&lt;/span&gt;=&lt;span class=&quot;s&quot;&gt;“/opt/rsyslog/hecout.py —source=rsyslog:hec —sourcetype=syslog —index=main”&lt;/span&gt;
&lt;a name=&quot;rsyslogd.d.conf.example-4&quot;&gt;&lt;/a&gt; &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;=&lt;span class=&quot;s&quot;&gt;“RSYSLOG_TraditionalFileFormat”&lt;/span&gt;)
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Building reliable rsyslogd infrastructure for Splunk]]></title><description><![CDATA[Overview Preparation of a base infrastructure for high availability ingestion of syslog data with a default virtual server and configuration…]]></description><link>https://rfaircloth.com/2016/05/16/building-reliable-rsyslogd-infrastructure-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2016/05/16/building-reliable-rsyslogd-infrastructure-splunk/</guid><pubDate>Mon, 16 May 2016 12:53:18 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Preparation of a base infrastructure for high availability ingestion of syslog data with a default virtual server and configuration for test data on boarding. Reference technology specific on boarding procedures.&lt;/p&gt;
&lt;h2&gt;Requirement&lt;/h2&gt;
&lt;p&gt;Multiple critical log sources require a reliable syslog infrastructure. The following attributes must be present for the solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enterprise supported linux such as RHEL, OR Centos, or recent Ubuntu LTS&lt;/li&gt;
&lt;li&gt;Syslog configuration which will not impact the logging of the host on which syslog is configured&lt;/li&gt;
&lt;li&gt;External Load Balancing utilizing DNAT lacking available enterprise shared services NLB devices KEMP offers a free to use version of their product up to 20 Mbs suitable for many cases&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Technical Environment&lt;/h2&gt;
&lt;p&gt;The following systems will be created utilizing physical or virtual systems. System specifications will vary due estimated load.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;servers in n+1 configuration
&lt;ul&gt;
&lt;li&gt;Minimum 2 GB memory&lt;/li&gt;
&lt;li&gt;Minimum 2 x 2.3 GHZ core&lt;/li&gt;
&lt;li&gt;Mounts configure per enterprise standard with the following additions
&lt;ul&gt;
&lt;li&gt;/opt/splunk 40 GB XFS&lt;/li&gt;
&lt;li&gt;/var/splunk-syslog 40 GB XFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dual interfaced load balancer configured for DNAT support.&lt;/li&gt;
&lt;li&gt;Subnet with at minimum the number of unique syslog sources (technologies) additional space for growth is strongly advised&lt;/li&gt;
&lt;li&gt;Subnet allocated for syslog servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Solution Prepare the rsyslogd servers&lt;/h2&gt;
&lt;p&gt;The following procedure will be utilized to prepare the rsyslogd servers&lt;/p&gt;
&lt;p&gt;. Install the base operating system and harden according to enterprise standards 2. Provision and mount the application partitions /opt/splunk and /var/splunk-syslog according the estimates required for your environment.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Note 1 typical configuration utilize noatime on both mounts&lt;/li&gt;
&lt;li&gt;Note 2 typical configuration utilizes no execute on the syslog mount&lt;/li&gt;
&lt;li&gt;Create the following directories for modular configuration of rsyslogd&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  mkdir -p /etc/rsyslog.d/conf.d/splunk-0-rules
  mkdir -p /etc/rsyslog.d/conf.d/splunk-1-inputs&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Create the Splunk master syslog-configuration /etc/rsyslog.d/splunk.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  #
  # Include all config files for splunk /etc/rsyslog.d/
  #

  $IncludeConfig /etc/rsyslog.d/splunk-0-rules/*.conf
  $IncludeConfig /etc/rsyslog.d/splunk-1-inputs/*.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Create the catch all syslog collection source. /etc/rsyslog.d/splunk-1-inputs/default.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  #define syslog source
  input(type=&quot;imptcp&quot; port=&quot;8100&quot; ruleset=&quot;default_file&quot;);
  input(type=&quot;impudp&quot; port=&quot;8100&quot; ruleset=&quot;default_file&quot;);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;Define a rule for all incoming data on the default port /etc/rsyslog.d/splunk-0-rules/default.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  ruleset(name=&quot;default_file&quot;){
      $RulesetCreateMainQueue
      $template DynaFile,&quot;/var/splunk-syslog/default/%HOSTNAME%.log&quot;
      *.* -?DynaFile
      stop
  }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;7&quot;&gt;
&lt;li&gt;Ensure splunk can read from the syslog folders. The paths should exist at this point due to the dedicated mount&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  chown -R splunk:splunk /var/splunk-syslog
  chmod -R 0755 /var/splunk-syslog&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;Reload rsyslogd&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  systemctl reload rsyslog&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;9&quot;&gt;
&lt;li&gt;Create log rotation configuration /etc/logrotate.d/splunk-syslog&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  /var/splunk-syslog/*/*.log
  {
      daily
      compress
      delaycompress
      rotate 4
      ifempty
      maxage 7
      nocreate
      missingok
      sharedscripts
      postrotate
      /bin/kill -HUP `cat /var/run/syslogd-ng.pid 2&gt; /dev/null` 2&gt; /dev/null || true
      endscript
  }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;10&quot;&gt;
&lt;li&gt;Allow firewall access to the new ports (RHEL based)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  firewall-cmd --permanent --zone=public --add-port=8100/tcp
  firewall-cmd --permanent --zone=public --add-port=8100/udp
  firewall-cmd --reload&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Solution Prepare KEMP Loadbalancer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Deploy virtual load balancer to hypervisor with two virtual interfaces
&lt;ul&gt;
&lt;li&gt;#1 Enterprise LAN&lt;/li&gt;
&lt;li&gt;#2 Private network for front end of syslog servers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Login to the load balancer web UI&lt;/li&gt;
&lt;li&gt;Apply free or purchased license&lt;/li&gt;
&lt;li&gt;Navigate to network setup
&lt;ul&gt;
&lt;li&gt;Set eth0 external ip&lt;/li&gt;
&lt;li&gt;Set eth1 internal ip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add the first virtual server (udp)
&lt;ul&gt;
&lt;li&gt;Navigate to Virtual Services –&gt; Add New&lt;/li&gt;
&lt;li&gt;set the virtual address&lt;/li&gt;
&lt;li&gt;set port 514&lt;/li&gt;
&lt;li&gt;set port name syslog-default-8100-udp&lt;/li&gt;
&lt;li&gt;set protocol udp&lt;/li&gt;
&lt;li&gt;Click Add this virtual service&lt;/li&gt;
&lt;li&gt;Adjust virtual service settings
&lt;ul&gt;
&lt;li&gt;Force Layer 7&lt;/li&gt;
&lt;li&gt;Transparency&lt;/li&gt;
&lt;li&gt;set persistence mode source ip&lt;/li&gt;
&lt;li&gt;set persistence time 6 min&lt;/li&gt;
&lt;li&gt;set scheduling method lest connected&lt;/li&gt;
&lt;li&gt;Use Server Address for NAT&lt;/li&gt;
&lt;li&gt;Click Add new real server - Enter IP of syslog server 1 - Enter port 8100&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add the first virtual server (tcp)
&lt;ul&gt;
&lt;li&gt;Navigate to Virtual Services –&gt; Add New&lt;/li&gt;
&lt;li&gt;set the virtual address&lt;/li&gt;
&lt;li&gt;set port 514&lt;/li&gt;
&lt;li&gt;set port name syslog-default-8100-tcp&lt;/li&gt;
&lt;li&gt;set protocol tcp&lt;/li&gt;
&lt;li&gt;Click Add this virtual service&lt;/li&gt;
&lt;li&gt;Adjust virtual service settings
&lt;ul&gt;
&lt;li&gt;Service type Log Insight&lt;/li&gt;
&lt;li&gt;Transparency&lt;/li&gt;
&lt;li&gt;set scheduling method lest connected&lt;/li&gt;
&lt;li&gt;TCP Connection only check port 8100&lt;/li&gt;
&lt;li&gt;Click Add new real server - Enter IP of syslog server 1 - Enter port 8100&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat the add virtual server process for additional resource servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Update syslog server routing configuration&lt;/h2&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Update the default gateway of the syslog servers to utilize the NLB internal interface&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Validation procedure&lt;/h2&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;from a linux host utilize the following commands to validate the NLB and log servers are working together
logger -P 514 -T -n &amp;lt;vip_ip&gt; &quot;test TCP&quot;
logger -P 514 -d -n &amp;lt;vip_ip&gt; &quot;test UDP&quot;
verify the messages are logged in /var/splunk-syslog/default&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Prepare Splunk Infrastructure for syslog&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Follow procedure for deployment of the Universal Forwarder with deployment client ensure the client has has valid outputs and base configuration&lt;/li&gt;
&lt;li&gt;Create the indexes syslog and syslog_unclassified&lt;/li&gt;
&lt;li&gt;Deploy input configuration for the default input&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[monitor:///var/splunk-syslog/default/*.log]
host_regex = .*\/(.*)\.log
sourcetype = syslog
source = syslog_enterprise_default
index = syslog_unclassified
disabled = enabled&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Validate the index contains data&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Ghost Detector (CVE-2015-7547)]]></title><description><![CDATA[4375461 Just in case you need need yet another reason to utilize passive DNS analytic, a new significant vulnerability is out for GLIBC…]]></description><link>https://rfaircloth.com/2016/02/18/354/</link><guid isPermaLink="false">https://rfaircloth.com/2016/02/18/354/</guid><pubDate>Fri, 19 Feb 2016 00:12:03 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/02/4375461.jpg&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/02/4375461.jpg?resize=400%2C300&quot; alt=&quot;4375461&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Just in case you need need yet another reason to utilize passive DNS analytic, a new significant vulnerability is out for GLIBC. Have stream? You can monitor your queries for this IOC&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://sourceware.org/ml/libc-alpha/2016-02/msg00416.html&quot;&gt;https://sourceware.org/ml/libc-alpha/2016-02/msg00416.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Update: the attack requires both A and AAAA records. Only show possible attacks with both involved. This should return zero results. If results are returned there “may” be something of interest drill into the answers involved to determine if they are malicious based on the CVE above.&lt;/p&gt;
&lt;p&gt;index=streams sourcetype=stream:dns (query_type=A OR query_type=AAAA)&lt;br&gt;
[&lt;br&gt;
search index=streams sourcetype=stream:dns (query_type=A OR query_type=AAAA)&lt;br&gt;
| rare limit=20 dest&lt;br&gt;
| fields + dest | format&lt;br&gt;
]&lt;br&gt;
| stats max(bytes_in) max(bytes_out) max(bytes) values(query_type) as qt by src,dest,query&lt;br&gt;
| where mvcount(qt)&gt;=2&lt;br&gt;
| sort – max*&lt;br&gt;
| lookup domain_segments_lookup domain as query OUTPUT privatesuffix as domain&lt;br&gt;
| lookup alexa_lookup_by_str domain OUTPUT rank&lt;br&gt;
| where isnull(rank)&lt;/p&gt;
&lt;p&gt;Don’t have stream yet? Deploy in under 20 minutes.&lt;br&gt;
&lt;a href=&quot;http://www.rfaircloth.com/2015/11/06/get-started-with-splunk-app-stream-6-4-dns/&quot;&gt;http://www.rfaircloth.com/2015/11/06/get-started-with-splunk-app-stream-6-4-dns/&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dealing with bad threat data]]></title><description><![CDATA[Every now and then a threat data provider will include invalid entries in their threat list creating loads of false positives in Enterprise…]]></description><link>https://rfaircloth.com/2016/02/17/dealing-with-bad-threat-data/</link><guid isPermaLink="false">https://rfaircloth.com/2016/02/17/dealing-with-bad-threat-data/</guid><pubDate>Wed, 17 Feb 2016 09:04:16 GMT</pubDate><content:encoded>&lt;p&gt;Every now and then a threat data provider will include invalid entries in their threat list creating loads of false positives in Enterprise Security. For “reasons” namely performance ES will append new entries to the internal threat system but does not remove entries no longer present in a source. You can easily clear an entire threat collection which will allow your system to reload from the current sources.&lt;/p&gt;
&lt;p&gt;splunk stop&lt;br&gt;
splunk clean inputdata threatlist&lt;br&gt;
splunk clean inputdata threat_intelligence_manager&lt;br&gt;
splunk start&lt;br&gt;
splunk clean kvstore -app DA-ESS-ThreatIntelligence -collection &lt;collection&gt;Common values for collection are http_intel and domain_intel&lt;/p&gt;
&lt;/collection&gt;</content:encoded></item><item><title><![CDATA[Building Reliable Syslog infrastructure on Centos 7 for Splunk]]></title><description><![CDATA[Overview Preparation of a base infrastructure for high availability ingestion of syslog data with a default virtual server and configuration…]]></description><link>https://rfaircloth.com/2016/01/17/building-reliable-syslog-infrastructure-on-centos-7/</link><guid isPermaLink="false">https://rfaircloth.com/2016/01/17/building-reliable-syslog-infrastructure-on-centos-7/</guid><pubDate>Mon, 18 Jan 2016 00:47:49 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Preparation of a base infrastructure for high availability ingestion of syslog data with a default virtual server and configuration for test data on boarding. Reference technology specific on boarding procedures.&lt;/p&gt;
&lt;h2&gt;Requirement&lt;/h2&gt;
&lt;p&gt;Multiple critical log sources require a reliable syslog infrastructure. The following attributes must be present for the solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enterprise supported linux such as RHEL, OR Centos&lt;/li&gt;
&lt;li&gt;Syslog configuration which will not impact the logging of the host on which syslog is configured&lt;/li&gt;
&lt;li&gt;External Load Balancing utilizing DNAT lacking available enterprise shared services NLB devices KEMP offers a free to use version of their product up to 20 Mbs suitable for many cases&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Technical Environment&lt;/h2&gt;
&lt;p&gt;The following systems will be created utilizing physical or virtual systems. System specifications will vary due estimated load.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Centos 7.x (current) servers in n+1 configuration
&lt;ul&gt;
&lt;li&gt;Minimum 2 GB memory&lt;/li&gt;
&lt;li&gt;Minimum 2 x 2.3 GHZ core&lt;/li&gt;
&lt;li&gt;Mounts configure per enterprise standard with the following additions
&lt;ul&gt;
&lt;li&gt;/opt/splunk 40 GB XFS&lt;/li&gt;
&lt;li&gt;/var/splunk-syslog 40 GB XFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dual interfaced load balancer configured for DNAT support.&lt;/li&gt;
&lt;li&gt;Subnet with at minimum the number of unique syslog sources (technologies) additional space for growth is strongly advised&lt;/li&gt;
&lt;li&gt;Subnet allocated for syslog servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Solution Prepare the syslog-ng servers&lt;/h2&gt;
&lt;p&gt;The following procedure will be utilized to prepare the syslog-ng servers&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the base operating system and harden according to enterprise standards&lt;/li&gt;
&lt;li&gt;Provision and mount the application partitions /opt/splunk and /var/splunk-syslog according the estimates required for your environment.&lt;/li&gt;
&lt;li&gt;Note 1 typical configuration utilize noatime on both mounts&lt;/li&gt;
&lt;li&gt;Note 2 typical configuration utilizes no execute on the syslog moun&lt;/li&gt;
&lt;li&gt;Enable the EPEL repository for RHEL/CENTOS as the source for syslog-ng installation&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  yum -y &lt;span class=&quot;token function&quot;&gt;install&lt;/span&gt; epel-release
  yum -y repolist
  yum -y update
  &lt;span class=&quot;token function&quot;&gt;reboot&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Install the syslog-ng software&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  yum y &lt;span class=&quot;token function&quot;&gt;install&lt;/span&gt; syslog-ng&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Replace /etc/syslog-ng/syslog-ng.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  @version:3.5
  @include &quot;scl.conf&quot;

# syslog-ng configuration file

#

# SecKit template

# We utilize syslog-ng on Centos to allow syslog ingestion without

# interaction with the OS

# Note: it also sources additional configuration files (*.conf)

# located in /etc/syslog-ng/conf.d/

  options {
      flush_lines (0);
      time_reopen (10);
      log_fifo_size (1000);
      chain_hostnames (off);
      use_dns (no);
      use_fqdn (no);
      create_dirs (no);
      keep_hostname (yes);
  };

# Source additional configuration files (.conf extension only)

  @include &quot;/etc/syslog-ng/conf.d/*.conf&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;Create the following directories for modular configuration of syslog-ng&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; -p /etc/syslog-ng/conf.d/splunk-0-source
  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; -p /etc/syslog-ng/conf.d/splunk-1-dest
  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; -p /etc/syslog-ng/conf.d/splunk-2-filter
  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; -p /etc/syslog-ng/conf.d/splunk-3-log
  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; -p /etc/syslog-ng/conf.d/splunk-4-simple&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;7&quot;&gt;
&lt;li&gt;Create the Splunk master syslog-configuration /etc/syslog-ng/conf.d/splunk.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  &lt;span class=&quot;token comment&quot;&gt;################################################################################&lt;/span&gt;
  &lt;span class=&quot;token comment&quot;&gt;# SecKit syslog template based on the work of Vladimir&lt;/span&gt;
  &lt;span class=&quot;token comment&quot;&gt;# Template from https://github.com/hire-vladimir/SA-syslog_collection/&lt;/span&gt;
  &lt;span class=&quot;token comment&quot;&gt;################################################################################&lt;/span&gt;

  &lt;span class=&quot;token comment&quot;&gt;################################################################################&lt;/span&gt;
  &lt;span class=&quot;token comment&quot;&gt;#### Global config ####&lt;/span&gt;
  options &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    create-dirs&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;yes&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# Specific file/directory permissions can be set&lt;/span&gt;
    &lt;span class=&quot;token comment&quot;&gt;# this is particularly needed, if Splunk UF is running as non-root&lt;/span&gt;
    owner&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;splunk&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    group&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;splunk&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    dir-owner&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;splunk&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    dir-group&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;splunk&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    dir-perm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;0755&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    perm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;0755&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

    time-reopen&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    keep-hostname&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;yes&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    log-msg-size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;65536&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  @include &lt;span class=&quot;token string&quot;&gt;&quot;/etc/syslog-ng/conf.d/splunk-0-source/*.conf&quot;&lt;/span&gt;
  @include &lt;span class=&quot;token string&quot;&gt;&quot;/etc/syslog-ng/conf.d/splunk-1-dest/*.conf&quot;&lt;/span&gt;
  @include &lt;span class=&quot;token string&quot;&gt;&quot;/etc/syslog-ng/conf.d/splunk-2-filter/*.conf&quot;&lt;/span&gt;
  @include &lt;span class=&quot;token string&quot;&gt;&quot;/etc/syslog-ng/conf.d/splunk-3-log/*.conf&quot;&lt;/span&gt;
  @include &lt;span class=&quot;token string&quot;&gt;&quot;/etc/syslog-ng/conf.d/splunk-4-simple/*.conf&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;Create the catch all syslog collection source. /etc/syslog-ng/conf.d/splunk-4-simple/8100-default.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  ################################################################################
  #### Enable listeners ####
  source remote8100_default
  {
      udp(port(8100));
      tcp(port(8100));
  };

#### Log remote sources classification ####

  destination d_default_syslog {
          file(&quot;/var/splunk-syslog/default/$HOST.log&quot;);
  };

# catch all, all data that did not meet above criteria will end up here

  log {
          source(remote8100_default);
          destination(d_default_syslog);
          flags(fallback);
  };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;9&quot;&gt;
&lt;li&gt;Ensure splunk can read from the syslog folders. The paths should exist at this point due to the dedicated mount&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  &lt;span class=&quot;token function&quot;&gt;chown&lt;/span&gt; -R splunk:splunk /var/splunk-syslog
  &lt;span class=&quot;token function&quot;&gt;chmod&lt;/span&gt; -R 0755 /var/splunk-syslog&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;10&quot;&gt;
&lt;li&gt;Verify syslog-ng configuration no errors should be reported (no output)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  syslog-ng -s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;11&quot;&gt;
&lt;li&gt;Update the systemd servics configuration to correctly support both rsyslog and syslog-ng edit /lib/systemd/system/syslog-ng.service&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  find:
  &lt;span class=&quot;token assign-left variable&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;/usr/sbin/syslog-ng -F -p /var/run/syslogd.pid
  replace:
  &lt;span class=&quot;token assign-left variable&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;/usr/sbin/syslog-ng -F -p /var/run/syslogd-ng.pid&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;12&quot;&gt;
&lt;li&gt;Create log rotation configuration /etc/logrotate.d/splunk-syslog&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  /var/splunk-syslog/*/*.log
  {
      daily
      compress
      delaycompress
      rotate 4
      ifempty
      maxage 7
      nocreate
      missingok
      sharedscripts
      postrotate
      /bin/kill -HUP `cat /var/run/syslogd-ng.pid 2&gt; /dev/null` 2&gt; /dev/null || true
      endscript
  }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;13&quot;&gt;
&lt;li&gt;Resolve SELinux blocked actions&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  semanage port -a -t syslogd_port_t -p tcp &lt;span class=&quot;token number&quot;&gt;8100&lt;/span&gt;
  semanage port -a -t syslogd_port_t -p udp &lt;span class=&quot;token number&quot;&gt;8100&lt;/span&gt;
  semanage fcontext -a -t var_log_t /var/splunk-syslog
  restorecon -v &lt;span class=&quot;token string&quot;&gt;&apos;/var/splunk-syslog&apos;&lt;/span&gt;
  logger -d -P &lt;span class=&quot;token number&quot;&gt;8100&lt;/span&gt; -n &lt;span class=&quot;token number&quot;&gt;127.0&lt;/span&gt;.0.1 -p &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;test2&quot;&lt;/span&gt;
  &lt;span class=&quot;token builtin class-name&quot;&gt;cd&lt;/span&gt; /root
  &lt;span class=&quot;token function&quot;&gt;mkdir&lt;/span&gt; selinux
  &lt;span class=&quot;token builtin class-name&quot;&gt;cd&lt;/span&gt; selinux
  audit2allow -M syslog-ng-modified -l -i /var/log/audit/audit.log
  &lt;span class=&quot;token comment&quot;&gt;#verify the file does not contain anything no related to syslog&lt;/span&gt;
  &lt;span class=&quot;token function&quot;&gt;vim&lt;/span&gt; syslog-ng-modified.te
  semodule -i syslog-ng-modified.pp&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;14&quot;&gt;
&lt;li&gt;Allow firewall access to the new ports&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  firewall-cmd --permanent --zone&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;public --add-port&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8100&lt;/span&gt;/tcp
  firewall-cmd --permanent --zone&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;public --add-port&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8100&lt;/span&gt;/udp
  firewall-cmd --reload&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;15&quot;&gt;
&lt;li&gt;Enable and start syslog-ng&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;bash&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;  systemctl &lt;span class=&quot;token builtin class-name&quot;&gt;enable&lt;/span&gt; syslog-ng
  systemctl start syslog-ng&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Solution Prepare KEMP Loadbalancer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Deploy virtual load balancer to hypervisor with two virtual interfaces
&lt;ul&gt;
&lt;li&gt;#1 Enterprise LAN&lt;/li&gt;
&lt;li&gt;#2 Private network for front end of syslog servers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Login to the load balancer web UI&lt;/li&gt;
&lt;li&gt;Apply free or purchased license&lt;/li&gt;
&lt;li&gt;Navigate to network setup
&lt;ul&gt;
&lt;li&gt;Set eth0 external ip&lt;/li&gt;
&lt;li&gt;Set eth1 internal ip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add the first virtual server (udp)
&lt;ul&gt;
&lt;li&gt;Navigate to Virtual Services –&gt; Add New&lt;/li&gt;
&lt;li&gt;set the virtual address&lt;/li&gt;
&lt;li&gt;set port 514&lt;/li&gt;
&lt;li&gt;set port name syslog-default-8100-udp&lt;/li&gt;
&lt;li&gt;set protocol udp&lt;/li&gt;
&lt;li&gt;Click Add this virtual service&lt;/li&gt;
&lt;li&gt;Adjust virtual service settings
&lt;ul&gt;
&lt;li&gt;Force Layer 7&lt;/li&gt;
&lt;li&gt;Transparency&lt;/li&gt;
&lt;li&gt;set persistence mode source ip&lt;/li&gt;
&lt;li&gt;set persistence time 6 min&lt;/li&gt;
&lt;li&gt;set scheduling method lest connected&lt;/li&gt;
&lt;li&gt;Use Server Address for NAT&lt;/li&gt;
&lt;li&gt;Click Add new real server - Enter IP of syslog server 1 - Enter port 8100&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add the first virtual server (tcp)
&lt;ul&gt;
&lt;li&gt;Navigate to Virtual Services –&gt; Add New&lt;/li&gt;
&lt;li&gt;set the virtual address&lt;/li&gt;
&lt;li&gt;set port 514&lt;/li&gt;
&lt;li&gt;set port name syslog-default-8100-tcp&lt;/li&gt;
&lt;li&gt;set protocol tcp&lt;/li&gt;
&lt;li&gt;Click Add this virtual service&lt;/li&gt;
&lt;li&gt;Adjust virtual service settings
&lt;ul&gt;
&lt;li&gt;Service type Log Insight&lt;/li&gt;
&lt;li&gt;Transparency&lt;/li&gt;
&lt;li&gt;set scheduling method lest connected&lt;/li&gt;
&lt;li&gt;TCP Connection only check port 8100&lt;/li&gt;
&lt;li&gt;Click Add new real server - Enter IP of syslog server 1 - Enter port 8100&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat the add virtual server process for additional resource servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Update syslog server routing configuration&lt;/h2&gt;
&lt;p&gt;Update the default gateway of the syslog servers to utilize the NLB internal interface&lt;/p&gt;
&lt;h2&gt;Validation procedure&lt;/h2&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#from a linux host utilize the following commands to validate the NLB and log servers are working together
logger -P 514 -T -n &amp;lt;vip_ip&gt; &quot;test TCP&quot;
logger -P 514 -d -n &amp;lt;vip_ip&gt; &quot;test UDP&quot;
verify the messages are logged in /var/splunk-syslog/default&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Prepare Splunk Infrastructure for syslog&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Follow procedure for deployment of the Universal Forwarder with deployment client ensure the client has has valid outputs and base configuration&lt;/li&gt;
&lt;li&gt;Create the indexes syslog and syslog_unclassified&lt;/li&gt;
&lt;li&gt;Deploy input configuration for the default input&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[monitor:///var/splunk-syslog/default/*.log]
host_regex = .*\/(.*)\.log
sourcetype = syslog
source = syslog_enterprise_default
index = syslog_unclassified
disabled = enabled&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Validate the index contains data&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[When you have 100 problems, more logs are not the answer]]></title><description><![CDATA[big_fire_01 Often SIEM projects begin where log aggregation projects end. So many logs cut into organized stacks of wood ready to burn for…]]></description><link>https://rfaircloth.com/2016/01/16/when-you-have-100-problems-more-logs-are-not-the-answer/</link><guid isPermaLink="false">https://rfaircloth.com/2016/01/16/when-you-have-100-problems-more-logs-are-not-the-answer/</guid><pubDate>Sat, 16 Jan 2016 16:55:16 GMT</pubDate><content:encoded>&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/01/big_fire_01-300x200.jpg?resize=300%2C200&quot; alt=&quot;big_fire_01&quot;&gt; Often SIEM projects begin where log aggregation projects end. So many logs cut into organized stacks of wood ready to burn for value. I can be quoted on this “All logs can be presumed to have security value”. One project to build the worlds largest bonfire however is seldom the correct answer. What value you may ask? Value will be gained in one or more of these categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compliance – External regulation required collection and retention of this data for a specified period of time.&lt;/li&gt;
&lt;li&gt;Forensics – External event causes the organization to be aware of an activity and the data can be used to determine if a unauthorized or malicious act occurred and the scope of the damage. Bonus if the actor can be identified this is not always the case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Detection – Rules based logic can be applied to the data as generated to detect interesting events for investigation. At Splunk we call these searches correlation searches, which generate notable events. Notable events may or may not be incidents.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analytic – Aggregation of the can allow human or machine learning based detection of actions that are notable.&lt;/li&gt;
&lt;li&gt;Tuning – Rules based logic applied to detection and aggregation sources can identify opportunities to improve preventive controls which will reduce the risk profile of the organization. This will include such things as adjustment to web proxy and UTM devices when malware is detected on an endpoint.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of these value sources their perceived importance will vary greatly to the data’s customer. Lets talk about typical customers for a few minutes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Legal – The legal team is one of the ultimate customers of the solution and often the most powerful agent of change in achieving funding. Conservative and often paranoid their goals are easily understood and testable. As such their primary concern in forensic value.
&lt;ul&gt;
&lt;li&gt;Retention – Their ask is to retain data for length of time required for greater of the following and no longer per log source. The standards are “business reason” meaning it is not correct to take the most demanding standard and apply to all things log.
&lt;ul&gt;
&lt;li&gt;The business to operate (SOC)&lt;/li&gt;
&lt;li&gt;The requirements of contractually imposed standards such as PCI, HIPPA (BA) OR specific contract terms.&lt;/li&gt;
&lt;li&gt;The requirements of legal standards. For example seven years in the case of change logs for financial controls where the electronic record is the sole control.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Availability – The original log form (not the original medium) must be available for discovery for the period of time required. Additionally unrelated log sources should not be commingled. If provided to a third party in discovery that additional information can be used against the organization in legal proceedings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Brand Protection/marketing – While a non consumer of the log information this group within your organization can indeed be a champion. Their desire is to be able to answer questions to consumers and the public. “What happened”, “When did it start”, “When did it end”. This group will derive value from both forensic and detection in the event of a breach. However the value of detective controls can be realized to avoid two major categorizes of events
&lt;ul&gt;
&lt;li&gt;Detection – Denial of service through intentional destruction of IT systems&lt;/li&gt;
&lt;li&gt;Detection – Defacement such as loading of unauthorized text or media onto customer facing systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Loss Prevention/Fraud/Corporate Security – Direct financial consequences of security events of today are often included inside this group today. External events will often trigger investigations which will require assistance from the IT security organization. Forensic log data easily produced on demand will improve the operation of this group.&lt;/li&gt;
&lt;li&gt;Human Resources – This group represents an excellent opportunity for partnership.
&lt;ul&gt;
&lt;li&gt;Detection – Policy violations and the organizations response when such events may rise to “resume updating” levels require methodical execution to avoid legal exposure.&lt;/li&gt;
&lt;li&gt;Forensic – Records will permit rapid resolution of externally reported events such as workplace harassment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compliance – This may take the form of multiple groups within the organization inside of other larger groups. In most cases value is taken from forensic usage of data. Compliance groups often initiate the funding log aggregation projects and can be significant partners in funding the Log Aggregation + SIEM solutions. For systems involved in the scope of a compliance program
&lt;ul&gt;
&lt;li&gt;Forensic – Data for the operation of the systems&lt;/li&gt;
&lt;li&gt;Forensic – Data to evidence the performance of the SOC for events related to compliance systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IT Security Systems Engineering SSE – This group often does not desire consolidated views of data. Objections are often centered on the “hear no evil see no evil” view point in that aggregated data will inform the team of opportunities which will require effort the teams are not staffed for. This objection can often be overcome by demonstration of “work smarter not harder” utilizing data driven decision making and prioritization.
&lt;ul&gt;
&lt;li&gt;Tuning – what systems generate the most noise&lt;/li&gt;
&lt;li&gt;Tuning – what systems not log logging useful information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IT Security Operations – This group can quickly be overwhelmed with a poorly planed SIEM implementation. Pre SIEM SOC teams will most often utilize alert by email from security tools such as endpoint anti malware and intrusion detection.
&lt;ul&gt;
&lt;li&gt;Tuning – evidence of miss-configuration in IT systems increasing risk through accepting of risky behavior or noise preventing identification of true positives.&lt;/li&gt;
&lt;li&gt;Forensic – fast access to original events and context provided by assets and identities reduce the triage time and allow the organization to increase value from the existing staff.&lt;/li&gt;
&lt;li&gt;Detection – Consolidation of Alerts from existing detection technology combined with work prioritization utilizing assets and identities. This critical capability allows the security operations team to reduce the burden of compliance by ensuring their process “exists and is followed” to work events based on the perceived significance. This is often the first value realized by this team and is often ignored in favor of “new capabilities”. This error can easily be said to cause the majority of SIEM project failures from the perspective of this team.&lt;/li&gt;
&lt;li&gt;Detection – Utilization of the SIEM product capabilities to identify violation of policy and known bad behavior (rules based). Such events represent new knowledge of activities in the environment and can be of significant value provided the staff is available to execute. Effective execution of Tuning and Detection based on alerts will enable to organization to realize this value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IT Security Intelligence (SIC) and or hunt team – A relatively new group, often a person who’s charter is to pursue identification if risk and bad actors in the environment using new concepts outside of the burden of compliance. Then work with the SOC and SSE teams to operationalize valid detection. Additionally this team will often own the configuration of behavior analysis technology
&lt;ul&gt;
&lt;li&gt;Analytic – This team will slice dice and aggregate data looking for anomalies which often result in the creation of new rules based detection.&lt;/li&gt;
&lt;li&gt;Forensic – This data is required to analyze the detected events for validation of positive vs false positive.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given our end customers/consumers and organizational goals we can identify which log sources and SIEM capabilities will provide the greatest return on investment. To seed your initial discussion consider the following as a typical road map for value from data consumption. I order the data source based on my experience with value delivered to the organizations I have serviced. For any give source I can provide a full thesis on my position which would simply be to much to read in this post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consolidation of data source useful for identification of authorized assets and identities. You can find more on this blog about specific approaches. Useful for Detection, Analytic, and Forensic
&lt;ul&gt;
&lt;li&gt;Networks owned.&lt;/li&gt;
&lt;li&gt;Compute assets, endpoints, servers, storage&lt;/li&gt;
&lt;li&gt;Other assets, switches routers, network appliances, UPS etc&lt;/li&gt;
&lt;li&gt;System Service Accounts&lt;/li&gt;
&lt;li&gt;Local Accounts (mapped to owners)&lt;/li&gt;
&lt;li&gt;Humans
&lt;ul&gt;
&lt;li&gt;Elevated&lt;/li&gt;
&lt;li&gt;Non Elevated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Application accounts
&lt;ul&gt;
&lt;li&gt;Built in&lt;/li&gt;
&lt;li&gt;Service&lt;/li&gt;
&lt;li&gt;Human&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Endpoint anti malware – Detection&lt;/li&gt;
&lt;li&gt;Network IDS – Detection&lt;/li&gt;
&lt;li&gt;Firewall – Detection, Forensic, Analytic&lt;/li&gt;
&lt;li&gt;Network Application (WebProxy, NG Firewall) Detection, Forensic, Analytic&lt;/li&gt;
&lt;li&gt;Endpoint intelligence (process launch context and hash) Forensic, Analytic&lt;/li&gt;
&lt;li&gt;Web Access logs for employee systems such as Exchange, SSO and HR portal. Detection, Analytic&lt;/li&gt;
&lt;li&gt;System Authentication logs.&lt;/li&gt;
&lt;li&gt;Customer facing web access logs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What should you do next? Use case development, is the process of identification of requirements for detection, matching source data to support the requirements and documenting a process to deliver and respond on the new capability. This should be continued until the organization observes the cost of obtaining additional value from existing logs exceeds the perceived value delivered. Larger organizations should allow use case development to identify future log sources, small to medium organizations should limit scope at avoid analysis paralysis. At the end of the day this approach will yield a slowing uptake on the size of your log aggregation and SIEM in the short term.&lt;/p&gt;
&lt;p&gt;In the long term your system will have a lower TCO and grander scale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/01/aRV5xnB_700b.jpg?resize=587%2C427&quot; alt=&quot;aRV5xnB_700b&quot;&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Making Asset data useful with Splunk Enterprise Security CSC 1 Part 1]]></title><description><![CDATA[54080041 Update broken link 2017-10-04 Friend we need to talk, there is something important that you have been overlooking for a long time…]]></description><link>https://rfaircloth.com/2016/01/07/making-asset-data-useful-with-splunk-enterprise-security/</link><guid isPermaLink="false">https://rfaircloth.com/2016/01/07/making-asset-data-useful-with-splunk-enterprise-security/</guid><pubDate>Fri, 08 Jan 2016 03:31:55 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/01/54080041.jpg&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2016/01/54080041.jpg?resize=500%2C378&quot; alt=&quot;54080041&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Update broken link 2017-10-04&lt;/p&gt;
&lt;p&gt;Friend we need to talk, there is something important that you have been overlooking for a long time. Two years ago when you implemented your first SIEM you gave your consultant an excel file listing all of your servers on the corporate network. You promised you would spend time on it after the consultant left but, then you got the new FireEye. You didn’t forget but then the you got a new Next Gen firewall and after there was the new red team initiative.&lt;/p&gt;
&lt;p&gt;It is time make a difference in the security posture of your organization. It is time to take a bite out of CSC #1a that’s not a typo we need to work on #1a, #2 can wait. so can #1b .It is time to work SANs critical control #1. I know the CMDB is out of date and doesn’t reflect today’s architecture. We can do a lot with a small amount of work, today I will share how to lay a foundation to address &lt;span class=&quot;label&quot;&gt;CSC 1: Inventory of Authorized (a) and Unauthorized Devices (b).&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;h6&gt;Objective 1: &lt;strong&gt;Identify the location of each asset using latitude, longitute, city state and zip&lt;/strong&gt;&lt;/h6&gt;
&lt;h6&gt;Objective 2: &lt;strong&gt;Identify the compliance zone for each network segment&lt;/strong&gt;&lt;/h6&gt;
&lt;h6&gt;Objective 3: &lt;strong&gt;Identify categories that can assist the analyst in review of events related to the network containing the source or destination&lt;/strong&gt;&lt;/h6&gt;
&lt;h6&gt;Objective 4: &lt;strong&gt;Identify the minimum priority of devices in a given network segment.&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;Code is provided via &lt;a href=&quot;https://splunkbase.splunk.com/app/3055/&quot;&gt;Security Kit&lt;/a&gt; Install the app “SecKit_SA_idm_common” on your ES Search head.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h5&gt;Don’t forget to update the app imports to include “SecKit_SA_.*”&lt;/h5&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Walkthrough&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Update seckit_idm_pre_cidr_location.csv so that for each subnet in cidr notation define the location. On a very large campus it may be desirable to present a point on a specific building however in most cases it will be adequate to have a single lat/long pair for all subnets on a campus. Include all private and public spaces owned or managed by your organization do not include any public space not external spaces such as hosting providers and cloud services.&lt;/li&gt;
&lt;li&gt;Update seckit_idm_pre_cidr_category.csv note subnet in this case may be larger or smaller than used in locations. The most precise definition will be utilized by Splunk Identity Management within Enterprise Security. This may contain cloud address space if the ip space is not continually re-purposed&lt;/li&gt;
&lt;li&gt;Populate pci_cidr_domain we will overload this field for non PCI environments.
&lt;ol&gt;
&lt;li&gt;PCI usage “wireless ORtrust|cardholder OR trust|dmz OR empty (empty or default represents untrust&lt;/li&gt;
&lt;li&gt;Non PCI usage substitute other compliance in place of cardholder such as pii, sox, hippa, cip&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Populate cidr_priority
&lt;ol&gt;
&lt;li&gt;low the most often used value should represent the majority of your devices&lt;/li&gt;
&lt;li&gt;medium common servers&lt;/li&gt;
&lt;li&gt;high devices of significant importance&lt;/li&gt;
&lt;li&gt;critical devices requiring immediate response such as
&lt;ol&gt;
&lt;li&gt;A server whose demise would cause you to work on Christmas&lt;/li&gt;
&lt;li&gt;A server whose demise could cause the closure of the company even if you work on Christmas&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Populate cidr_category values provided here would apply to all devices in this network. I will list some very common categories I apply note each category needs to be pipe “|” separated and may not contain a space
&lt;ol&gt;
&lt;li&gt;net_internal – internal IP space&lt;/li&gt;
&lt;li&gt;net_external – external IP space&lt;/li&gt;
&lt;li&gt;netid_ddd_ddd_ddd_ddd_bits – applied to each allocated subnet (smallest assigned unit.&lt;/li&gt;
&lt;li&gt;zone_string – where string is one of dmz, server, endpoint, storage management, wan, vip, nat&lt;/li&gt;
&lt;li&gt;facility_string – where string is the internal facility identification code&lt;/li&gt;
&lt;li&gt;facility_type_string – where string is a common identifier such as datacenter, store, office, warehouse, port, mine, airport, moonbase, cloud, dr, ship&lt;/li&gt;
&lt;li&gt;net_assignment_string – where string is static dyndhcp, dynvirt&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Run the saved search “seckit_idm_common_assets_networks_lookup_gen” and review the results in seckit_idm_common_assets_networks.csv you may run this report on demand as the lookup files above are changed or on a schedule of your choice.&lt;/li&gt;
&lt;li&gt;Enable the asset file in enterprise security by navigating to Configuration –&gt; Enrichment –&gt; Assets and Identities then clicking enable on “seckit_idm_common_assets_networks”&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Bonus Objective&lt;/h2&gt;
&lt;p&gt;Enhance your existing server and network device assets list by integrating the following lookups and merging the OUTPUT fields with the device specific asset data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;| lookup seckit_idm_pre_cidr_category_by_cidr_lookup cidr as lip OUTPUT cidr_pci_domain as pci_domain cidr_category as category&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;2. ```
| lookup idm_shared_cidr_location_lookup cidr as ip OUTPUT lat long city country&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Share that search! Building a content pack for Splunk Enterprise Security 4.0+]]></title><description><![CDATA[Splunk has initial support for export of “content” which can be dashboards and correlation searches created by the user to share with…]]></description><link>https://rfaircloth.com/2016/01/07/shareable-content-for-splunk-enterprise-security/</link><guid isPermaLink="false">https://rfaircloth.com/2016/01/07/shareable-content-for-splunk-enterprise-security/</guid><pubDate>Thu, 07 Jan 2016 11:29:22 GMT</pubDate><content:encoded>&lt;p&gt;Splunk has initial support for export of “content” which can be dashboards and correlation searches created by the user to share with another team. What if you need to be a little more complex for example including a lookup generating search? This will get a little more complicated but very doable by the average admin. Our mission here is to implement UC0029. What is UC0029 glad you ask Each new malware signature detected should be reviewed by a security analyst to determine if proactive steps can be taken to prevent infection. We will create this as a notable event so that we can provide evidence to audit that the process exists and was followed.&lt;/p&gt;
&lt;p&gt;Source code will be provided so I will not detail step by step how objects will be created and defined for this post&lt;/p&gt;
&lt;h1&gt;UC0029 Endpoint new malware detected by signature&lt;/h1&gt;
&lt;p&gt;My “brand” is SecKit so you will see this identifier in content I have created alone or with my team here at Splunk. As per our best practice adopt your own brands and use appropriately for your content. There is no technical reason to replace the “brand” on third party content you elect to utilize.&lt;/p&gt;
&lt;p&gt;Note ensure all knowledge objects are exported as all app’s owned by admin as you go&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a DA-ESS-SecKit-EndpointProtection
&lt;ul&gt;
&lt;li&gt;This will contain ES specific content such as menus dashboards, and correlation searches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create the working app SA-SecKit-EndpointProtection
&lt;ul&gt;
&lt;li&gt;This will contain props transforms lookups and scheduled searches created outside of ES&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create the lookup seckit_endpoint_malware_tracker this lookup will contain each signature as it is detected in the environment and some handy information such as the endpoint first detected, user involved and the most recent detection.&lt;/li&gt;
&lt;li&gt;Create empty lookup CSV files
&lt;ul&gt;
&lt;li&gt;seckit_endpoint_malware_tracker.csv (note you will not ship this file in your content pack)&lt;/li&gt;
&lt;li&gt;seckit_endpoint_malware_tracker.csv.default&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and test the saved search SecKit Malware Tracker – Lookup Gen. This search will use tstats to find the first and last instance of all signatures in a time window and update the lookup if an earlier or later instance is found&lt;/p&gt;
&lt;p&gt;Build and test the correlation search UC0029-S01-V001 New malware signature detected. This search will find “new” signatures from the lookup we have created and create a notable event”Make it default” In both apps move content from local/ to default/ this will allow your users to customize the content without replacing the existing searches”Turn if off by default” It is best practice to ensure any load generating searches are disabled by default add disabled=1 to each savedsearches.conf stanza that does not end in”- Rule”add disabled=1 to each correleationsearches.conf&lt;/p&gt;
&lt;p&gt;Create a spl (tar.gz) containing both apps createdWrite a blog post explaining what you did, how the searches work and share the code!Gain fame and respect maybe a fez or a cape&lt;/p&gt;
&lt;p&gt;The source code&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://bitbucket.org/rfaircloth-splunk/securitykit/src/1ea60c46b685622116e28e8f1660a6c63e7d9e96/base/ess/?at=master&quot;&gt;https://bitbucket.org/rfaircloth-splunk/securitykit/src/1ea60c46b685622116e28e8f1660a6c63e7d9e96/base/ess/?at=master&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bonus: Delegate administration of content app&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using your favorite editor edit app/metadata/local.meta&lt;/li&gt;
&lt;li&gt;Update the following permisions adding “ess_admin” role&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;## access = read : [ * ], write : [ admin,role2,role3 ]&lt;br&gt;
[savedsearches]&lt;br&gt;
access = read : [ * ], write : [ admin,&lt;span style=&quot;color: #ff00ff;&quot;&gt;ess_admin &lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;[correlationsearches]&lt;br&gt;
access = read : [ * ], write : [ admin,&lt;span style=&quot;color: #ff00ff;&quot;&gt;ess_admin&lt;/span&gt; ]&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded></item><item><title><![CDATA[Advancing security through the use of security assessments]]></title><description><![CDATA[Long ago our in the distant past that is the late 1970s individuals were alone and unconnected. Visionaries of the future began to connect…]]></description><link>https://rfaircloth.com/2015/12/23/advancing-security-through-the-use-of-security-assessments/</link><guid isPermaLink="false">https://rfaircloth.com/2015/12/23/advancing-security-through-the-use-of-security-assessments/</guid><pubDate>Wed, 23 Dec 2015 18:15:28 GMT</pubDate><content:encoded>&lt;p&gt;Long ago our in the distant past that is the late 1970s individuals were alone and unconnected. Visionaries of the future began to connect the individuals in communities. These communities were open and without borders, individuals could enter and use all dwellings with ease. The community thrived with each individual adding unique value.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i0.wp.com/cdn.quotesgram.com/img/82/91/1736830500-security-chain-fence-funny-demotivational-posters-1296250082.jpg?resize=640%2C512&quot; alt=&quot;&quot;&gt;&lt;br&gt;
As the community grew the individuals began to notice unwelcome occurrences. Dwellings changed without the approval of its owner, items moved from dwelling to dwelling without kind notes left. Most disturbing of all some smaller individuals would simply disappear. Each community started to address the concerns of its individuals on their own. Some communities fared better than others, elders would meet together and discuss the successes in their communities (just the sucesses). In order to determine which elders community had fared the best consultants were hired to asses the communities security. The following levels commonly used to range community security.&lt;/p&gt;
&lt;p&gt;Level 0 Awareness of roaming beats in the village as identified by missing young children, food supplies and occasional spotting of red eyed monsters in the night.&lt;/p&gt;
&lt;p&gt;Level 1 Young boys with clubs seek to prove the existence of of such beasts despite denial by the elders.&lt;/p&gt;
&lt;p&gt;Level 2 A small dog has fallen dead of age in town square signs are placed elsewhere presuming the animals will read and obey the signs.&lt;/p&gt;
&lt;p&gt;Level 3 As children continue to disappear in the night demands that more must be done continue. Young men are given small stones and placed at the community gates. Additional signs are added to ensure beats will only use the main gate&lt;/p&gt;
&lt;p&gt;Level 4 Losses continue, recent reports of missing valuables such as silver and gold alarm the elders. Each community member is interviewed and background checks are completed. Community leaders and elders and guards are excluded from the process.&lt;/p&gt;
&lt;p&gt;Level 5 Media reports of losses become public. Elders embarrassed demand more actions from the guards. New guards are posted around certain well lighted intersections. Guards dance every 30 minutes between 9 AM and 10 AM around the intersection ensuring the requirement of more activity is satisfied.&lt;/p&gt;
&lt;p&gt;Level 6 Additional losses occur new Elders are brought in by the community to solve the problem. Immediately all guards are replaced with new cards from neighboring communities suffer more public and higher losses. The new elders carry forth a plan to double their efforts. The following plan is put in place&lt;br&gt;
A single new guard is set to walk along the parameter of the community during business hours Monday through Thursday&lt;br&gt;
The number of intersections guarded are doubled. The dance is performed every 15 minutes and the intersection guards are equipped with monoculars.&lt;/p&gt;
&lt;p&gt;Outside actors are hired to impersonate monsters of the night by entering the community at night and taking small tokens such as napkins. The actors are immediately fired for not playing fairly for reasons not disclosed to the elders.&lt;/p&gt;
&lt;p&gt;Level 7 The new guard leadership brings additional guards from neighboring community to patrol the perimeter outside of business hours. The outsourced guards are instructed to awake a day guard should anything severe or important be observed.&lt;/p&gt;
&lt;p&gt;Outside actors are again hired and directed to attempt to take a small flyer from sign post at a single intersection. After repeated success all guards are placed at the same intersection and a successful test is reported to the elders.&lt;/p&gt;
&lt;p&gt;Level 8 The senior elders european beach vacation photos are placed around the community near the fading signs installed when the community reached level 1. Senior guards are replace. The new senior guards offer to higher the “best” of the outsourced guards for the new perimeter security program. The terms of the offer were not disclosed 2% of the staff takes the offer. The outsource firm does not counter to retain the guards. The new firm observes the photo liberators have opposable thumbs. Reduction security processes for small animals is reduced increasing the rate of loss for small animals and children. Elders are not allowed contact to life forms with opposable thumbs. The probation is receded after 1 hour.&lt;/p&gt;
&lt;p&gt;Level 9 The senior guards request more outside assistance, new consultants recommend a new monitoring system built of mirrors allowing the guards to view the intersections from a central location on a single glass wall. Perimeter guards and intersection guards are immediately discontinued. Days latter all small farm animals disappear without notice. On the one year anniversary a senior investigator comes to the elders with a fantastic story of finding a single chicken they must have been taken from this community at a black market in a far away land. The Senior guards initially are certain this must be an isolated incident however a manual inspection of the community find all small animals are indeed missing.&lt;/p&gt;
&lt;p&gt;Level 10 Senior guards are once again replaced. The single wall of glass vendor is brought in to explain why their solution has failed. The vendor quickly finds the system was implemented in the very same way as the neighboring communities system. The vendor points out the shape of their neighbors community differs greatly the mirrors as installed have excellent visibility of the latrine and the community dump but have very limited visibility on the perimeter. The vendor recommends a larger glass system to provide visibility on the perimeter in addition to the current solution. Construction begins on a larger hut with bigger glass walls.&lt;/p&gt;
&lt;p&gt;Level 11 Following delays in construction to the new hut Additional Senior guards are engaged from far away with experience in guarding large animals. Additional guards are hired with differing skills. Each guard begins to adjust the minors to their personal liking. Often complaining they spend to much time in the hut. Senior guards begin to require each guard to roam the community during the day looking for signs of wild beats.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Finding signal in the noise of DNS data using Splunk]]></title><description><![CDATA[DNS is a fundamental component of our computing infrastructure before we identify bad actions easily we should remove what we can easily…]]></description><link>https://rfaircloth.com/2015/11/06/finding-signal-in-the-noise-of-dns-data-using-splunk/</link><guid isPermaLink="false">https://rfaircloth.com/2015/11/06/finding-signal-in-the-noise-of-dns-data-using-splunk/</guid><pubDate>Fri, 06 Nov 2015 14:35:44 GMT</pubDate><content:encoded>&lt;p&gt;DNS is a fundamental component of our computing infrastructure before we identify bad actions easily we should remove what we can easily identify to be good. For all of our queries we will rely on common information model fields and extractions. For most customers I will assist them in deploying the Splunk App for Stream to collect query information from their DNS servers in a reliable way regardless of the logging capabilities of their chosen server product.&lt;/p&gt;
&lt;p&gt;Note: Be sure to install Cedric’s &lt;a href=&quot;https://splunkbase.splunk.com/app/2734/&quot;&gt;URLToolbox&lt;/a&gt; Add on we will make use of its power here.&lt;/p&gt;
&lt;p&gt;Lets start by looking at the data everyone is spending the most time talking about queries for A (ipv4) and AAA (ipv6). Lets search for no more than the last 60 min while we are working to be kind to our indexers. For real analysis you will use bigger windows.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;tag=dns tag=resolution tag=dns index=* NOT source=”stream:Splunk_*” (query_type=A OR query_type=AAAA)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My sample environment is small very small 5 users, 10 windows servers. In the last 24 hours this query gave me 24,000+ results way more than I can examine lets start to cut that down. We also need to remember what we will probably be learning from our data that is which domains require investigation for suspicion of involvement in malicious activity.&lt;/p&gt;
&lt;p&gt;Reduction #1 Lets remove all domains owned by our organization for email or web hosting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Update the following files to include the domains used for email or web hosting.
&lt;ol&gt;
&lt;li&gt;Splunk_SA_CIM/lookups/cim_corporate_email_domains.csv&lt;/li&gt;
&lt;li&gt;Splunk_SA_CIM/lookups/cim_corporate_web_domains.csv&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Update our search to extract the domain and tld for latter use. this is more complicated than it looks so we will make up a uri and let UTToolbox do the work for us&lt;/li&gt;
&lt;li&gt;The new base search will look like this&lt;br&gt;
tag=dns tag=resolution tag=dns NOT source=”stream:Splunk_*” index=* (query_type=A OR query_type=AAAA)&lt;br&gt;
| eval uri=”dnsquery://”+query&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;ut\_parse(uri)&lt;/code&gt; | fields – ut_fragment ut_netloc ut_params ut_path ut_port ut_query ut_scheme&lt;/li&gt;
&lt;li&gt;Now we can use our email and web domain lookups to reduce the data set we are working with. This took about about 13% of my results. Notice I use fields – to get rid of stuff I don’t need moved from my indexer back to my search heads.&lt;/li&gt;
&lt;li&gt;tag=dns tag=resolution tag=dns NOT source=”stream:Splunk_*” index=* (query_type=A OR query_type=AAAA)&lt;br&gt;
| eval uri=”dnsquery://”+query&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;ut\_parse(uri)&lt;/code&gt;&lt;br&gt;
| fields – ut_fragment ut_netloc ut_params ut_path ut_port ut_query ut_scheme&lt;br&gt;
| lookup cim_corporate_email_domain_lookup domain as ut_domain OUTPUT domain as cim_email_domain&lt;br&gt;
| lookup cim_corporate_web_domain_lookup domain as ut_domain OUTPUT domain as cim_web_domain&lt;br&gt;
| where isnull(cim_email_domain) AND isnull(cim_web_domain)&lt;br&gt;
| fields -cim_email_domain cim_web_domain)&lt;/li&gt;
&lt;li&gt;The next easy win is to remove all queries for one of our assets we do that by kicking out all queries for one of our assets by dns name or where the resulting IP is one of our assets&lt;/li&gt;
&lt;li&gt;tag=dns tag=resolution tag=dns NOT source=”stream:Splunk_*” index=* (query_type=A OR query_type=AAAA)&lt;br&gt;
| eval uri=”dnsquery://”+query&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;ut\_parse(uri)&lt;/code&gt;&lt;br&gt;
| fields – ut_fragment ut_netloc ut_params ut_path ut_port ut_query ut_scheme&lt;br&gt;
| lookup cim_corporate_email_domain_lookup domain as ut_domain OUTPUT domain as cim_email_domain&lt;br&gt;
| lookup cim_corporate_web_domain_lookup domain as ut_domain OUTPUT domain as cim_web_domain&lt;br&gt;
| where isnull(cim_email_domain) AND isnull(cim_web_domain)&lt;br&gt;
| fields – cim_email_domain cim_web_domain&lt;br&gt;
| lookup asset_lookup_by_str dns as query OUTPUTNEW asset_id as query_asset_id&lt;br&gt;
| lookup asset_lookup_by_cidr ip as host_addr OUTPUTNEW asset_id as host_addr_asset_id&lt;br&gt;
| where isnull(query_asset_id) AND isnull(host_addr_asset_id)&lt;br&gt;
| fields – query_asset_id host_addr_asset_id&lt;/li&gt;
&lt;li&gt;Next up is to remove all queries for Alexa Top 1 M domains why? well in the Top 1M we will probably not find any new domains, or any domains being used for C2 using a DNS channel. Thats not to say XML file on drop box or feedburner can’t be used but we won’t find that threat here. This further reduced by data set by 92%&lt;/li&gt;
&lt;li&gt;tag=dns tag=resolution tag=dns NOT source=”stream:Splunk_*” index=* (query_type=A OR query_type=AAAA)&lt;br&gt;
| eval uri=”dnsquery://”+query&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;ut\_parse(uri)&lt;/code&gt;&lt;br&gt;
| fields – ut_fragment ut_netloc ut_params ut_path ut_port ut_query ut_scheme&lt;br&gt;
| lookup cim_corporate_email_domain_lookup domain as ut_domain OUTPUT domain as cim_email_domain&lt;br&gt;
| lookup cim_corporate_web_domain_lookup domain as ut_domain OUTPUT domain as cim_web_domain&lt;br&gt;
| where isnull(cim_email_domain) AND isnull(cim_web_domain)&lt;br&gt;
| fields – cim_email_domain cim_web_domain&lt;br&gt;
| lookup asset_lookup_by_str dns as query OUTPUTNEW asset_id as query_asset_id&lt;br&gt;
| lookup asset_lookup_by_cidr ip as host_addr OUTPUTNEW asset_id as host_addr_asset_id&lt;br&gt;
| where isnull(query_asset_id) AND isnull(host_addr_asset_id)&lt;br&gt;
| fields – query_asset_id host_addr_asset_id&lt;br&gt;
| lookup alexa_lookup_by_str domain as ut_domain OUTPUTNEW rank as alexa_rank&lt;br&gt;
| where isnull(alexa_rank)&lt;/li&gt;
&lt;li&gt;Down from 24K to under 1700 but that’s still alot, at this point I noticed a couple of things. I have queries for .local domains I can’t explain but I know are not malicious, bare host names (no period) and I have a couple of devices servicing DNS from guest wifi identify those points and update the search to remove them. This leaves me with 216 domains to investigate. But we can tune this even further lets keep going.&lt;/li&gt;
&lt;li&gt;CDN networks can host malicious content however dns analysis is again not the way to find such threats. This takes me down to 173 domains
&lt;ul&gt;
&lt;li&gt;Create a new lookup Splunk_SA_cim/lookups/custom_cim_cdn_domains.csv you may find new domains and need to update this list over time&lt;/li&gt;
&lt;li&gt;Upload this file &lt;a href=&quot;http://www.rfaircloth.com/wp-content/uploads/2015/11/custom_cim_cdn_domains.csv&quot;&gt;custom_cim_cdn_domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;add a new lookup via Splunk_SA_cim/local/transforms.conf [custom_cim_cdn_domain_lookup]&lt;br&gt;
filename = custom_cim_cdn_domains.csv&lt;br&gt;
match_type = WILDCARD(domain)&lt;br&gt;
max_matches = 1&lt;/li&gt;
&lt;li&gt;Update with a new search to exclude known CDN domains&lt;/li&gt;
&lt;li&gt;tag=dns tag=resolution tag=dns NOT source=”stream:Splunk_*” index=* (query_type=A OR query_type=AAAA)&lt;br&gt;
query=”*.*” NOT query=”*.local”&lt;br&gt;
| eval uri=”dnsquery://”+query&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;ut\_parse(uri)&lt;/code&gt;&lt;br&gt;
| fields – ut_fragment ut_netloc ut_params ut_path ut_port ut_query ut_scheme&lt;br&gt;
| lookup cim_corporate_email_domain_lookup domain as ut_domain OUTPUT domain as cim_email_domain&lt;br&gt;
| lookup cim_corporate_web_domain_lookup domain as ut_domain OUTPUT domain as cim_web_domain&lt;br&gt;
| where isnull(cim_email_domain) AND isnull(cim_web_domain)&lt;br&gt;
| fields – cim_email_domain cim_web_domain&lt;br&gt;
| lookup asset_lookup_by_str dns as query OUTPUTNEW asset_id as query_asset_id&lt;br&gt;
| lookup asset_lookup_by_cidr ip as host_addr OUTPUTNEW asset_id as host_addr_asset_id&lt;br&gt;
| where isnull(query_asset_id) AND isnull(host_addr_asset_id)&lt;br&gt;
| fields – query_asset_id host_addr_asset_id&lt;br&gt;
| lookup alexa_lookup_by_str domain as ut_domain OUTPUTNEW rank as alexa_rank&lt;br&gt;
| where isnull(alexa_rank)&lt;br&gt;
| lookup custom_cim_cdn_domain_lookup domain as query OUTPUTNEW is_cdn |&lt;br&gt;
where isnull(is_cdn)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Optional Step if you have domain tools integration enabled (whois) the following lines added to your search will show when the domain was first seen by you and when it was registered.&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;| rename ut_domain as domain&lt;br&gt;
| &lt;code class=&quot;language-text&quot;&gt;get\_whois&lt;/code&gt;&lt;br&gt;
| eval “Age (days)”=ceil((now()-newly_seen)/86400)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;Many people of written on what to do with this data now, go hunting!&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Get started with Splunk App Stream 6.4 for DNS Analysis]]></title><description><![CDATA[Passive DNS analysis is all the rage right now, the detection opportunities presented have been well discussed for some time. If your…]]></description><link>https://rfaircloth.com/2015/11/06/get-started-with-splunk-app-stream-6-4-dns/</link><guid isPermaLink="false">https://rfaircloth.com/2015/11/06/get-started-with-splunk-app-stream-6-4-dns/</guid><pubDate>Fri, 06 Nov 2015 12:00:49 GMT</pubDate><content:encoded>&lt;p&gt;Passive DNS analysis is all the rage right now, the detection opportunities presented have been well discussed for some time. If your organization is like most now is the time you are being asked how you can implement these detection strategies. Leveraging your existing Splunk investment you can get started very quickly with less change to your organization than one might think. Here is what we will use older versions will work fine however the screen shots will be a bit off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Splunk Enterprise 6.3.1&lt;/li&gt;
&lt;li&gt;Splunk App for Stream 6.4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will assume Splunk Enterprise 6.3.1has already been installed.&lt;/p&gt;
&lt;p&gt;Decide where to install your Stream App. Typically this will be the Enterprise Security search head. &lt;del&gt;However if your ES search head is also a search head cluster you will need to use an AD-HOC search head, dedicated search head or a deployment server. &lt;/del&gt;Current versions of Stream fully support installation on a Search Head Cluster.&lt;/p&gt;
&lt;p&gt;Note: If using the deployment server (DS) you must configure the server to search the indexer or index cluster containing your stream data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install Splunk App for Stream using the standard procedures located &lt;a href=&quot;http://docs.splunk.com/Documentation/StreamApp/latest/DeployStreamApp/AboutSplunkAppforStream&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Copy the deployment TA to your deployment server if you installed on a search head. /opt/splunk/etc/deployment-apps/Splunk_TA_stream&lt;/li&gt;
&lt;li&gt;On your deployment server create a new folder to contain configuration for your stream dns server group.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;mkdir -p Splunk_TA_stream_infra_dns/local&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Copy the inputs.conf from the default TA to the new TA for group management&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;cp Splunk_TA_stream/local/inputs.conf Splunk_TA_stream_infra_dns/local/&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Update the inputs.conf to include your forwarder group id&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;vi Splunk_TA_stream_infra_dns/local/inputs.conf&lt;/li&gt;
&lt;li&gt;Alter “stream_forwarder_id =” to “stream_forwarder_id =infra_dns”&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;Create a new server class “infra_stream_dns” include both the following apps and deploy to all DNS servers (Windows DNS or BIND)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Splunk_TA_stream&lt;/li&gt;
&lt;li&gt;Splunk_TA_stream_infra_dns&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;7&quot;&gt;
&lt;li&gt;Reload your deployment server&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Excellent at this point the Splunk Stream app will be deployed to all of your DNS servers and sit idle. The next few steps will prepare the environment to start collections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new index I typically will create stream_dns and setup retention for 30 days.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configure your deployment group&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Login to the search head with the Splunk App for Stream&lt;/li&gt;
&lt;li&gt;Navigate to Splunk App for Stream&lt;/li&gt;
&lt;li&gt;If this is your first time you may find you need to complete the welcome wizard .&lt;/li&gt;
&lt;li&gt;Click on Configure the “Distributed Forwarder Management”&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure_dfm.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure_dfm.png?resize=673%2C153&quot; alt=&quot;stream_configure_dfm&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Click Create New Group as follows then click Next&lt;/li&gt;
&lt;li&gt;Name Infra_DNS&lt;/li&gt;
&lt;li&gt;Description Applied to All DNS servers&lt;/li&gt;
&lt;li&gt;Include Ephemeral Streams? No&lt;/li&gt;
&lt;li&gt;Enter “infra_dns” as this will ensure all clients deployed above will pickup this configuration from the Stream App&lt;/li&gt;
&lt;li&gt;Search for “Splunk_DNS” and select each match then Click Finish&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_dns_aggs.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_dns_aggs.png?resize=694%2C494&quot; alt=&quot;stream_dns_aggs&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;Click on Configuration then “Configure Streams”&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure.png?resize=673%2C153&quot; alt=&quot;stream_configure&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Click on New Stream&lt;/li&gt;
&lt;li&gt;Setup basic info as follows then click Next&lt;/li&gt;
&lt;li&gt;Protocol DNS&lt;/li&gt;
&lt;li&gt;Name “Infra_DNS”&lt;/li&gt;
&lt;li&gt;Description “Capture DNS on internal DNS servers”&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure_dns.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_configure_dns.png?resize=760%2C389&quot; alt=&quot;stream_configure_dns&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;We will no use Aggregation so leave this as “No” and click Next&lt;/li&gt;
&lt;li&gt;The default fields will meet our needs so go ahead and click Next&lt;/li&gt;
&lt;li&gt;Optional Step: Create filters in most cases requests from the DNS server to the outside are not interesting as they are generated based on client requests that cannot be answer from the cache. Creating filters will reduce the total volume of data by approximately 50%&lt;/li&gt;
&lt;li&gt;Click create filter&lt;/li&gt;
&lt;li&gt;Select src_ip as the field&lt;/li&gt;
&lt;li&gt;Select “Not Regular Expression” as the type&lt;/li&gt;
&lt;li&gt;Provide a regex capture that will match all DNS server IPs example “(172\.16\.0\.(19|20|21))” will match in my lab network.
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_filter.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/11/stream_filter.png?resize=441%2C363&quot; alt=&quot;stream_filter&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Click next&lt;/li&gt;
&lt;li&gt;Select only the Infra_DNS group and click Create Stream&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point stream will deploy and begin collection however index selection is not permitted in this workflow so we need to go back and set it up now.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find Infra_DNS and click edit&lt;/li&gt;
&lt;li&gt;Select the index appropriate for your environment&lt;/li&gt;
&lt;li&gt;Click save&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ready to check your work? Run this search replace index=* with your index&lt;/p&gt;
&lt;p&gt;index=* sourcetype=stream:dns | stats count by query | sort – count&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Getting all the logs and avoiding the WEC]]></title><description><![CDATA[I get asked about this one often, I happen to have a bit of experience with this which is very rare. There is scant documentation on the…]]></description><link>https://rfaircloth.com/2015/11/05/getting-all-the-logs-avoiding-the-wec/</link><guid isPermaLink="false">https://rfaircloth.com/2015/11/05/getting-all-the-logs-avoiding-the-wec/</guid><pubDate>Thu, 05 Nov 2015 15:32:43 GMT</pubDate><content:encoded>&lt;p&gt;I get asked about this one often, I happen to have a bit of experience with this which is very rare. There is scant documentation on the technology from Microsoft or anyone else. I do know of some success being had with very specific low volume use cases but that’s not what I do. I’m a specialist of sorts I walk of a Delta plane, drop my bag at a Marriott then walk into change someones world with data. Actual facts about their environment from their environment and I need and use data my customers don’t know they had. Which brings me to Windows Event Collection (WEC).&lt;/p&gt;
&lt;p&gt;Customer ask me about it its seems so easy lets talk about the parts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group policy use to make changes to all systems in an environment.&lt;/li&gt;
&lt;li&gt;Remote Power Shell&lt;/li&gt;
&lt;li&gt;COM/DCOM/Com+ and all of the RPC that goes with it&lt;/li&gt;
&lt;li&gt;Kerberos authentication&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does it work?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group policy instructs the computer to connect to a collector and gather a policy&lt;/li&gt;
&lt;li&gt;Policy read causes a Com+ server to read the event log (yes this is code you have not been running it can and will impact your endpoints)&lt;/li&gt;
&lt;li&gt;Local filter determines what do do with this event (xml parsing with XPATH and XSLT)&lt;/li&gt;
&lt;li&gt;RPC call using computer account to Collector&lt;/li&gt;
&lt;li&gt;Denial (Auth required)&lt;/li&gt;
&lt;li&gt;Authentication (event log write on DC and on Collector)&lt;/li&gt;
&lt;li&gt;Serial write with sync and block to round robin data base on the server. So if 300 events come in these have to get in queue to go to disk.&lt;/li&gt;
&lt;li&gt;Close connection&lt;/li&gt;
&lt;li&gt;Poll period go back to 3&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Lots of steps? Lets ask about failure modes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens if my collector is down
&lt;ul&gt;
&lt;li&gt;Answer client goes to sleep and retries hope your logs don’t wrap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What happens if my collector won’t get back up
&lt;ul&gt;
&lt;li&gt;Answer build a new one, open a change record, wait for approval, explain to audit why you don’t have logs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What happens to the format of the logs?
&lt;ul&gt;
&lt;li&gt;Answer Good question I can’t explain what MS is doing to these logs if you know please share&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What about log rotation and archival
&lt;ul&gt;
&lt;li&gt;Answer not possible you need another tool to read back and store them some place (splunk)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;My collector isn’t keeping up what do I do now?
&lt;ul&gt;
&lt;li&gt;Answer Well hopefully the org structure of your Domain will support creating an assignment policy at the OU level, you might be able to use the same policy/collector pair at multiple OU points but you might also need to break up the OUs to manage the policy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cross domain?
&lt;ul&gt;
&lt;li&gt;Answer 1 or more collectors per domain.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Wait I only want events XX and ZZYY from certain servers for compliance.
&lt;ul&gt;
&lt;li&gt;Answer you get another collection policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I can’t make this work on server2134
&lt;ul&gt;
&lt;li&gt;Answer call Support at MS, explain what event collection is, hopefully convince that person it is supported&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;My sensitive “application/service log” doesn’t use the event log
&lt;ul&gt;
&lt;li&gt;Answer logfile this is windows who would do that?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets compare to universal forwarders with Splunk&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens if my “indexer” is down
&lt;ul&gt;
&lt;li&gt;Answer Client connect to another indexer, in a production system the indexer itself is replicated and you retain access to all data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What happens if my collector won’t get back up
&lt;ul&gt;
&lt;li&gt;Answer. Data is replicated still available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What happens to the format of the logs?
&lt;ul&gt;
&lt;li&gt;Answer We capture the original text of all logs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What about log rotation and archival
&lt;ul&gt;
&lt;li&gt;Answer Built in&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;My collector isn’t keeping up what do I do now?
&lt;ul&gt;
&lt;li&gt;Answer Horizontal scaling Splunk will help you plan for this with experience and performance data from real world implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cross domain?
&lt;ul&gt;
&lt;li&gt;Certainly, WAN no issue, Cloud not a problem. VPN sure why not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Wait I only want events XX and ZZYY from certain servers for compliance.
&lt;ul&gt;
&lt;li&gt;Deployment server will push a configuration based on the server names you select&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I can’t make this work on server2134
&lt;ul&gt;
&lt;li&gt;Answer call Support (paid) at Splunk, we have real people with real knowledge and a great community who has probably solved that problem before.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;My sensitive system doesn’t use the event log file it
&lt;ul&gt;
&lt;li&gt;Answer probably not a problem, files, database, network capture can be a data source we do this all the time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Getting all the logs, avoid the syslog]]></title><description><![CDATA[Big data, open world a utopia we may one day have. Today I want my logs all of my logs, and then I want more. I often want to collect…]]></description><link>https://rfaircloth.com/2015/09/16/getting-all-the-logs-avoid-the-syslog/</link><guid isPermaLink="false">https://rfaircloth.com/2015/09/16/getting-all-the-logs-avoid-the-syslog/</guid><pubDate>Thu, 17 Sep 2015 03:07:07 GMT</pubDate><content:encoded>&lt;p&gt;Big data, open world a utopia we may one day have. Today I want my logs all of my logs, and then I want more. I often want to collect additional data such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance counters on Windows operating systems&lt;/li&gt;
&lt;li&gt;Appended files on all platforms&lt;/li&gt;
&lt;li&gt;Script and executable output to translate the odd and the weird stuff developers create&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All to often there is resistance to this lofty goal of security information awareness. Why might you ask. To be honest often security people have a certain reputation. I’m not talking about the funk or the mothers basement kind of reputation. There is a reputation for breaking the environment and stopping the business. IT ops is in agent overload, license compliance, monitoring, data loss prevention, av, endpoint security all want their agents. Log management often is late to the party and is viewed as a bridge to far. In some cases an ineffective solution was in place and there is resistance to replacing a legacy collection tool. Yes indeed the reason people don’t want to install a proper collection tool is the broken solution being replaced worked just fine. Really actually had this conversation.&lt;/p&gt;
&lt;p&gt;I’m a Splunk user and customer turned consultant. I bleed green but this isn’t about Splunk it does support the idea that using the Splunk tool set including the Universal Forwarder is the best choice. But if your log collection tool is another enterprise ready product this applies to you as well.&lt;/p&gt;
&lt;p&gt;Issue number 1: Supportability each agent will parse or fail parse and provide log data in a unique format. Each security solution vendor will be able to best test with their native language (format) if supportable and tested is a goal. You want to use the best tool.&lt;/p&gt;
&lt;p&gt;Issue number 2: Reliable delivery each agent from a commercial vendor using a native protocol will support acknowledgment and store and forward. Any vendor neutral agent using the syslog feature will not support this feature meaning you can not assure any auditor with any level of google foo your log solution has integrity and is complete.&lt;/p&gt;
&lt;p&gt;Issue number 3: Reliable resumption each commercial agent includes support for high water tracking with windows events, and tail tracking for files. Snare (unreliable) Lasso (unreliable) Logstash, Gray log Fluentd do not support this feature. Without this feature any time the agent stops, abends, or the system reboots data is lost. So this is not acceptable for regulated environment. Including small matters like PCI, SOX, HIPPA, GLBA to name an American focused few.&lt;/p&gt;
&lt;p&gt;Issue number 4: The position that using a freeware or vendor neutral collection tool is reliable places you alone outside of industry support. Splunk, HP logger, Mcafee Nitro, Q1 Radar all provide reliable collection agents. Where support for syslog based solutions exists it is limited and second class at best.&lt;/p&gt;
&lt;p&gt;Issue number 5: Cost its not free, every issue encountered will cost human labor time, opportunity (delays) and potentially leave your company open to audit finding for non compliance.&lt;/p&gt;
&lt;p&gt;Issue number 6: False belief that performance will be impacted by these vendor agents. While for some specific vendor agents and use cases this may be true. It is no more likely (or less likley) to be than using a unsupported log collection tool.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Splunk Universal Forwarder Version 6.2.3+ Ubuntu 15.04]]></title><description><![CDATA[Author: Ryan Faircloth Summary: Using repositories for version managment of the Splunk Universal Forwarder assists in ensuring managed…]]></description><link>https://rfaircloth.com/2015/05/28/splunk-universal-forwarder-version-6-2-3-ubuntu-15-04/</link><guid isPermaLink="false">https://rfaircloth.com/2015/05/28/splunk-universal-forwarder-version-6-2-3-ubuntu-15-04/</guid><pubDate>Thu, 28 May 2015 15:06:05 GMT</pubDate><content:encoded>&lt;p&gt;Author: Ryan Faircloth&lt;/p&gt;
&lt;p&gt;Summary: Using repositories for version managment of the Splunk Universal Forwarder assists in ensuring managed Ubuntu systems are using the approved version of the software at all times.&lt;/p&gt;
&lt;h2&gt;Setup the repository server&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Install reprepro and nginx &lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;sudo apt-get install reprepro nginx packaging-dev -y&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a user to work with the repository &lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;adduser --disabled-password --disabled-login --home /srv/reprepro --group reprepro&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change user to our reprepro user all commands for the repository should be executed using this ID&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt; sudo su - reprepro&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Generate GPG Keys&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Change user to our reprepro user all commands for the repository should be executed using this ID&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;sudo su - reprepro
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Create the default configuration for gpg by running the command&lt;br&gt;
&lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;gpg --list-keys&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit ~/.gnupg/gpg.conf&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;uncomment the line &lt;code class=&quot;language-text&quot;&gt;no-greeting&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;add the following content to the end of the file&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Prioritize stronger algorithms for new keys.
default-preference-list SHA512 SHA384 SHA256 SHA224 AES256 AES192 AES CAST5 BZIP2 ZLIB ZIP UNCOMPRESSED
# Use a stronger digest than the default SHA1 for certifications.
cert-digest-algo SHA512
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Generate a new key with the command &lt;code class=&quot;language-text&quot;&gt;gpg --gen-key&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select the folowing options&lt;/li&gt;
&lt;li&gt;Type of key “(1) RSA and RSA (default)”&lt;/li&gt;
&lt;li&gt;Key size “4096”&lt;/li&gt;
&lt;li&gt;Expires “10y”&lt;/li&gt;
&lt;li&gt;Confirm “Y”&lt;/li&gt;
&lt;li&gt;Real Name “Splunk local repository”&lt;/li&gt;
&lt;li&gt;Email address on repository contact this generally should be an alias or distribution list&lt;/li&gt;
&lt;li&gt;Leave the comment blank&lt;/li&gt;
&lt;li&gt;Confirm and “O” to Okay&lt;/li&gt;
&lt;li&gt;Leave passphrase blank and confirm, a key will be generated not the sub KEY ID in the following example * E507D48E *&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;gpg: checking the trustdb&amp;lt;br&gt;&amp;lt;/br&gt;gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model&amp;lt;br&gt;&amp;lt;/br&gt;gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u&amp;lt;br&gt;&amp;lt;/br&gt;gpg: next trustdb check due at 2025-05-24&amp;lt;br&gt;&amp;lt;/br&gt;pub 4096R/410E1699 2015-05-27 [expires: 2025-05-24]&amp;lt;br&gt;&amp;lt;/br&gt; Key fingerprint = 7CB8 81A9 E07F DA7B 83FF 2E1B 8B31 DA83 410E 1699&amp;lt;br&gt;&amp;lt;/br&gt;uid Splunk local repository &amp;lt;repo@example.com&gt;&amp;lt;br&gt;&amp;lt;/br&gt;sub 4096R/E507D48E 2015-05-27 [expires: 2025-05-24]&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt; 6. Export the signing keys public component save this content for use later&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;br&gt;&amp;lt;/br&gt;gpg --export --armor KEY_ID &gt;~/repo.pub&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Configure Prerepro&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Change user to our reprepro user all commands for the repository should be executed using this ID &lt;code class=&quot;language-text&quot;&gt;sudo su - reprepro&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create the directory structure &lt;code class=&quot;language-text&quot;&gt;sudo mkdir -p /srv/reprepro/ubuntu/{conf,dists,incoming,indices,logs,pool,project,tmp}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change directories to the new repository &lt;code class=&quot;language-text&quot;&gt;cd /srv/reprepro/ubuntu/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit the file &lt;code class=&quot;language-text&quot;&gt;/srv/reprepro/ubuntu/conf/distributions&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the file contents&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Origin: SplunkEnterprise
Label: SplunkEnterprise
Codename: ponies
Architectures: i386 amd64 source
Components: main
Description: Splunk Enterprise and Universal Forwarders for Debian based systems
SignWith: YOUR-KEY-ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;Edit the file &lt;code class=&quot;language-text&quot;&gt;/srv/reprepro/ubuntu/conf/options&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the file contents&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;ask-passphrase
basedir .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Load the packages&lt;/h2&gt;
&lt;p&gt;Load the packages using the following commands syntax replace package.deb with the correct path to the splunkforwarder deb file&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;reprepro -S utils -P standard includedeb ponies package.deb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Setup the web server&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create the file &lt;code class=&quot;language-text&quot;&gt;/etc/nginx/sites-available/vhost-packages.conf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use the following content replacing package.local with the fqdn of the repository host ```
server {
listen 80;
server_name packages.internal;
access_log /var/log/nginx/packages-access.log;
error_log /var/log/nginx/packages-error.log;
location / {
root /srv/reprepro;
index index.html;
}
location ~ /(.*)/conf {
deny all;
}
location ~ /(.*)/db {
deny all;
}
}&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;3. Increase the server name hash bucket by creating the following file `/etc/nginx/conf.d/server_names_hash_bucket_size.conf`
4. Use the following content `server_names_hash_bucket_size 64;`
5. Enable the new configuration
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;sudo ln -s /etc/nginx/sites-available/vhost-packages.conf /etc/nginx/sites-enabled/vhost-packages.conf
sudo service nginx reload&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
Configure the repository
------------------------

1. Edit the file ```
/etc/apt/sources.list.d/packages.internal.list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Use the following content ```
deb &lt;a href=&quot;http://packages.internal/ubuntu/&quot;&gt;http://packages.internal/ubuntu/&lt;/a&gt; ponies main&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;3. Import the public key ```
sudo apt-key add /tmp/repo.pub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Update the repository cache ```
sudo apt-get update&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;
Install the Splunk Universal Forwarder
--------------------------------------

Run the following command

`sudo apt-get install splunkforwarder`

Configure the universal forwarder

- Using best practices to manually create the org\_deploymentclient configuration app
- Using RPM based configuration package
- Using Configuration Managment system such as Puppet or Chef

Create and install a configuration package for the Universal Forwarder
----------------------------------------------------------------------

In the following procedure “org” should be replace with the abbreviate of the organization using the configuration.

1. Create the paths `/srv/reprepro/org_debs/`
2. Create the path for the first version of the package ie `mkdir org-splunk-ufconfig-1`
3. Change to the new directory
4. Create the following structure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;├── DEBIAN
│   ├── control (file)
│   ├── postinst (file)
│   ├── preinst (file)
│   └── prerm (file)
└── opt
└── splunkforwarder
└── etc
└── apps
└── org_all_deploymentclient
└── default
├── deploymentclient.conf (file)&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;5. Edit the DEBIAN/control file as follows `&amp;lt;br&gt;&amp;lt;/br&gt;Package: org-splunk-ufconfig&amp;lt;br&gt;&amp;lt;/br&gt;Section: base&amp;lt;br&gt;&amp;lt;/br&gt;Priority: standard&amp;lt;br&gt;&amp;lt;/br&gt;Version: 1&amp;lt;br&gt;&amp;lt;/br&gt;Architecture: all&amp;lt;br&gt;&amp;lt;/br&gt;Maintainer: Your Name &amp;lt;you@email.com&gt;&amp;lt;br&gt;&amp;lt;/br&gt;Depends: splunkforwarder (&gt;=6.0.0)&amp;lt;br&gt;&amp;lt;/br&gt;Description: &amp;lt;insert up to 60 chars description&gt;&amp;lt;br&gt;&amp;lt;/br&gt; &amp;lt;insert long description, indented with spaces&gt;&amp;lt;br&gt;&amp;lt;/br&gt;`
6. Edit the DEBIAN/postinst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;#!/bin/bash
/opt/splunkforwarder/bin/splunk enable boot-start -user splunk —accept-license —answer-yes
service splunk start&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;7. Edit the DEBIAN/preinst ```
#!/bin/bash
file=&quot;/etc/init.d/splunk&quot;
if [ -f &quot;$file&quot; ]
then
    echo &quot;$file found.&quot;
    service splunk stop
else
    echo &quot;$file not found.&quot;
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;Edit the DEBIAN/prerm ```
#!/bin/bash
file=“/etc/init.d/splunk”
if [ -f “$file” ]
then
echo “$file found.”
service splunk stop
/opt/splunkforwarder/bin/splunk disable boot-start&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;else
echo “$file not found.”
fi&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;9. Update the contents of deploymentclient.conf with the appropriate information for you installation
10. Add additional content as required for your deployment
11. Change directories up to the parent of org-splunk-ufconfig–1
12. Create the debian package with the command `dpkg-deb --build org-splunk-ufconfig-1/`
13. Change to the repository directory `/srv/reprepro/ubuntu`
14. Store the new package in the repository

`reprepro -S utils -P standard includedeb ponies /srv/reprepro/org_debs/org-splunk-ufconfig-1.deb`
15. Install the new package on the client using the command `sudo apt-get install org-splunk-ufconfig` this will install the splunk forwarder package if has not yet been installed.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Splunk Universal Forwarder Version 6.2.3+ Microsoft System Center 2012 R2]]></title><description><![CDATA[Author: Ryan Faircloth Summary: Rapid deployment of the universal forwarder in a production environment is possible with a minimal amount of…]]></description><link>https://rfaircloth.com/2015/05/26/splunk-universal-forwarder-version-6-2-3-microsoft-system-center-2012-r2-2/</link><guid isPermaLink="false">https://rfaircloth.com/2015/05/26/splunk-universal-forwarder-version-6-2-3-microsoft-system-center-2012-r2-2/</guid><pubDate>Tue, 26 May 2015 14:09:43 GMT</pubDate><content:encoded>&lt;p&gt;Author: Ryan Faircloth&lt;/p&gt;
&lt;p&gt;Summary: Rapid deployment of the universal forwarder in a production environment is possible with a minimal amount of risk for the customer. The installation of a universal forwarder can be performed at any time without impact to the production system and without reboot. A small caution is required in that if an existing MSI installation has created on reboot actions the installation of the Splunk universal forwarder or any other MSI may trigger a reboot by the SCCM client.&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;This guide will deploy the universal forwarder to all servers with a supported version of the Microsoft Windows Server operating system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new folder to contain Splunk related collections&lt;/li&gt;
&lt;li&gt;Create one or more collection containing all systems which should receive the universal forwarder.&lt;/li&gt;
&lt;li&gt;Create a collection containing all systems where any version of the universal forwarder -has been deployed&lt;/li&gt;
&lt;li&gt;Create an application definition to deploy the universal forwarder without configuration&lt;/li&gt;
&lt;li&gt;Create an application definition to deploy an upgrade to the universal forwarder without configuration&lt;/li&gt;
&lt;li&gt;Create a package containing a powershell script to configure the universal forwarder&lt;/li&gt;
&lt;li&gt;Deploy the configuration script using a task sequence&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Prerequisite Steps&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Responsible&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Create CNAME for Deployment Server&lt;/td&gt;
&lt;td&gt;DNS Admin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Install Splunk Enterprise on Server&lt;/td&gt;
&lt;td&gt;Splunk Admin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Configure Splunk Instance as Deployment Server&lt;/td&gt;
&lt;td&gt;Splunk Admin&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Step by Step&lt;/h1&gt;
&lt;h2&gt;Create the deployment collection folder&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to Device Collections&lt;/li&gt;
&lt;li&gt;Right click&lt;/li&gt;
&lt;li&gt;Create new folder&lt;/li&gt;
&lt;li&gt;Name the new folder “Splunk Universal Forwarders”&lt;/li&gt;
&lt;li&gt;Navigate to the new folder&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Create a collection for deployment&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Right click and choose ““Create New Device Collection”&lt;/li&gt;
&lt;li&gt;Name the collection “Splunk Deployment Collection for Servers”&lt;/li&gt;
&lt;li&gt;Select “All Desktop and Server Clients” as the limiting collection&lt;br&gt;
&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-1_CreateDeviceCollection.png?w=1100&quot; alt=&quot;Create Device Collection&quot; title=&quot;Create Device Collection&quot;&gt;&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Add to define the criteria used to determine which devices will receive the Universal Forwarder&lt;/li&gt;
&lt;li&gt;Click Query&lt;/li&gt;
&lt;li&gt;Name the Query “Server OS”&lt;/li&gt;
&lt;li&gt;Click Edit&lt;/li&gt;
&lt;li&gt;Click Show query language&lt;/li&gt;
&lt;li&gt;Enter the following query:&lt;br&gt;
&lt;code class=&quot;language-text&quot;&gt;sql&amp;lt;br&gt;&amp;lt;/br&gt;select SMS_R_SYSTEM.ResourceID,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.ResourceType,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.Name,SMS_R_SYSTEM.SMSUniqueIdentifier,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.ResourceDomainORWorkgroup,SMS_R_SYSTEM.Client&amp;lt;br&gt;&amp;lt;/br&gt;from SMS_R_System&amp;lt;br&gt;&amp;lt;/br&gt;inner join&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_OPERATING_SYSTEM on SMS_G_System_OPERATING_SYSTEM.ResourceId = SMS_R_System.ResourceId&amp;lt;br&gt;&amp;lt;/br&gt;where&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_OPERATING_SYSTEM.ProductType = 2&amp;lt;br&gt;&amp;lt;/br&gt;or SMS_G_System_OPERATING_SYSTEM.ProductType = 3&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click OK&lt;/li&gt;
&lt;li&gt;Click OK again&lt;/li&gt;
&lt;li&gt;Enable Incremental Update by checking the box&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Close&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&gt; Note: the collection will contain zero members until the update collection background task completes&lt;/p&gt;
&lt;h2&gt;Create a collection of all successfully deployed universal forwarders&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Right click and choose “Create New Device Collection”&lt;/li&gt;
&lt;li&gt;Name the collection “Splunk Deployment Collection for Deployed Forwarders”&lt;/li&gt;
&lt;li&gt;Select “All Desktop and Server Clients” as the limiting collection&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Add to define the criteria used to determine which devices will receive the Universal Forwarder&lt;/li&gt;
&lt;li&gt;Click Query&lt;/li&gt;
&lt;li&gt;Name the Query “Server OS”&lt;/li&gt;
&lt;li&gt;Click Edit&lt;/li&gt;
&lt;li&gt;Click Show query language&lt;/li&gt;
&lt;li&gt;Enter the following query: &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-2_DeviceCollectionQueryExample.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;sql&amp;lt;br&gt;&amp;lt;/br&gt;Select&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.ResourceID,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.ResourceType,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.Name,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.SMSUniqueIdentifier,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.ResourceDomainORWorkgroup,&amp;lt;br&gt;&amp;lt;/br&gt;SMS_R_SYSTEM.Client&amp;lt;br&gt;&amp;lt;/br&gt;from SMS_R_System&amp;lt;br&gt;&amp;lt;/br&gt;inner join&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_ADD_REMOVE_PROGRAMS on SMS_G_System_ADD_REMOVE_PROGRAMS.ResourceID = SMS_R_System.ResourceId&amp;lt;br&gt;&amp;lt;/br&gt;inner join&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_INSTALLED_SOFTWARE on SMS_G_System_INSTALLED_SOFTWARE.ResourceID = SMS_R_System.ResourceId&amp;lt;br&gt;&amp;lt;/br&gt;inner join&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_ADD_REMOVE_PROGRAMS_64 on&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_ADD_REMOVE_PROGRAMS_64.ResourceId = SMS_R_System.ResourceId&amp;lt;br&gt;&amp;lt;/br&gt;where&amp;lt;br&gt;&amp;lt;/br&gt;SMS_G_System_ADD_REMOVE_PROGRAMS.DisplayName = &quot;UniversalForwarder&quot;&amp;lt;br&gt;&amp;lt;/br&gt;and SMS_G_System_ADD_REMOVE_PROGRAMS_64.DisplayName = &quot;UniversalForwarder&quot;&amp;lt;br&gt;&amp;lt;/br&gt;or SMS_G_System_INSTALLED_SOFTWARE.ProductName = &quot;UniversalForwarder&quot;&amp;lt;br&gt;&amp;lt;/br&gt;order by SMS_R_System.Name&amp;lt;br&gt;&amp;lt;/br&gt;&lt;/code&gt; 11. Click OK 12. Click OK again 13. Enable Incremental Update by checking the box 14. Click Next 15. Click Next 16. Click Close&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: the collection will contain zero members until the update collection background task completes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Create Application Definitions&lt;/h2&gt;
&lt;p&gt;Download both the 32bit and 64bit versions of the Splunk Universal Forwarder into the source folder structure used for SCCM deployment applications. Do this for all versions currently deployed as well as the new version to be deployed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In general the locations are similar to the path:&lt;br&gt;
\\servername\source\vendor\product\version\bitness&lt;br&gt;
\\servername\source\Splunk\UniversalForwarder\6.2.3\x86&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Create the application definition for the oldest deployed version of the Univeral Forwarder first.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to Applications in the Software Library screen&lt;/li&gt;
&lt;li&gt;Right click and create a new folder for Splunk definitions&lt;/li&gt;
&lt;li&gt;Right click on the new folder and choose Create New Application&lt;/li&gt;
&lt;li&gt;Locate the 64 bit MSI for this product version &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-3_1_ApplicationDef.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Next again&lt;/li&gt;
&lt;li&gt;Update the definition with the following information&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Name (Include version Number and bitness Version number i.e. Universal Forwarder 6.2.3 (x64)&lt;/li&gt;
&lt;li&gt;Publisher&lt;/li&gt;
&lt;li&gt;Version&lt;/li&gt;
&lt;li&gt;Update the command line by removing “/q” and appending “/quiet AGREETOLICENSE=Yes”
&lt;blockquote&gt;
&lt;p&gt;Note it is very important that /q is replaced by /quiet&lt;br&gt;
&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-3_2_ApplicationDef.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Close&lt;/li&gt;
&lt;li&gt;Right click on the new application definition and click properties&lt;/li&gt;
&lt;li&gt;Select the deployment type tab&lt;/li&gt;
&lt;li&gt;Select the first deployment and click edit&lt;/li&gt;
&lt;li&gt;Select the program tab&lt;/li&gt;
&lt;li&gt;update the uninstall command replacing /q with /quiet&lt;/li&gt;
&lt;li&gt;select the third browse next to product code and select the MSI&lt;/li&gt;
&lt;li&gt;Click requirements&lt;/li&gt;
&lt;li&gt;Click add&lt;/li&gt;
&lt;li&gt;Select category = device condition = operating system and provide the supported 64bit operating systems &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-3_3_ApplicationDef.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Create and additional requirements appropriate for your environment such as memory and disk space free&lt;/li&gt;
&lt;li&gt;Click OK&lt;/li&gt;
&lt;li&gt;Click OK again&lt;/li&gt;
&lt;li&gt;Add a new deployment type define the 32 bit MSI type using the information above&lt;/li&gt;
&lt;li&gt;Edit the new type using the information above to set the product MSI and verify requirements&lt;/li&gt;
&lt;li&gt;Select the supersedence tab&lt;/li&gt;
&lt;li&gt;click add&lt;/li&gt;
&lt;li&gt;Click Browse and select the oldest prior version of the application deployed to replace&lt;/li&gt;
&lt;li&gt;Map old deployment type to new ensuring the types match &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-3_4_ApplicationDef.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Click OK&lt;/li&gt;
&lt;li&gt;Add any other replacements required&lt;/li&gt;
&lt;li&gt;Verify your work and click OK&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat the application creation process for all versions of the UF in production If you are upgrading monitor your deployment progress You may continue with this procedure while the Universal Forwarder application is deployed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Create a Configuration Script&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a source folder to contain the configuration script for example \\servername\source\splunk\scripts\UF_Config_V1&lt;/li&gt;
&lt;li&gt;The following script can be used as a template for the appropriate configuration for your site. At minimum the deployment server FQDN must be customized. Name the script configure.ps1&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;#Splunk Configuration Script for SCCM Task Sequence
#Locate Splunk based on the MSI registration

function Get-IniContent ($filePath)
{
$ini = @{}
$section=&quot;GLOBAL&quot;
$CommentCount=0
switch -regex -file $FilePath
{

 &quot;^\[(.+)\]&quot; # Section
{
$section = $matches[1]
$ini[$section] = @{}
$CommentCount = 0
}
&quot;^(\#.*)$&quot; # Comment
{
$value = $matches[1]
$CommentCount = $CommentCount + 1
$name = &quot;Comment&quot; + $CommentCount
#$ini[$section][$name] = $value
}
&quot;(.+?)\s*=(.*)&quot; # Key
{
$name,$value = $matches[1..2]
$ini[$section][$name] = $value
}
}
return $ini
}

$location =&quot;C:\Program Files\SplunkUniversalForwarder\&quot;

#note if splunk may not be installed at the default location uncomment the following lines
#$list = Get-WmiOBject -Class Win32_Product | Where-Object {
# $_.Name -eq &apos;UniversalForwarder&apos; -or $_.Name -eq &apos;Splunk&apos; }

#$splunkprod = $list | where-Object { $_.InstallLocation }

#$location = $splunkprod.InstallLocation

$scriptappver = 2

$splunkcmd = $location + &quot;bin\splunk.exe&quot;
$staticapp = $location + &quot;etc\apps\_static_all_universalforwarder\&quot;
$staticdefault = $staticapp + &quot;default\&quot;
$staticlocal = $staticapp + &quot;local\&quot;

$staticdefault_dc = $staticdefault + &quot;deploymentclient.conf&quot;
$staticlocal_dc = $staticlocal + &quot;deploymentclient.conf&quot;
$staticdefault_app = $staticdefault + &quot;app.conf&quot;

if (!(Test-Path -Path $staticapp)) {new-item -ItemType Directory -Path $staticapp}

if (!(Test-Path -Path $staticdefault)) {new-item -ItemType Directory -Path $staticdefault}

if (!(Test-Path -Path $staticlocal)) {new-item -ItemType Directory -Path $staticlocal}

if (!(Test-Path -Path $staticdefault_app))
{
 new-item -path $staticdefault_app -ItemType File
 Add-Content -Path $staticdefault_app -Value &quot;#Generated by scripting&quot;
 #Add-Content -Path $staticdefault_app -Value &quot;`r`n&quot;
 Add-Content -Path $staticdefault_app -Value &quot;[_static_all_universalforwarder]&quot;
 Add-Content -Path $staticdefault_app -Value &quot;author=Ryan Faircloth&quot;
 Add-Content -Path $staticdefault_app -Value &quot;description=Script Generated UF default configuration applied by SCCM&quot;
 Add-Content -Path $staticdefault_app -Value &quot;version=1&quot;
 Add-Content -Path $staticdefault_app -Value &quot;[ui]&quot;
 Add-Content -Path $staticdefault_app -Value &quot;is_visible = false&quot;
}

$appconf = Get-IniContent $staticdefault_app
$appver = $appconf[“_static_all_universalforwarder”][“version”]

if ($appver -ne $scriptappver)
{
if (!(Test-Path -Path $staticdefault_dc))
{
 new-item -path $staticdefault_dc -ItemType File
 Add-Content -Path $staticdefault_dc -Value &quot;#Generated by scripting&quot;
 Add-Content -Path $staticdefault_dc -Value &quot;[deployment-client]&quot;
 Add-Content -Path $staticdefault_dc -Value &quot;clientName=ScriptDeployed|&quot;
 Add-Content -Path $staticdefault_dc -Value &quot;[target-broker:deploymentServer]&quot;
 Add-Content -Path $staticdefault_dc -Value &quot;targetUri=srvsplunk.ad.domainname.com:8089&quot;
 Add-Content -Path $staticdefault_dc -Value &quot;&quot;

}

&amp;amp; $splunkcmd &quot;restart&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Create a Package to contain the configuration script&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a new package folder Splunk&lt;/li&gt;
&lt;li&gt;Create a new folder on a network share Splunk_config_vx where X is the version of the script and include a customized version of the config script provided&lt;/li&gt;
&lt;li&gt;Right click on the package folder create package&lt;/li&gt;
&lt;li&gt;Name the package Splunk Configuration Script v1&lt;/li&gt;
&lt;li&gt;Select the source folder &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-3_PackageDef.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click do not create a program&lt;/li&gt;
&lt;li&gt;Click next&lt;/li&gt;
&lt;li&gt;Click next&lt;/li&gt;
&lt;li&gt;Click Close&lt;/li&gt;
&lt;li&gt;Right click on the package and click “Distribute Content” using appropriate options for the environment. &lt;strong&gt;&lt;em&gt;Do not click deploy&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create the Task Sequence&lt;/li&gt;
&lt;li&gt;Crea a new Task Sequence Folder “Splunk”&lt;/li&gt;
&lt;li&gt;Right click the Task Sequence Folder Create Task Sequence&lt;/li&gt;
&lt;li&gt;Name the task Splunk Config Vx&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Close&lt;/li&gt;
&lt;li&gt;Right click on the task sequence&lt;/li&gt;
&lt;li&gt;Click properties&lt;/li&gt;
&lt;li&gt;Click the advance tab&lt;/li&gt;
&lt;li&gt;Select suppress task sequence notifications and disable this task sequence on computers where it is deployed&lt;/li&gt;
&lt;li&gt;Right click on the task sequence and choose edit&lt;/li&gt;
&lt;li&gt;Click Add General —&gt; powershell script&lt;/li&gt;
&lt;li&gt;Set the script name i.e. configure.ps1 and execution policy=bypass&lt;/li&gt;
&lt;li&gt;Click OK&lt;/li&gt;
&lt;li&gt;Right click on the task and deploy to the deployed collection created second above&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Create the configuration task sequence&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to Software Library&lt;/li&gt;
&lt;li&gt;Navigate to Operating System Deployment&lt;/li&gt;
&lt;li&gt;Navigate to Task Sequence&lt;/li&gt;
&lt;li&gt;Optional Create a new folder called Splunk&lt;/li&gt;
&lt;li&gt;Right click and Create a new task sequence&lt;/li&gt;
&lt;li&gt;Select Custom Sequence &lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-4_1_TaskSequence.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Name the sequence i.e. &lt;em&gt;Splunk Configuration Script Vx&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Next&lt;/li&gt;
&lt;li&gt;Click Close&lt;/li&gt;
&lt;li&gt;Right click on the task sequence&lt;/li&gt;
&lt;li&gt;Click properties&lt;/li&gt;
&lt;li&gt;Click the advanced tab&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Select suppress task sequence notifications&lt;/li&gt;
&lt;li&gt;disable this task sequence on computers where it is deployed&lt;br&gt;
&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-4_2_TaskSequence.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;15&quot;&gt;
&lt;li&gt;Click Ok&lt;/li&gt;
&lt;li&gt;Right click on the task sequence and choose edit&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Click Add General —&gt; powershell script&lt;/li&gt;
&lt;li&gt;Set the script name and execution policy=bypass&lt;br&gt;
&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/05/wpid-4_3_TaskSequence.png?w=1100&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;17&quot;&gt;
&lt;li&gt;Click OK&lt;/li&gt;
&lt;li&gt;Right click on the task and deploy to the deployed collection created second above&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Staying up to date with the updates, update smart with Microsoft and Secunia]]></title><description><![CDATA[Its been a busy year already Oracle’s Java, Adobe’s Flash, and so many Updates to Windows. Most users by how have heard they should keep…]]></description><link>https://rfaircloth.com/2015/01/25/staying-date-updates-update-smart-microsoft-secunia/</link><guid isPermaLink="false">https://rfaircloth.com/2015/01/25/staying-date-updates-update-smart-microsoft-secunia/</guid><pubDate>Sun, 25 Jan 2015 22:41:29 GMT</pubDate><content:encoded>&lt;p&gt;Its been a busy year already Oracle’s Java, Adobe’s Flash, and so many Updates to Windows. Most users by how have heard they should keep their Windows PCs up to date to avoid infection. Unfortunately, our adversaries have heard the same speech and are trying to deceive through fake updates for your computer. First reliable companies will not notify you by email, instant message, or advertisement that your computer is out of date and needs an update. You may see email or advertisements for new versions or upgrades, and subscription renewals. Some leading software companies are helping us stay secure through automatic or seamless updates such as Google’s Chrome browser, the FireFox Browser, and Adobe’s Flash. Security updates for these productions will simply install in the backgroud without needing your help. You can keep yourself safer by taking a few steps to secure your computer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lets take care of our operating system first.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open “Computer” on Windows 7 or “This PC” on Windows 8.1&lt;/li&gt;
&lt;li&gt;Click on Control Panel in the menu bar.&lt;br&gt;
&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-05-51.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-05-51.png?resize=1100%2C81&quot; alt=&quot;2015-01-25_17-05-51&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Search for “Windows Update” (1) in control panel then select “Turn automatic updating on or off” (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-07-24.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-07-24.png?resize=1100%2C527&quot; alt=&quot;2015-01-25_17-07-24&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup the options (1) (2) (3) as shown below then click ok (4)&lt;br&gt;
&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-09-13.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-09-13.png?resize=1100%2C527&quot; alt=&quot;2015-01-25_17-09-13&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Windows will now check all Microsoft Products daily for updates and install them as needed. You will be asked to reboot your computer to finish applying updates this is very important don’t put it off. Now what about non Microsoft programs? Secunia provides a product called PSI to help us with this task.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update the rest of our software with Secuina&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First download and install Secuina PSI it is very important for you to download from this link. There are a number of sites offering versions modified to include malware.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://secunia.com/vulnerability%5C_scanning/personal/&quot;&gt;http://secunia.com/vulnerability\_scanning/personal/&lt;/a&gt; you will need to provide your name and email address.&lt;/li&gt;
&lt;li&gt;Then look for the big “Download Now” button. “Try Now” is for a separate business grade product.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-24-09.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-24-09.png?resize=735%2C235&quot; alt=&quot;2015-01-25_17-24-09&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run and install PSISetup.Exe, this is a simple next, next, finish, default choices will be best.&lt;/li&gt;
&lt;li&gt;After you click finish the software will start to update your computer. I installed an old version of Java to demonstrate the process below:&lt;br&gt;
&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-29-31.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-29-31.png?resize=859%2C676&quot; alt=&quot;2015-01-25_17-29-31&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;After the updates complete you will see an updated list of software and your are done.&lt;br&gt;
&lt;a href=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-33-08.png&quot;&gt;&lt;img src=&quot;https://i0.wp.com/www.rfaircloth.com/wp-content/uploads/2015/01/2015-01-25_17-33-08.png?resize=1053%2C805&quot; alt=&quot;2015-01-25_17-33-08&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PSI will not upgrade software however, for example Adobe Acrobat XII (Future software) or the new Java JRE 1.8 will require you to visit the software vendor to download or purchase an upgrade at some point in the future.&lt;/p&gt;</content:encoded></item></channel></rss>